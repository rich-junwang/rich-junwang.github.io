<!DOCTYPE html>
<html lang="en-us" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Parallelism | Jun&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we&rsquo;ll dive deep into parallel training in recent distributed training paradigms. A lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We&rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency. Data Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is">
<meta name="author" content="Jun">
<link rel="canonical" href="https://rich-junwang.github.io/en-us/posts/tech/parallelism/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://rich-junwang.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://rich-junwang.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://rich-junwang.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://rich-junwang.github.io/img/Q.gif">
<link rel="mask-icon" href="https://rich-junwang.github.io/img/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<meta property="og:title" content="Parallelism" />
<meta property="og:description" content="Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we&rsquo;ll dive deep into parallel training in recent distributed training paradigms. A lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We&rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency. Data Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/parallelism/" />
<meta property="og:image" content="https://rich-junwang.github.io/images/speedup.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-08T12:01:14-07:00" />
<meta property="article:modified_time" content="2023-10-08T12:01:14-07:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://rich-junwang.github.io/images/speedup.jpg" />
<meta name="twitter:title" content="Parallelism"/>
<meta name="twitter:description" content="Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we&rsquo;ll dive deep into parallel training in recent distributed training paradigms. A lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We&rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency. Data Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šArticles",
          "item": "https://rich-junwang.github.io/en-us/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ‘¨ğŸ»â€ğŸ’» Tech",
          "item": "https://rich-junwang.github.io/en-us/posts/tech/"
        }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Parallelism",
      "item": "https://rich-junwang.github.io/en-us/posts/tech/parallelism/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Parallelism",
  "name": "Parallelism",
  "description": "Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we\u0026rsquo;ll dive deep into parallel training in recent distributed training paradigms. A lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We\u0026rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency. Data Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is",
  "keywords": [
    ""
  ],
  "articleBody": "Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, weâ€™ll dive deep into parallel training in recent distributed training paradigms.\nA lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. Weâ€™ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency.\nData Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is fed with one shard of data. Forward and backward computation is in parallel (simutaneously) and then there is a synchronization step where gradients are averaged across workers to update parameters. The DP computation can be summarized as the following three steps:\nEach machine computes local gradients given local inputs and a consistent global view of the parameters. LocalGrad_i = f(Inputs_i, Targets_i, Params) Sum up all the local gradients and distribute that sum to each machine, so there is a consistent global view of the gradients. GlobalGrad = all_reduce(LocalGrad_i) Each machine can now locally update the parameters and optimizer state under the assumption that the exact same calculation will happen on all machines. NewParams, NewOptimState = g(Params, OldOptimState, GlobalGrad) Pipeline Parallelism Pipeline parallelism (PP) is from model parallelism. Model parallelism is initially proposed to solve that challenge that one model canâ€™t fit into one GPU. The idea is we can vertically slice model into different layers (e.g. one or more layers in transformer models) and put different layers in different GPUs. The issue with this method is that because sequential computation order of layers, if we feed single large batch data into one of the workers, all other workers are idle. This is the so-called bubble waiting time.\nTo solve the problem, we can reuse the data parallelism idea. Instead of feeding a single large batch into a model shard, we can partition data into small chunks. Each chunk of data goes through different model shards (workers) in a pipeline way. The following figure illustrates how this works.\nPipeline parallelism. image from [4] Tensor Parallelism The bottleneck of neural network training is compute. Among all the computation parts, the general matrix multiplication (GEMM) consumes the most of time. One way to parallize the matrix multiplication is to use matrix decomposition. Specifically, we can split a matrix into two or multiple parts based on row or column. Then we can aggregate results after the computation of each parts in the end. This is the core idea of tensor parallelism (TP).\nAs these three parallelism is orthogonal to each other, itâ€™s easy to combine them together. The following diagram shows how to combine pipeline parallelism with data parallelism.\nCombination of pipeline parallelism and data parallelism. Image from Deepspeed tutorial ZeRO DP Zero Redundancy Optimizer (ZeRO) is an optimizied data parallelism proposed by Deepspeed team. The idea is instead of replicating the whole model, optimizer on each of workers, we can only store needed part.\nZero DP. Image from Deepspeed Parallelism in Megatron Megatron-LM and NeMo are the open source libraries from Nvidia for the distributed training. In these two libs, there are two constraints for the parallelism settings.\n- world_size = TP * PP * DP - global_batch_size % (PP * DP) == 0 References [1] https://huggingface.co/blog/bloom-megatron-deepspeed [2] https://github.com/NVIDIA/NeMo [3] https://openai.com/blog/techniques-for-training-large-neural-networks/ [4] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism [5] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism [6] https://www.deepspeed.ai/tutorials/pipeline/\n",
  "wordCount" : "631",
  "inLanguage": "en-us",
  "image":"https://rich-junwang.github.io/images/speedup.jpg","datePublished": "2023-10-08T12:01:14-07:00",
  "dateModified": "2023-10-08T12:01:14-07:00",
  "author":[{
    "@type": "Person",
    "name": "Jun"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://rich-junwang.github.io/en-us/posts/tech/parallelism/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jun's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://rich-junwang.github.io/img/Q.gif"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://rich-junwang.github.io/en-us/" accesskey="h" title="Jun&#39;s Blog (Alt + H)">
            <img src="https://rich-junwang.github.io/img/Q.gif" alt="logo" aria-label="logo"
                 height="35">Jun&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://rich-junwang.github.io/en-us/search" title="ğŸ” Search (Alt &#43; /)" accesskey=/>
                <span>ğŸ” Search</span>
                </a>
            </li>
            <li>
                <a href="https://rich-junwang.github.io/en-us/" title="ğŸ  Home">
                <span>ğŸ  Home</span>
                </a>
            </li>
            <li>
                <a href="https://rich-junwang.github.io/en-us/posts" title="ğŸ“š Posts">
                <span>ğŸ“š Posts</span>
                </a>
            </li>
            <li>
                <a href="https://rich-junwang.github.io/en-us/tags" title="ğŸ§© Tags">
                <span>ğŸ§© Tags</span>
                </a>
            </li>
            <li>
                <a href="https://rich-junwang.github.io/en-us/archives/" title="â±ï¸ Archives">
                <span>â±ï¸ Archives</span>
                </a>
            </li>
            <li>
                <a href="https://rich-junwang.github.io/en-us/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ About">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://rich-junwang.github.io/en-us/">ğŸ  Home</a>&nbsp;Â»&nbsp;<a href="https://rich-junwang.github.io/en-us/posts/">ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href="https://rich-junwang.github.io/en-us/posts/tech/">ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div>
            <h1 class="post-title">
                Parallelism
            </h1>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2023-10-08
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>631 words
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>2 min
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Jun
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
            </span>
        </span>
    </span>
</span><span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://rich-junwang.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId:  null , 
                                region:  null , 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1">
    <img style="zoom:;" loading="lazy" srcset="https://rich-junwang.github.io/en-us/posts/tech/parallelism/images/speedup_hu5af8b6a5d74444e6aa1581c4e93a14db_52100_360x0_resize_q75_box.jpg 360w ,https://rich-junwang.github.io/en-us/posts/tech/parallelism/images/speedup.jpg 460w"
    sizes="(min-width: 768px) 720px, 100vw" src="https://rich-junwang.github.io/en-us/posts/tech/parallelism/images/speedup.jpg" alt=""
    width="460" height="215">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#data-parallelism" aria-label="Data Parallelism">Data Parallelism</a></li>
                <li>
                    <a href="#pipeline-parallelism" aria-label="Pipeline Parallelism">Pipeline Parallelism</a></li>
                <li>
                    <a href="#tensor-parallelism" aria-label="Tensor Parallelism">Tensor Parallelism</a></li>
                <li>
                    <a href="#zero-dp" aria-label="ZeRO DP">ZeRO DP</a></li>
                <li>
                    <a href="#parallelism-in-megatron" aria-label="Parallelism in Megatron">Parallelism in Megatron</a></li></ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><p>Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we&rsquo;ll dive deep into parallel training in recent distributed training paradigms.</p>
<p>A lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We&rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency.</p>
<p align="center">
    <img alt="gopher dataset" src="images/speedup.jpg" width="60%"/>
    <br>
</p>
<h3 id="data-parallelism">Data Parallelism<a hidden class="anchor" aria-hidden="true" href="#data-parallelism">#</a></h3>
<p>Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is fed with one shard of data. Forward and backward computation is in parallel (simutaneously) and then there is a synchronization step where gradients are averaged across workers to update parameters. The DP computation can be summarized as the following <a href="https://www.adept.ai/blog/sherlock-sdc">three steps</a>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>Each machine computes local gradients given local inputs and a consistent global view of the parameters.
</span></span><span style="display:flex;"><span>LocalGrad_i = f(Inputs_i, Targets_i, Params)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Sum up all the local gradients and distribute that sum to each machine, so there is a consistent global view of the gradients.
</span></span><span style="display:flex;"><span>GlobalGrad = all_reduce(LocalGrad_i)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Each machine can now locally update the parameters and optimizer state under the assumption that the exact same calculation will happen on all machines.
</span></span><span style="display:flex;"><span>NewParams, NewOptimState = g(Params, OldOptimState, GlobalGrad)
</span></span></code></pre></div><h3 id="pipeline-parallelism">Pipeline Parallelism<a hidden class="anchor" aria-hidden="true" href="#pipeline-parallelism">#</a></h3>
<p>Pipeline parallelism (PP) is from model parallelism. Model parallelism is initially proposed to solve that challenge that one model can&rsquo;t fit into one GPU. The idea is we can vertically slice model into different layers (e.g. one or more layers in transformer models) and put different layers in different GPUs. The issue with this method is that because sequential computation order of layers, if we feed single large batch data into one of the workers, all other workers are idle. This is the so-called <code>bubble</code> waiting time.</p>
<p>To solve the problem, we can reuse the data parallelism idea. Instead of feeding a single large batch into a model shard, we can partition data into small chunks. Each chunk of data goes through different model shards (workers) in a pipeline way. The following figure illustrates how this works.</p>
<p align="center">
    <img alt="gopher dataset" src="images/pipeline.png" width="100%"/>
    <br>
    <em>Pipeline parallelism. image from [4]</em>
    <br>
</p>
<h3 id="tensor-parallelism">Tensor Parallelism<a hidden class="anchor" aria-hidden="true" href="#tensor-parallelism">#</a></h3>
<p>The bottleneck of neural network training is compute. Among all the computation parts, the general matrix multiplication (GEMM) consumes the most of time. One way to parallize the matrix multiplication is to use matrix decomposition. Specifically, we can split a matrix into two or multiple parts based on row or column. Then we can aggregate results after the computation of each parts in the end. This is the core idea of tensor parallelism (TP).</p>
<p>As these three parallelism is orthogonal to each other, it&rsquo;s easy to combine them together. The following diagram shows how to combine pipeline parallelism with data parallelism.</p>
<p align="center">
    <img alt="dp with pp" src="images/parallelism-zero-dp-pp.png" width="100%"/>
    <br>
    <em>Combination of pipeline parallelism and data parallelism. Image from Deepspeed tutorial</em>
    <br>
</p>
<h3 id="zero-dp">ZeRO DP<a hidden class="anchor" aria-hidden="true" href="#zero-dp">#</a></h3>
<p>Zero Redundancy Optimizer (ZeRO) is an optimizied data parallelism proposed by Deepspeed team. The idea is instead of replicating the whole model, optimizer on each of workers, we can only store needed part.</p>
<p align="center">
    <img alt="zero dp" src="images/zero.png" width="100%"/>
    <br>
    <em>Zero DP. Image from Deepspeed</em>
    <br>
</p>
<h3 id="parallelism-in-megatron">Parallelism in Megatron<a hidden class="anchor" aria-hidden="true" href="#parallelism-in-megatron">#</a></h3>
<p>Megatron-LM and NeMo are the open source libraries from Nvidia for the distributed training. In these two libs, there are two constraints for the parallelism settings.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>- world_size = TP * PP * DP
</span></span><span style="display:flex;"><span>- global_batch_size % (PP * DP) == 0
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] <a href="https://huggingface.co/blog/bloom-megatron-deepspeed">https://huggingface.co/blog/bloom-megatron-deepspeed</a> <br>
[2] <a href="https://github.com/NVIDIA/NeMo">https://github.com/NVIDIA/NeMo</a> <br>
[3] <a href="https://openai.com/blog/techniques-for-training-large-neural-networks/">https://openai.com/blog/techniques-for-training-large-neural-networks/</a> <br>
[4] <a href="https://arxiv.org/abs/1811.06965">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a> <br>
[5] <a href="https://arxiv.org/abs/1909.08053">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a> <br>
[6] <a href="https://www.deepspeed.ai/tutorials/pipeline/">https://www.deepspeed.ai/tutorials/pipeline/</a></p>


        </div>
        <div class="post-reward">
            <div style="padding: 0 0 0 0; margin: 0 0 0 0; width: 100%; font-size:16px; text-align: center;">
                <div id="QR" style="opacity: 0;">
                    <div id="wechat" style="display: inline-block">
                        <a class="fancybox" rel="group">
                            <img id="wechat_qr" src="https://rich-junwang.github.io/img/wechat_pay.png" alt="wechat_pay"></a>
                        <p>å¾®ä¿¡</p>
                    </div>
                    <div id="alipay" style="display: inline-block">
                        <a class="fancybox" rel="group">
                            <img id="alipay_qr" src="https://rich-junwang.github.io/img/alipay.png" alt="alipay"></a>
                        <p>æ”¯ä»˜å®</p>
                    </div>
                </div>
                <button id="rewardButton"
                        onclick="
                    var qr = document.getElementById('QR');
                    if (qr.style.opacity === '0') {
                        qr.style.opacity='1';
                    } else {
                        qr.style.opacity='0'
                    }"
                >
                    <span>ğŸ§§ é¼“åŠ±</span>
                </button>
            </div>
        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="next" href="https://rich-junwang.github.io/en-us/posts/tech/large_scale_pretraining/">
    <span class="title"> Â»</span>
    <br>
    <span>Large Scale Pretraining</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰Comment';
        color: var(--content);
    }

    .comments_details[open] summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‡Collapse';
        color: var(--content);
    }
</style>


<div>
    <details class="comments_details">
        <summary style="cursor: pointer; margin: 50px 0 20px 0;width: 130px;">
            <span style="font-size: 20px;color: var(--content);">...</span>
        </summary>
        <div id="tcomment"></div>
    </details>
    <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js">
    </script>
    <script>
        twikoo.init({
            envId:  null ,
            el: "#tcomment",
            lang: 'en-us',
            region:  null ,
            path: window.TWIKOO_MAGIC_PATH || window.location.pathname,
        })
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>
        Copyright
        &copy;
        2020-2023
        <a href="https://rich-junwang.github.io/en-us/" style="color:#939393;">Jun&#39;s Blog</a>
        All Rights Reserved
    </span>
    <a href="https://beian.miit.gov.cn/" target="_blank" style="color:#939393;"></a>&nbsp;
    <span>
        <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=null"
           style="display:inline-block;text-decoration:none;height:20px;color:#939393;">
            <img src="" style="float:left;margin: 0px 5px 0px 0px;"/>
            
        </a>
    </span>
    <span id="busuanzi_container">
        <span class="fa fa-user"></span> <span id="busuanzi_value_site_uv"></span>
        <span class="fa fa-eye"></span> <span id="busuanzi_value_site_pv"></span>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"Jun's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"Jun's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"Jun's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>
</body>

</html>
