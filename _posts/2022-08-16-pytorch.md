---
layout: post
title: Pytorch Multiple-GPU Training 
date: 2022-08-16
description: tricks and tips
tags: Tricks and Tips
categories: software
---

Using PyTorch for NN model training on single GPU is simple and easy. However, when it comes to multiple GPU training, there could be various of issues. In this blog, I'll summarize all kinds of issues I ran into during model training/evaluation.

### Loading a pretrained checkpoint
A lot of times when we save a checkpoint of a pretrained model, we also save the trainer (or model state) information. This means when we load model checkpoint again, model will already have a preallocated device. When we use the same number of GPU to continue training, it will work as expected. However, the issue will arise when we have different number of GPUs for two runs. Let's say, we first trained model on a single GPU, then we want to use multiple GPU to continue the training. When we move model to multiple GPU, there will be something weird. For instance, on GPU 0, you might see multiple process (normally one process per GPU). Or in other cases, you can see GPU 0 has much higher memory usage than other GPUs. 

Solution: when we load model, we only load parameters and strip all state information. This might be tricky sometimes. The simplest way to solve this issue is to wrap the command with with PyTorch distributed data parallel. 
```
python3 -m torch.distributed.launch --nnodes=1 --nproc_per_node=8 my_script.py my_config_file 
```


### Install Apex
Sometimes to use the latest distributed training feature, we have to install Apex. As Apex is closely coupled with Cuda, we need to follow the next few steps to correctlly install apex.
- Find out the Cuda version used in the system. 
```
python -c "import torch; print(torch.version.cuda)"
```

- Install from source
```
git clone https://github.com/NVIDIA/apex
cd apex
CUDA_HOME=/usr/local/cuda-{your-version-here}/ pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
```


### Commonly Used Pytorch Tricks
Distributed training is error-prone, so effective ways of debugging is needed. Here I document some of these commands
```
# print the whole tensor
torch.set_printoptions(profile="full")
torch.set_printoptions(linewidth=16000)
```