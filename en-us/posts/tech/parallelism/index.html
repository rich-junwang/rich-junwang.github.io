<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Parallelism in LLM Training | Jun's Blog</title>
<meta name=keywords content><meta name=description content="Parallelism in LLM training"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/parallelism/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/parallelism/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="Parallelism in LLM Training"><meta property="og:description" content="Parallelism in LLM training"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/parallelism/"><meta property="og:image" content="https://rich-junwang.github.io/images/speedup.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-08T12:01:14-07:00"><meta property="article:modified_time" content="2022-07-08T12:01:14-07:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rich-junwang.github.io/images/speedup.jpg"><meta name=twitter:title content="Parallelism in LLM Training"><meta name=twitter:description content="Parallelism in LLM training"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Parallelism in LLM Training","item":"https://rich-junwang.github.io/en-us/posts/tech/parallelism/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Parallelism in LLM Training","name":"Parallelism in LLM Training","description":"Parallelism in LLM training","keywords":[""],"articleBody":"Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we‚Äôll dive deep into parallel training in recent distributed training paradigms.\nA lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We‚Äôll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency.\nData Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is fed with one shard of data. Forward and backward computation is in parallel (simutaneously) and then there is a synchronization step where gradients are averaged across workers to update parameters. The DP computation can be summarized as the following three steps:\nEach machine computes local gradients given local inputs and a consistent global view of the parameters. LocalGrad_i = f(Inputs_i, Targets_i, Params) Sum up all the local gradients and distribute that sum to each machine, so there is a consistent global view of the gradients. GlobalGrad = all_reduce(LocalGrad_i) Each machine can now locally update the parameters and optimizer state under the assumption that the exact same calculation will happen on all machines. NewParams, NewOptimState = g(Params, OldOptimState, GlobalGrad) Pipeline Parallelism Pipeline parallelism (PP) is from model parallelism. Model parallelism is initially proposed to solve that challenge that one model can‚Äôt fit into one GPU. The idea is we can vertically slice model into different layers (e.g. one or more layers in transformer models) and put different layers in different GPUs. The issue with this method is that because sequential computation order of layers, if we feed single large batch data into one of the workers, all other workers are idle. This is the so-called bubble waiting time.\nTo solve the problem, we can reuse the data parallelism idea. Instead of feeding a single large batch into a model shard, we can partition data into small chunks. Each chunk of data goes through different model shards (workers) in a pipeline way. The following figure illustrates how this works.\nPipeline parallelism. image from [4] Tensor Parallelism The bottleneck of neural network training is compute. Among all the computation parts, the general matrix multiplication (GEMM) consumes the most of time. One way to parallize the matrix multiplication is to use matrix decomposition. Specifically, we can split a matrix into two or multiple parts based on row or column. Then we can aggregate results after the computation of each parts in the end. This is the core idea of tensor parallelism (TP).\nIn it‚Äôs essence, tensor parallelism is block matrix mutiplication. Based on how we partition the parameter matrix, there is row parallel partition and column parallel partition. For row parallel partition, there is\n$$ Y = XW = \\begin{bmatrix} X_1, \u0026 X_2\\end{bmatrix} \\begin{bmatrix} W_1 \\\\ W_2\\end{bmatrix} = X_1W_1 + X_2W_2 $$\nFor column parallel partition, there is $$ Y = XW = X\\begin{bmatrix} W_1 \\\\ W_2\\end{bmatrix} = XW_1 + XW_2 $$ Note that for row parallel, we need to partition the input into two parts as well. In original transformer MLP layer, there are two projection steps: hidden.size -\u003e 4 * hidden.size -\u003e hidden.size. In this case, in Megatron-LM MLP implementation, it first does column parallel partition, generating two matrices, then a row parallel partition. This is shown in the following figure:\nTensor Parallelism in Megatron-LM As these three parallelism is orthogonal to each other, it‚Äôs easy to combine them together. The following diagram shows how to combine pipeline parallelism with data parallelism.\nCombination of pipeline parallelism and data parallelism. Image from Deepspeed tutorial ZeRO DP Zero Redundancy Optimizer (ZeRO) is an optimizied data parallelism proposed by Deepspeed team. The idea is instead of replicating the whole model, optimizer on each of workers, we can only store needed part.\nZero DP. Image from Deepspeed Parallelism in Megatron Megatron-LM and NeMo are the open source libraries from Nvidia for the distributed training. In these two libs, there are two constraints for the parallelism settings.\n- world_size = TP * PP * DP - global_batch_size % (PP * DP) == 0 Sequence Parallel For operations such as layer normation, the operation can be paralleized on the sequence dimension. Remember that layernorm is normalization over the feature dimenstion, ie. a token representation of 2048 will be normalized over 2048 numbers. In light of this, sequence parallel is proposed to reduce GPU memory consumption.\nSequence parallelism Implementation A few key points in 3D parallelism implementation.\nTP is communication heavy, thus TP blocks should be put on different GPUs within the same node to leverage fast NVLink communication. On the contrary, PP communication is light, and it is usually put across nodes. Within a data parallel group, all GPUs hold the same model parameters. After each update, there will be gradient all-reduce operation. How to achieve this, in Megatron-LM, this is achieved by first partition all GPUs by pipeline parallelism. Then withnin the same pipeline block, partition GPUs based on tensor parallelism. After that, the number of copies within the pipeline block will be the data parallelism number.\nTraining Efficiency Metric A simpler metric for evaluation of training efficiency is model FLOPs utilization (MFU) which is defined as the ratio of the observed throughput to the theoretical maximum throughput with peak FLOPs. As can be seen in the definition, it‚Äôs very hardware dependent metric. For A100 GPUs, the metric can be calculated as $$\n\\frac{num_of_parameters * 6 * total_training_token_count}{num_of_gpus * 312e^{12} * training_days * 24 * 3600 }\n$$\nGenerally a good MFU should be above 40%.\nReferences [1] https://huggingface.co/blog/bloom-megatron-deepspeed [2] https://github.com/NVIDIA/NeMo [3] https://openai.com/blog/techniques-for-training-large-neural-networks/ [4] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism [5] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism [6] https://www.deepspeed.ai/tutorials/pipeline/ [7] MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs\n","wordCount":"1021","inLanguage":"en-us","image":"https://rich-junwang.github.io/images/speedup.jpg","datePublished":"2022-07-08T12:01:14-07:00","dateModified":"2022-07-08T12:01:14-07:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/parallelism/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>Parallelism in LLM Training</h1><div class=post-description>Parallelism in LLM training</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2022-07-08
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1021 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>3 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta></span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><figure class=entry-cover1><img style=zoom: loading=lazy srcset="https://rich-junwang.github.io/en-us/posts/tech/parallelism/images/speedup_hu5af8b6a5d74444e6aa1581c4e93a14db_52100_360x0_resize_q75_box.jpg 360w ,https://rich-junwang.github.io/en-us/posts/tech/parallelism/images/speedup.jpg 460w" sizes="(min-width: 768px) 720px, 100vw" src=https://rich-junwang.github.io/en-us/posts/tech/parallelism/images/speedup.jpg alt width=460 height=215></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#data-parallelism aria-label="Data Parallelism">Data Parallelism</a></li><li><a href=#pipeline-parallelism aria-label="Pipeline Parallelism">Pipeline Parallelism</a></li><li><a href=#tensor-parallelism aria-label="Tensor Parallelism">Tensor Parallelism</a></li><li><a href=#zero-dp aria-label="ZeRO DP">ZeRO DP</a></li><li><a href=#parallelism-in-megatron aria-label="Parallelism in Megatron">Parallelism in Megatron</a></li><li><a href=#sequence-parallelhttpsbrowsearxivorgpdf220505198pdf aria-label="Sequence Parallel"><a href=https://browse.arxiv.org/pdf/2205.05198.pdf>Sequence Parallel</a></a></li><li><a href=#implementation aria-label=Implementation>Implementation</a></li><li><a href=#training-efficiency-metric aria-label="Training Efficiency Metric">Training Efficiency Metric</a></li></ul><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we&rsquo;ll dive deep into parallel training in recent distributed training paradigms.</p><p>A lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We&rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency.</p><h3 id=data-parallelism>Data Parallelism<a hidden class=anchor aria-hidden=true href=#data-parallelism>#</a></h3><p>Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is fed with one shard of data. Forward and backward computation is in parallel (simutaneously) and then there is a synchronization step where gradients are averaged across workers to update parameters. The DP computation can be summarized as the following <a href=https://www.adept.ai/blog/sherlock-sdc>three steps</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Each machine computes local gradients given local inputs and a consistent global view of the parameters.
</span></span><span style=display:flex><span>LocalGrad_i = f(Inputs_i, Targets_i, Params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Sum up all the local gradients and distribute that sum to each machine, so there is a consistent global view of the gradients.
</span></span><span style=display:flex><span>GlobalGrad = all_reduce(LocalGrad_i)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Each machine can now locally update the parameters and optimizer state under the assumption that the exact same calculation will happen on all machines.
</span></span><span style=display:flex><span>NewParams, NewOptimState = g(Params, OldOptimState, GlobalGrad)
</span></span></code></pre></div><h3 id=pipeline-parallelism>Pipeline Parallelism<a hidden class=anchor aria-hidden=true href=#pipeline-parallelism>#</a></h3><p>Pipeline parallelism (PP) is from model parallelism. Model parallelism is initially proposed to solve that challenge that one model can&rsquo;t fit into one GPU. The idea is we can vertically slice model into different layers (e.g. one or more layers in transformer models) and put different layers in different GPUs. The issue with this method is that because sequential computation order of layers, if we feed single large batch data into one of the workers, all other workers are idle. This is the so-called <code>bubble</code> waiting time.</p><p>To solve the problem, we can reuse the data parallelism idea. Instead of feeding a single large batch into a model shard, we can partition data into small chunks. Each chunk of data goes through different model shards (workers) in a pipeline way. The following figure illustrates how this works.</p><p align=center><img alt="gopher dataset" src=images/pipeline.png width=100%><br><em>Pipeline parallelism. image from [4]</em></p><h3 id=tensor-parallelism>Tensor Parallelism<a hidden class=anchor aria-hidden=true href=#tensor-parallelism>#</a></h3><p>The bottleneck of neural network training is compute. Among all the computation parts, the general matrix multiplication (GEMM) consumes the most of time. One way to parallize the matrix multiplication is to use matrix decomposition. Specifically, we can split a matrix into two or multiple parts based on row or column. Then we can aggregate results after the computation of each parts in the end. This is the core idea of tensor parallelism (TP).</p><p>In it&rsquo;s essence, tensor parallelism is block matrix mutiplication. Based on how we partition the parameter matrix, there is row parallel partition and column parallel partition.
For row parallel partition, there is</p><p>$$
Y = XW = \begin{bmatrix} X_1, & X_2\end{bmatrix} \begin{bmatrix} W_1 \\ W_2\end{bmatrix} = X_1W_1 + X_2W_2
$$</p><p>For column parallel partition, there is
$$
Y = XW = X\begin{bmatrix} W_1 \\ W_2\end{bmatrix} = XW_1 + XW_2
$$
Note that for row parallel, we need to partition the input into two parts as well.
In original transformer MLP layer, there are two projection steps: <code>hidden.size -> 4 * hidden.size -> hidden.size</code>. In this case, in Megatron-LM MLP implementation, it first does column parallel partition, generating two matrices, then a row parallel partition. This is shown in the following figure:</p><p align=center><img alt="dp with pp" src=images/megatron-mlp.png width=60%>
<em>Tensor Parallelism in Megatron-LM</em><br></p><p>As these three parallelism is orthogonal to each other, it&rsquo;s easy to combine them together. The following diagram shows how to combine pipeline parallelism with data parallelism.</p><p align=center><img alt="dp with pp" src=images/parallelism-zero-dp-pp.png width=100%>
<em>Combination of pipeline parallelism and data parallelism. Image from Deepspeed tutorial</em></p><h3 id=zero-dp>ZeRO DP<a hidden class=anchor aria-hidden=true href=#zero-dp>#</a></h3><p>Zero Redundancy Optimizer (ZeRO) is an optimizied data parallelism proposed by Deepspeed team. The idea is instead of replicating the whole model, optimizer on each of workers, we can only store needed part.</p><p align=center><img alt="zero dp" src=images/zero.png width=100%>
<em>Zero DP. Image from Deepspeed</em><br></p><h3 id=parallelism-in-megatron>Parallelism in Megatron<a hidden class=anchor aria-hidden=true href=#parallelism-in-megatron>#</a></h3><p>Megatron-LM and NeMo are the open source libraries from Nvidia for the distributed training. In these two libs, there are two constraints for the parallelism settings.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>- world_size = TP * PP * DP
</span></span><span style=display:flex><span>- global_batch_size % (PP * DP) == 0
</span></span></code></pre></div><h3 id=sequence-parallelhttpsbrowsearxivorgpdf220505198pdf><a href=https://browse.arxiv.org/pdf/2205.05198.pdf>Sequence Parallel</a><a hidden class=anchor aria-hidden=true href=#sequence-parallelhttpsbrowsearxivorgpdf220505198pdf>#</a></h3><p>For operations such as layer normation, the operation can be paralleized on the sequence dimension. Remember that layernorm is normalization over the feature dimenstion, ie. a token representation of 2048 will be normalized over 2048 numbers. In light of this, sequence parallel is proposed to reduce GPU memory consumption.</p><p align=center><img alt="zero dp" src=images/seq_parallel.png width=100%>
<em>Sequence parallelism</em><br></p><h3 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h3><p>A few key points in 3D parallelism implementation.</p><ul><li>TP is communication heavy, thus TP blocks should be put on different GPUs within the same node to leverage fast NVLink communication. On the contrary, PP communication is light, and it is usually put across nodes.</li><li>Within a data parallel group, all GPUs hold the same model parameters. After each update, there will be gradient all-reduce operation.</li></ul><p>How to achieve this, in Megatron-LM, this is achieved by first partition all GPUs by pipeline parallelism. Then withnin the same pipeline block, partition GPUs based on tensor parallelism. After that, the number of copies within the pipeline block will be the data parallelism number.</p><h3 id=training-efficiency-metric>Training Efficiency Metric<a hidden class=anchor aria-hidden=true href=#training-efficiency-metric>#</a></h3><p>A simpler metric for evaluation of training efficiency is model FLOPs utilization (MFU) which is defined as the ratio of the observed throughput to the theoretical maximum throughput with peak FLOPs.
As can be seen in the definition, it&rsquo;s very hardware dependent metric. For A100 GPUs, the metric can be calculated as
$$</p><p>\frac{num_of_parameters * 6 * total_training_token_count}{num_of_gpus * 312e^{12} * training_days * 24 * 3600 }</p><p>$$</p><p>Generally a good MFU should be above 40%.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <a href=https://huggingface.co/blog/bloom-megatron-deepspeed>https://huggingface.co/blog/bloom-megatron-deepspeed</a><br>[2] <a href=https://github.com/NVIDIA/NeMo>https://github.com/NVIDIA/NeMo</a><br>[3] <a href=https://openai.com/blog/techniques-for-training-large-neural-networks/>https://openai.com/blog/techniques-for-training-large-neural-networks/</a><br>[4] <a href=https://arxiv.org/abs/1811.06965>GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a><br>[5] <a href=https://arxiv.org/abs/1909.08053>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a><br>[6] <a href=https://www.deepspeed.ai/tutorials/pipeline/>https://www.deepspeed.ai/tutorials/pipeline/</a><br>[7] <a href=https://arxiv.org/pdf/2402.15627.pdf>MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs</a></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/antlr/><span class=title>¬´</span><br><span>ANTLR Parser Generator</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/diffusion/><span class=title>¬ª</span><br><span>Diffusion Probabilistic Models</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2024
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>