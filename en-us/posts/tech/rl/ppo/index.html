<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PPO and Its Implementation | Jun's Blog</title>
<meta name=keywords content><meta name=description content="Proximal Policy Optimization"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/rl/ppo/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/rl/ppo/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="PPO and Its Implementation"><meta property="og:description" content="Proximal Policy Optimization"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/rl/ppo/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-05T00:18:23-07:00"><meta property="article:modified_time" content="2024-07-05T00:18:23-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="PPO and Its Implementation"><meta name=twitter:description content="Proximal Policy Optimization"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"PPO and Its Implementation","item":"https://rich-junwang.github.io/en-us/posts/tech/rl/ppo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PPO and Its Implementation","name":"PPO and Its Implementation","description":"Proximal Policy Optimization","keywords":[""],"articleBody":"In this blog, I‚Äôll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.\nBasics Monte Carlo Approximation Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as\n$$ \\mathbb{E_{x\\sim p(x)}}\\left(f(x)\\right) = \\int{f(x)p(x)} dx $$ when it‚Äôs a continuous random variable with a probability density function of $p$, or $$ \\mathbb{E}\\left(f(x)\\right) = \\sum_x{f(x)p(x)} $$ when it‚Äôs a discrete random variable with probability mass function of $p$. Then the Monte Carlo approximation says that the expectation is: $$ \\mathbb{E}\\left(f(x)\\right) \\approx \\frac{1}{N}\\sum_{i=1}^{N}{f(x_i)} $$\nassuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution.\nImportance Sampling In reality, it could be very challenging to sample data according to the distribution $p(x)$ as it is usually unknown to us. A workaround is to have another known distribution $q(x)$, and define the expectation as: $$ \\mathbb{E_{x\\sim p(x)}}[f] = \\int{q(x)\\frac{p(x)}{q(x)}f(x)} dx $$ This can be seen as the expectation of function $\\frac{p(x)}{q(x)}f(x)$ according to the distribution of $q(x)$. The distribution is sometimes called the proposal distribution. Then the expectation can be estimated as $$ \\mathbb{E_{x\\sim q(x)}}[f] \\approx \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{p(x_i)}{q(x_i)}f(x_i)} $$ Here the ratios $\\frac{p(x_i)}{q(x_i)}$ are referred sa the importance weights. The above derivation looks nice. However, we need to notice that the although the expectation is similar in both cases, the variance is different:\n$$ Var_{x\\sim p(x)}[f] = \\mathbb{E_{x\\sim p(x)}}[f(x)^2] - ({\\mathbb{E_{x\\sim p(x)}}[f(x)]})^2 $$\n$$ \\begin{aligned} Var_{x\\sim q(x)}[f] \u0026= \\mathbb{E_{x\\sim q(x)}}[({\\frac{p(x_i)}{q(x_i)}f(x_i)})^2] - (\\mathbb{E_{x\\sim q(x)}}[{\\frac{p(x_i)}{q(x_i)}f(x_i)}])^2 \\\\ \u0026= \\mathbb{E_{x\\sim p(x)}}[{\\frac{p(x_i)}{q(x_i)}f(x_i)^2}] - (\\mathbb{E_{x\\sim p(x)}}[f(x_i)])^2 \\end{aligned} $$ Notice that the second equation here, in the second step derivation, the expectation is relative to distribution of $p(x)$. From the above two equations, we can see that to make the sampling distribution as close as possible to the original distribution, the ratio $\\frac{p(x_i)}{q(x_i)}$ has to be close to 1.\nPolicy Gradient First, let‚Äôs remind ourselves some basics. The discounted return for a trajectory is defined as: $$ U_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + ‚Ä¶ $$\nConsequently, the action-value function is defined as $$ Q_{\\pi}(s_t, a_t) = \\mathbb{E_t}[U_t|S_t=s_t, A_t=a_t] $$\nState-value function (or value function) can be calculated as: $$ V_{\\pi}(s_t) = \\mathbb{E_A}[Q_{\\pi}(s_t, A)] = \\sum_a \\pi(a|s_t) \\cdot Q_{\\pi}(s_t, a) $$\nIn policy gradient algorithm, the policy function $\\pi(a|s_t)$ is approximated by policy network $\\pi(a|s_t; \\theta)$. $\\theta$ here is the neural network model parameters. Then the policy-based learning is to maximize the objective function $$ \\begin{aligned} J(\\theta) \u0026= \\mathbb{E_S}[V(S; \\theta)] \\\\ \u0026= \\sum_{s\\in S} d_{\\pi}(s) V_{\\pi}(s_t; \\theta) \\\\ \u0026= \\sum_{s\\in S} d_{\\pi}(s) \\sum_a \\pi(a|s_t; \\theta) \\cdot Q_{\\pi}(s_t, a) \\end{aligned} $$\nwhere $d_{\\pi}(s)$ is the stationary distribution of Markov chain for $\\pi_{\\theta}$, namely the state distribution under policy $\\pi$. Now we know the objective function of the policy-based algorithm, we can learn the parameters $\\theta$ through policy gradiet ascent.\nNow we can look at how to get the policy gradient. Since the first summation of the last step in the above equation has nothing to do with $\\theta$, so we can focus on getting the derivatives of the value function $V_{\\pi}(s; \\theta)$. Using chain rule, it‚Äôs easy to get: $$ \\begin{aligned} \\frac{\\partial{V(s; \\theta)}}{\\partial{\\theta}} \u0026= \\sum_a \\frac{\\partial{\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\\\ \u0026= \\sum_a \\pi(a|s_t; \\theta) \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\\\ \u0026= \\mathbb{E_A}\\left[ \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\right] \\end{aligned} $$ The last step assumes that $\\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a)$ follows a distribution of $\\pi(a|s_t; \\theta)$ with respect to the random variable $A$.\nLet‚Äôs take another look at the policy gradient here. First, in practice, when we calculate the expectation we can use Monte Carlo Approximation. The gradient here becomes summations as below:\n$$ \\nabla_{\\theta}(J(\\theta)) = \\sum_{t} \\nabla_{\\theta}{\\log\\pi (a|s; \\theta)} \\cdot Q_{\\pi}(s, a) $$\nThis is also called Monte Carlo policy gradient. Since gradient is a direction, this formula shows that policy gradient estimation is the direction of the steepest increase in reward/return. When reward is larger, the policy gradient will be larger.\nTemporal Difference (TD) Learning Temporal Difference (TD) learning is one of the core concepts in Reinforcement Learning. Temporal difference algorithm always aims to bring the expected prediction and the new prediction together, thus matching expectations with reality and gradually increasing the accuracy of the entire chain of prediction.\nThe most basic version is TD(0) method. Specifically, if our agent is in a current state $s_t$, takes the action $a_t$ and receives the reward $r_t$, then we update our estimate of $V$ following\n$$ V(s_t) \\xleftarrow[]{} V(s_t) + \\alpha[r_{t+1} + \\gamma V(s_{t+1}) ‚Äì V(s_t)] $$\nHere $r_{t+1} + \\gamma V(s_{t+1})$ is TD target and $r_{t+1} + \\gamma V(s_{t+1}) ‚Äì V(s_t)$ is called TD error ($\\delta$).\nThere is SARSA (state-action-reward-state-action), where we replace the value function as the action-state value function.\n$$ Q(s_t, a_t) \\xleftarrow[]{} Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) ‚Äì Q(s_t, a_t)] $$\nAnd TD with Q-learning $$ Q(s_t, a_t) \\xleftarrow[]{} Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) ‚Äì Q(s_t, a_t)] $$\nREINFORCE Since $Q_{\\pi}(s, a)$ is the expectation of the return, we can once again use Monte Carlo approximation, $$ \\begin{aligned} Q_{\\pi}(s_t, a_t) \u0026= u_t \\\\ \u0026= \\sum_{i=t}^{N} {\\gamma^{i-t} \\cdot r_{i}} \\end{aligned} $$ The above MCPG actually gives us a practical algorithm to do policy gradient based RL. Let‚Äôs summarize it as follows:\nPlay one episode of game to get the trajectory: $s_1, a_1, r_1, s_2, a_2, r_2, ‚Ä¶$ Estimate all $q_t \\approx u_t$ using above equation Differentiate policy network to get $d_{\\theta, t}$ Compute policy gradient $g(a_t, \\theta_t) = q_t \\cdot d_{\\theta, t}$ Advantage Function and Generalized Advantage Estimation The above equation is the vanilla policy gradient method. More policy gradient algorithms are proposed later to reduce high variance of the vanilla version. John Schulman‚Äôs GAE paper summarized all the improvement methods. In the derivation, the policy gradient is represented as $$ \\frac{\\partial{V(s; \\theta)}}{\\partial{\\theta}} = \\mathbb{E_A}\\left[ \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] $$ where $\\hat{A_t}(s, a)$ is the advantage function. In implementation, we construct loss function in a way such that the policy gradient $g$ equals to the above result $$ L(\\theta) = \\mathbb{E_t}\\left[ \\log\\pi (a|s; \\theta) \\hat{A_t}(s, a) \\right] $$\nThe idea is that the Advantage function calculates how better taking that action at a state is compared to the average value of the state. It‚Äôs subtracting the mean value of the state from the state action pair. Mathematically, $A(s_t, a_t) = Q(s_t, a_t) ‚àí V (s_t)$, where $Q(s_t, a_t)$ is the action-value function, representing the expected return after taking action at at state $s$, and $V (s_t)$ is the value function, representing the average expected return at state $s_t$.\nBased on the above advantage definition, we have $$ \\begin{aligned} \\hat{A_t^{(1)}} \u0026= r_t + \\gamma V(s_{t+1}) - V(s) \\\\ \\hat{A_t^{(2)}} \u0026= r_t + \\gamma r_{t+1} +\\gamma^2 V(s_{t+2}) - V(s) \\\\ ‚Ä¶\\\\ \\hat{A_t^{(\\infty)}} \u0026= r_t + \\gamma r_{t+1} +\\gamma^2 r_{t+2} + ‚Ä¶ - V(s) \\end{aligned} $$\nNotice that $\\hat{A_t^{(1)}}$ has high bias, low variance, whilst $\\hat{A_t^{(\\infty)}}$ is unbiased, high variance. A weighted average of $\\hat{A_t^{(k)}}$ can be used to balance bias and variance. $$\\hat{A_t} = \\hat{A_t^{GAE}} = \\frac{\\sum_k w_k \\hat{A_t^{(k)}}}{\\sum_k w_k}$$ We set $w_k = \\lambda^{k-1}$, this gives clean calculation for $\\hat{A_t}$. Below we have the recursion equations. (Refer to [11] to learn how to derive the second equation here.)\n$$ \\begin{aligned} \\delta_t \u0026= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\ \\hat{A_t} \u0026= \\delta_t + \\gamma \\lambda \\delta_{t+1} + ‚Ä¶ + (\\gamma \\lambda)^{T - t + 1} \\delta_{T - 1} \\\\ \u0026= \\delta_t + \\gamma \\lambda \\hat{A_{t+1}} \\end{aligned} $$\nActor-Critic Algorithm There we give a recap of how actor-critic method works. In Actor-Critic algorithm, we use one neural network $\\pi(a|s; \\theta)$ to approximate policy function $\\pi(a|s)$ and use another neural network $q(s, a; w)$ to approximate value function $Q_{\\pi}(s, a)$.\nObserve state $s_t$, and randomly sample action from policy $a_t \\sim \\pi(\\cdot | s_t; \\Theta_t)$ Let agent perform action $a_t$, and get new state $s_{t+1}$ and reward $r_t$ from environment Randomly sample $\\tilde{a}_{t+1} \\sim \\pi(\\cdot | s_t; \\Theta_t)$ without performing the action Evaluate value network: $q_t = q(s_t, a_t; W_t)$ and $q_{t+1} = q(s_{t+1}, \\tilde{a}_{t+1}; W_t)$ Compute TD error: $\\delta_t = q_t - (r_t + \\gamma \\cdot q_{t+1})$ Differentiate value network: $d_{w,t} = \\frac{\\partial{q(s_t, a_t, w)}}{\\partial{w}}$ (autograd will do this for us) Update value network: $ w_{t+1} = w_t - \\alpha \\cdot \\delta_t \\cdot d_{w, t}$ Differentiate policy network: $ d_{\\theta, t} = \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} $ (again autograd will do this for us) Update policy network: $\\theta_{t+1} = \\theta_t + \\beta \\cdot q_t \\cdot d_{\\theta, t}$. We can also use: $\\theta_{t+1} = \\theta_t + \\beta \\cdot \\delta_t \\cdot d_{\\theta, t}$ to update policy network. This is called policy gradient with baseline. Essentially, the algorithm alternates between sampling and optimization. The expectation in the above equation indicates that we need to average over a finite batch of empirical samples. Proximal Policy Optimization Vanilla policy gradient method uses on-policy update. Concretely, the algorithm samples empirical data from a policy network $\\pi_{\\theta}$ parameterized with $\\theta$. After updating the network itself, the new policy network is $\\pi_{\\theta_{new}}$ and the old policy $\\pi_{\\theta}$ is out of use and future sampling will be from $\\pi_{\\theta_{new}}$. This whole process is not efficient enough. The solution to this is to reuse the old samples to achieve off-policy training. From above importance sampling section, we know that:\n$$ \\mathbb{E_{x\\sim p(x)}}\\left[f \\right] = \\mathbb{E_{x\\sim q(x)}} \\left[ \\frac{p(x_i)}{q(x_i)}f(x_i) \\right] $$\nSimilarly, we can make a change to the objective function of our policy gradient, and the resulting policy gradient will become $$ \\begin{aligned} g \u0026= \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta}}}\\left[ \\frac{\\partial{\\log\\pi (a_t|s_t; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] \\\\ \u0026= \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t; \\theta)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\frac{\\partial{\\log\\pi (a_t|s_t; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] \\end{aligned} $$ Consequently, the loss becomes\n$$ L(\\theta) = \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\hat{A_t}(s, a) \\right] $$ This is so-called surrogate objective function. In the above section, we mentioned how to use chain rule to get the expectation format of gradient, here we just to reverse the process to get the above loss function.\nIn the importance sampling section, we saw that the variance of new distribution could be large when the proposal distribution is not so close to the original distribution. Thus, to deal with this, people add KL diveragence to the loss function to limit the old and new policy difference. Using Largrangian dual method, we can add this constraint to the objective function:\n$$ L(\\theta) = \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\hat{A_t}(s, a) - \\beta KL[\\pi_{\\theta_{old}}(a_t|s_t), \\pi_{\\theta}(a_t|s_t)]\\right] $$\nImplementation For language generation task, generating a token is an action. Agent is the target language model we want to train.\nHere we first look at the implementation from Deepspeed-chat model. The actor-critic algorithm requires to load four model in training: actor model, critic model, reference model and reward mdoel. Actor model is the poliy network and critice model is the value network. Reference model and reward model are frozen in training. Reference model is used to contrain the actor model predictions so that they won‚Äôt divege too much. Reward model gives the current step reward.\nReferences [1] High-Dimensional Continuous Control Using Generalized Advantage Estimation [2] Proximal Policy Optimization Algorithms [3] Policy Gradient Methods for Reinforcement Learning with Function Approximation [4] Dueling Network Architectures for Deep Reinforcement Learning [5] https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/schedule.html [6] https://github.com/wangshusen/DRL [7] https://www.davidsilver.uk/teaching/ [8] Fine-Tuning Language Models from Human Preferences [9] https://zhuanlan.zhihu.com/p/677607581 [10] DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [11] Secrets of RLHF in Large Language Models Part I: PPO [12] Secrets of RLHF in Large Language Models Part II: Reward Modeling [13] The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization [14] Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study [15] Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO [16] Advanced Tricks for Training Large Language Models with Proximal Policy Optimization\n","wordCount":"2025","inLanguage":"en-us","datePublished":"2024-07-05T00:18:23-07:00","dateModified":"2024-07-05T00:18:23-07:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/rl/ppo/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>PPO and Its Implementation</h1><div class=post-description>Proximal Policy Optimization</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-07-05
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>2025 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>5 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/rl/ style=color:var(--secondary)!important>RL</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#basics aria-label=Basics>Basics</a><ul><li><a href=#monte-carlo-approximation aria-label="Monte Carlo Approximation">Monte Carlo Approximation</a></li><li><a href=#importance-sampling aria-label="Importance Sampling">Importance Sampling</a></li><li><a href=#policy-gradient aria-label="Policy Gradient">Policy Gradient</a></li><li><a href=#temporal-difference-td-learning aria-label="Temporal Difference (TD) Learning">Temporal Difference (TD) Learning</a></li><li><a href=#reinforce aria-label=REINFORCE>REINFORCE</a></li><li><a href=#advantage-function-and-generalized-advantage-estimation aria-label="Advantage Function and Generalized Advantage Estimation">Advantage Function and Generalized Advantage Estimation</a></li><li><a href=#actor-critic-algorithm aria-label="Actor-Critic Algorithm">Actor-Critic Algorithm</a></li><li><a href=#proximal-policy-optimization aria-label="Proximal Policy Optimization">Proximal Policy Optimization</a></li></ul></li><li><a href=#implementation aria-label=Implementation>Implementation</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>In this blog, I&rsquo;ll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.</p><h3 id=basics>Basics<a hidden class=anchor aria-hidden=true href=#basics>#</a></h3><h4 id=monte-carlo-approximation>Monte Carlo Approximation<a hidden class=anchor aria-hidden=true href=#monte-carlo-approximation>#</a></h4><p>Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as</p><p>$$
\mathbb{E_{x\sim p(x)}}\left(f(x)\right) = \int{f(x)p(x)} dx
$$
when it&rsquo;s a continuous random variable with a probability density function of $p$, or
$$
\mathbb{E}\left(f(x)\right) = \sum_x{f(x)p(x)}
$$
when it&rsquo;s a discrete random variable with probability mass function of $p$.
Then the Monte Carlo approximation says that the expectation is:
$$
\mathbb{E}\left(f(x)\right) \approx \frac{1}{N}\sum_{i=1}^{N}{f(x_i)}
$$</p><p>assuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution.</p><h4 id=importance-sampling>Importance Sampling<a hidden class=anchor aria-hidden=true href=#importance-sampling>#</a></h4><p>In reality, it could be very challenging to sample data according to the distribution $p(x)$ as it is usually unknown to us. A workaround is to have another known distribution $q(x)$, and define the expectation as:
$$
\mathbb{E_{x\sim p(x)}}[f] = \int{q(x)\frac{p(x)}{q(x)}f(x)} dx
$$
This can be seen as the expectation of function $\frac{p(x)}{q(x)}f(x)$ according to the distribution of $q(x)$. The distribution is sometimes called the <strong>proposal distribution</strong>. Then the expectation can be estimated as
$$
\mathbb{E_{x\sim q(x)}}[f] \approx \frac{1}{N}\sum_{i=1}^{N}{\frac{p(x_i)}{q(x_i)}f(x_i)}
$$
Here the ratios $\frac{p(x_i)}{q(x_i)}$ are referred sa the importance weights.
The above derivation looks nice. However, we need to notice that the although the expectation is similar in both cases, the variance is different:</p><p>$$
Var_{x\sim p(x)}[f] = \mathbb{E_{x\sim p(x)}}[f(x)^2] - ({\mathbb{E_{x\sim p(x)}}[f(x)]})^2
$$</p><p>$$
\begin{aligned}
Var_{x\sim q(x)}[f] &= \mathbb{E_{x\sim q(x)}}[({\frac{p(x_i)}{q(x_i)}f(x_i)})^2] - (\mathbb{E_{x\sim q(x)}}[{\frac{p(x_i)}{q(x_i)}f(x_i)}])^2 \\
&= \mathbb{E_{x\sim p(x)}}[{\frac{p(x_i)}{q(x_i)}f(x_i)^2}] - (\mathbb{E_{x\sim p(x)}}[f(x_i)])^2
\end{aligned}
$$
Notice that the second equation here, in the second step derivation, the expectation is relative to distribution of $p(x)$. From the above two equations, we can see that to make the sampling distribution as close as possible to the original distribution, the ratio $\frac{p(x_i)}{q(x_i)}$ has to be close to 1.</p><h4 id=policy-gradient>Policy Gradient<a hidden class=anchor aria-hidden=true href=#policy-gradient>#</a></h4><p>First, let&rsquo;s remind ourselves some basics. The discounted return for a trajectory is defined as:
$$
U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + &mldr;
$$</p><p>Consequently, the action-value function is defined as
$$
Q_{\pi}(s_t, a_t) = \mathbb{E_t}[U_t|S_t=s_t, A_t=a_t]
$$</p><p>State-value function (or value function) can be calculated as:
$$
V_{\pi}(s_t) = \mathbb{E_A}[Q_{\pi}(s_t, A)] = \sum_a \pi(a|s_t) \cdot Q_{\pi}(s_t, a)
$$</p><p>In policy gradient algorithm, the policy function $\pi(a|s_t)$ is approximated by policy network $\pi(a|s_t; \theta)$. $\theta$ here is the neural network model parameters. Then the policy-based learning is to maximize the objective function
$$
\begin{aligned}
J(\theta) &= \mathbb{E_S}[V(S; \theta)] \\
&= \sum_{s\in S} d_{\pi}(s) V_{\pi}(s_t; \theta) \\
&= \sum_{s\in S} d_{\pi}(s) \sum_a \pi(a|s_t; \theta) \cdot Q_{\pi}(s_t, a)
\end{aligned}
$$</p><p>where $d_{\pi}(s)$ is the stationary distribution of Markov chain for $\pi_{\theta}$, namely the state distribution under policy $\pi$.
Now we know the objective function of the policy-based algorithm, we can learn the parameters $\theta$ through policy gradiet ascent.</p><p>Now we can look at how to get the policy gradient. Since the first summation of the last step in the above equation has nothing to do with $\theta$, so we can focus on getting the derivatives of the value function $V_{\pi}(s; \theta)$. Using chain rule, it&rsquo;s easy to get:
$$
\begin{aligned}
\frac{\partial{V(s; \theta)}}{\partial{\theta}} &= \sum_a \frac{\partial{\pi (a|s; \theta)}}{\partial{\theta}} \cdot Q_{\pi}(s, a) \\
&= \sum_a \pi(a|s_t; \theta) \frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} \cdot Q_{\pi}(s, a) \\
&= \mathbb{E_A}\left[ \frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} \cdot Q_{\pi}(s, a) \right]
\end{aligned}
$$
The last step assumes that $\frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} \cdot Q_{\pi}(s, a)$ follows a distribution of $\pi(a|s_t; \theta)$ with respect to the random variable $A$.</p><p>Let&rsquo;s take another look at the policy gradient here. First, in practice, when we calculate the expectation we can use Monte Carlo Approximation. The gradient here becomes summations as below:</p><p>$$
\nabla_{\theta}(J(\theta)) = \sum_{t} \nabla_{\theta}{\log\pi (a|s; \theta)} \cdot Q_{\pi}(s, a)
$$</p><p>This is also called Monte Carlo policy gradient. Since gradient is a direction, this formula shows that policy gradient estimation is the direction of the steepest increase in reward/return. When reward is larger, the policy gradient will be larger.</p><h4 id=temporal-difference-td-learning>Temporal Difference (TD) Learning<a hidden class=anchor aria-hidden=true href=#temporal-difference-td-learning>#</a></h4><p>Temporal Difference (TD) learning is one of the core concepts in Reinforcement Learning. Temporal difference algorithm always aims to bring the expected prediction and the new prediction together, thus matching expectations with reality and gradually increasing the accuracy of the entire chain of prediction.</p><p>The most basic version is TD(0) method. Specifically, if our agent is in a current state $s_t$, takes the action $a_t$ and receives the reward $r_t$, then we update our estimate of $V$ following</p><p>$$
V(s_t) \xleftarrow[]{} V(s_t) + \alpha[r_{t+1} + \gamma V(s_{t+1}) ‚Äì V(s_t)]
$$</p><p>Here $r_{t+1} + \gamma V(s_{t+1})$ is TD target and $r_{t+1} + \gamma V(s_{t+1}) ‚Äì V(s_t)$ is called TD error ($\delta$).</p><p>There is SARSA (state-action-reward-state-action), where we replace the value function as the action-state value function.</p><p>$$
Q(s_t, a_t) \xleftarrow[]{} Q(s_t, a_t) + \alpha[r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) ‚Äì Q(s_t, a_t)]
$$</p><p>And TD with Q-learning
$$
Q(s_t, a_t) \xleftarrow[]{} Q(s_t, a_t) + \alpha[r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) ‚Äì Q(s_t, a_t)]
$$</p><h4 id=reinforce>REINFORCE<a hidden class=anchor aria-hidden=true href=#reinforce>#</a></h4><p>Since $Q_{\pi}(s, a)$ is the expectation of the return, we can once again use Monte Carlo approximation,
$$
\begin{aligned}
Q_{\pi}(s_t, a_t) &= u_t \\
&= \sum_{i=t}^{N} {\gamma^{i-t} \cdot r_{i}}
\end{aligned}
$$
The above MCPG actually gives us a practical algorithm to do policy gradient based RL. Let&rsquo;s summarize it as follows:</p><ol><li>Play one episode of game to get the trajectory: $s_1, a_1, r_1, s_2, a_2, r_2, &mldr;$</li><li>Estimate all $q_t \approx u_t$ using above equation</li><li>Differentiate policy network to get $d_{\theta, t}$</li><li>Compute policy gradient $g(a_t, \theta_t) = q_t \cdot d_{\theta, t}$</li></ol><h4 id=advantage-function-and-generalized-advantage-estimation>Advantage Function and Generalized Advantage Estimation<a hidden class=anchor aria-hidden=true href=#advantage-function-and-generalized-advantage-estimation>#</a></h4><p>The above equation is the vanilla policy gradient method. More policy gradient algorithms are proposed later to reduce high variance of the vanilla version. John Schulman&rsquo;s <a href=https://arxiv.org/pdf/1506.02438.pdf>GAE paper</a> summarized all the improvement methods. In the derivation, the policy gradient is represented as
$$
\frac{\partial{V(s; \theta)}}{\partial{\theta}} = \mathbb{E_A}\left[ \frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} \cdot \hat{A_t}(s, a) \right]
$$
where $\hat{A_t}(s, a)$ is the advantage function. In implementation, we construct loss function in a way such that the policy gradient $g$ equals to the above result
$$
L(\theta) = \mathbb{E_t}\left[ \log\pi (a|s; \theta) \hat{A_t}(s, a) \right]
$$</p><p>The idea is that the Advantage function calculates how better taking that action at a state is compared to the average value of the state. It‚Äôs subtracting the mean value of the state from the state action pair. Mathematically, $A(s_t, a_t) = Q(s_t, a_t) ‚àí V (s_t)$, where $Q(s_t, a_t)$ is the action-value function, representing the expected return after taking action at at state $s$, and $V (s_t)$ is the value function, representing the average expected return at state $s_t$.</p><p>Based on the above advantage definition, we have
$$
\begin{aligned}
\hat{A_t^{(1)}} &= r_t + \gamma V(s_{t+1}) - V(s) \\
\hat{A_t^{(2)}} &= r_t + \gamma r_{t+1} +\gamma^2 V(s_{t+2}) - V(s) \\
&mldr;\\
\hat{A_t^{(\infty)}} &= r_t + \gamma r_{t+1} +\gamma^2 r_{t+2} + &mldr; - V(s)
\end{aligned}
$$</p><p>Notice that $\hat{A_t^{(1)}}$ has high bias, low variance, whilst
$\hat{A_t^{(\infty)}}$ is unbiased, high variance. A weighted average of $\hat{A_t^{(k)}}$ can be used to balance bias and variance.
$$\hat{A_t} = \hat{A_t^{GAE}} = \frac{\sum_k w_k \hat{A_t^{(k)}}}{\sum_k w_k}$$
We set $w_k = \lambda^{k-1}$, this gives clean calculation for $\hat{A_t}$. Below we have the recursion equations. (Refer to [11] to learn how to derive the second equation here.)</p><p>$$
\begin{aligned}
\delta_t &= r_t + \gamma V(s_{t+1}) - V(s_t)
\\
\hat{A_t} &= \delta_t + \gamma \lambda \delta_{t+1} + &mldr; +
(\gamma \lambda)^{T - t + 1} \delta_{T - 1}
\\
&= \delta_t + \gamma \lambda \hat{A_{t+1}}
\end{aligned}
$$</p><h4 id=actor-critic-algorithm>Actor-Critic Algorithm<a hidden class=anchor aria-hidden=true href=#actor-critic-algorithm>#</a></h4><p>There we give a recap of how actor-critic method works. In Actor-Critic algorithm, we use one neural network $\pi(a|s; \theta)$ to approximate policy function $\pi(a|s)$ and use another neural network $q(s, a; w)$ to approximate value function $Q_{\pi}(s, a)$.</p><ul><li>Observe state $s_t$, and randomly sample action from policy $a_t \sim \pi(\cdot | s_t; \Theta_t)$</li><li>Let agent perform action $a_t$, and get new state $s_{t+1}$ and reward $r_t$ from environment</li><li>Randomly sample $\tilde{a}_{t+1} \sim \pi(\cdot | s_t; \Theta_t)$ without performing the action</li><li>Evaluate value network: $q_t = q(s_t, a_t; W_t)$ and $q_{t+1} = q(s_{t+1}, \tilde{a}_{t+1}; W_t)$</li><li>Compute TD error: $\delta_t = q_t - (r_t + \gamma \cdot q_{t+1})$</li><li>Differentiate value network: $d_{w,t} = \frac{\partial{q(s_t, a_t, w)}}{\partial{w}}$ (autograd will do this for us)</li><li>Update value network: $ w_{t+1} = w_t - \alpha \cdot \delta_t \cdot d_{w, t}$</li><li>Differentiate policy network: $ d_{\theta, t} = \frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} $ (again autograd will do this for us)</li><li>Update policy network: $\theta_{t+1} = \theta_t + \beta \cdot q_t \cdot d_{\theta, t}$.<ul><li>We can also use: $\theta_{t+1} = \theta_t + \beta \cdot \delta_t \cdot d_{\theta, t}$ to update policy network. This is called policy gradient with baseline.
Essentially, the algorithm alternates between sampling and optimization. The expectation in the above equation indicates that we need to average over a finite batch of empirical samples.</li></ul></li></ul><h4 id=proximal-policy-optimization>Proximal Policy Optimization<a hidden class=anchor aria-hidden=true href=#proximal-policy-optimization>#</a></h4><p>Vanilla policy gradient method uses on-policy update. Concretely, the algorithm samples empirical data from a policy network $\pi_{\theta}$ parameterized with $\theta$. After updating the network itself, the new policy network is $\pi_{\theta_{new}}$ and the old policy $\pi_{\theta}$ is out of use and future sampling will be from $\pi_{\theta_{new}}$. This whole process is not efficient enough. The solution to this is to reuse the old samples to achieve off-policy training. From above importance sampling section, we know that:</p><p>$$
\mathbb{E_{x\sim p(x)}}\left[f \right] = \mathbb{E_{x\sim q(x)}} \left[ \frac{p(x_i)}{q(x_i)}f(x_i) \right]
$$</p><p>Similarly, we can make a change to the objective function of our policy gradient, and the resulting policy gradient will become
$$
\begin{aligned}
g &= \mathbb{E_{{(s_t, a_t)} \sim \pi_{\theta}}}\left[ \frac{\partial{\log\pi (a_t|s_t; \theta)}}{\partial{\theta}} \cdot \hat{A_t}(s, a) \right] \\
&= \mathbb{E_{{(s_t, a_t)} \sim \pi_{\theta_{old}}}}\left[ \frac{\pi_{\theta}(a_t|s_t; \theta)}{\pi_{\theta_{old}}(a_t|s_t)} \frac{\partial{\log\pi (a_t|s_t; \theta)}}{\partial{\theta}} \cdot \hat{A_t}(s, a) \right]
\end{aligned}
$$
Consequently, the loss becomes</p><p>$$
L(\theta) = \mathbb{E_{{(s_t, a_t)} \sim \pi_{\theta_{old}}}}\left[ \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A_t}(s, a) \right]
$$
This is so-called surrogate objective function. In the above section, we mentioned how to use chain rule to get the expectation format of gradient, here we just to reverse the process to get the above loss function.</p><p>In the importance sampling section, we saw that the variance of new distribution could be large when the proposal distribution is not so close to the original distribution. Thus, to deal with this, people add KL diveragence to the loss function to limit the old and new policy difference. Using Largrangian dual method, we can add this constraint to the objective function:</p><p>$$
L(\theta) = \mathbb{E_{{(s_t, a_t)} \sim \pi_{\theta_{old}}}}\left[ \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A_t}(s, a) - \beta KL[\pi_{\theta_{old}}(a_t|s_t), \pi_{\theta}(a_t|s_t)]\right]
$$</p><h3 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h3><p>For language generation task, generating a token is an action. Agent is the target language model we want to train.</p><p>Here we first look at the implementation from Deepspeed-chat model. The actor-critic algorithm requires to load four model in training: actor model, critic model, reference model and reward mdoel. Actor model is the poliy network and critice model is the value network. Reference model and reward model are frozen in training. Reference model is used to contrain the actor model predictions so that they won&rsquo;t divege too much. Reward model gives the current step reward.</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] <a href=https://arxiv.org/pdf/1506.02438.pdf>High-Dimensional Continuous Control Using Generalized Advantage Estimation</a><br>[2] <a href=https://arxiv.org/pdf/1707.06347.pdf>Proximal Policy Optimization Algorithms</a><br>[3] <a href=https://papers.nips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf>Policy Gradient Methods for Reinforcement Learning with Function Approximation</a><br>[4] <a href=https://arxiv.org/abs/1511.06581>Dueling Network Architectures for Deep Reinforcement Learning</a><br>[5] <a href=https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/schedule.html>https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/schedule.html</a><br>[6] <a href=https://github.com/wangshusen/DRL>https://github.com/wangshusen/DRL</a><br>[7] <a href=https://www.davidsilver.uk/teaching/>https://www.davidsilver.uk/teaching/</a><br>[8] <a href=https://arxiv.org/pdf/1909.08593.pdf>Fine-Tuning Language Models from Human Preferences</a><br>[9] <a href=https://zhuanlan.zhihu.com/p/677607581>https://zhuanlan.zhihu.com/p/677607581</a><br>[10] <a href=https://arxiv.org/abs/2308.01320>DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</a><br>[11] <a href=https://arxiv.org/pdf/2307.04964.pdf>Secrets of RLHF in Large Language Models Part I: PPO</a><br>[12] <a href=https://arxiv.org/pdf/2401.06080.pdf>Secrets of RLHF in Large Language Models Part II: Reward Modeling</a><br>[13] <a href=https://arxiv.org/abs/2403.17031>The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization</a><br>[14] <a href=https://arxiv.org/pdf/2404.10719.pdf>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</a><br>[15] <a href=https://arxiv.org/abs/2005.12729>Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO</a><br>[16] <a href="https://difficult-link-dd7.notion.site/eb7b2d1891f44b3a84e7396d19d39e6f?v=01bcb084210149488d730064cbabc99f">Advanced Tricks for Training Large Language Models with Proximal Policy Optimization</a></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/programming_language/python/asyncio/><span class=title>¬´</span><br><span>AsyncIO</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/diffusion/><span class=title>¬ª</span><br><span>Diffusion Probabilistic Models</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2024
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>