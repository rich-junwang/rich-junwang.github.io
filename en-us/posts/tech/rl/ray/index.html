<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ray | Jun's Blog</title>
<meta name=keywords content><meta name=description content="Async Ops in Ray"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/rl/ray/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/rl/ray/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="Ray"><meta property="og:description" content="Async Ops in Ray"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/rl/ray/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-18T00:18:23+08:00"><meta property="article:modified_time" content="2024-06-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ray"><meta name=twitter:description content="Async Ops in Ray"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"📚Articles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"👨🏻‍💻 Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Ray","item":"https://rich-junwang.github.io/en-us/posts/tech/rl/ray/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ray","name":"Ray","description":"Async Ops in Ray","keywords":[""],"articleBody":"Promise and Future Before diving deep into Ray, I’ll first give a brief introduction to the async ops in programming in C++. An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread’s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task’s result. The result of the delegated task is only required at some point in the future when it is needed.\nAn asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.\nPromise and future: worker gets the promise instance and main driver gets the future When an asynchronous call is created, an instance of std::future is returned to the creator of the asynchronous call (receiver). Meanwhile, the executor of the asynchronous call (sender) holds an instance of std::promise. The executor uses std::promise to fulfill its promise (a commitment to deliver the result at a future point in time after the operation is completed), while the creator uses std::future to obtain this future value (the result corresponding to the promise fulfilled in the future). The std::promise instance held by the executor and the std::future instance held by the creator are both connected to a shared object. This shared object establishes a communication channel for information synchronization between the creator and the executor of the asynchronous call, enabling both parties to exchange information about the execution status of the asynchronous operation through this channel.\nThe executor accesses this channel via its std::promise instance to write values into the channel. The creator uses its std::future instance to retrieve values from the channel. Once the executor completes the execution of the asynchronous operation, it writes the result of the operation into the channel via the std::promise instance. The creator then retrieves the result of the asynchronous operation through its std::future instance.\n#include #include #include #include #include #include int main() { // 创建一个promise对象实例 std::promise\u003cint\u003e _promise; // 从promise对象实例中获取对应的future对象实例 std::future\u003cint\u003e _future = _promise.get_future(); // 构建测试对象 std::vector\u003cint\u003e test_data = {1, 2, 3, 4, 5, 6}; // 创建一个任务，该任务对容器内的所有元素求和，求和完成之后通过promise来返回结果 auto sum_of_data = [](const std::vector\u003cint\u003e\u0026 data, std::promise\u003cint\u003e prom) { int sum = std::accumulate(data.begin(), data.end(), 0); std::this_thread::sleep_for(std::chrono::milliseconds(500)); // 休眠500ms prom.set_value(sum); // 完成承诺，将结果写入通道中 }; // 将这个任务交给一个线程，进行异步执行 std::thread work_thread(sum_of_data, test_data, std::move(_promise)); // 通过future的方法来获取异步调用的结果 std::cout \u003c\u003c \"async result is \" \u003c\u003c _future.get() \u003c\u003c std::endl; // 等待work_thread退出销毁完成 work_thread.join(); return 0; } // output // async result is 21 Introduction Ray was designed with training RL system in mind. There are three kinds of tasks in RL system training\nSimulation where agents interact with environments and environments omit responses Evaluation where agents generate rollout trajectory Training where policy is updated for improvement To handle these heterogenous tasks, Ray abstracts two kinds of computation:\nStateless task: functions decorated with @ray.remote in python Stateful actor: classes decorated with @ray.remote in python Below is one example of task computation in Ray.\n# Ray task example import ray ray.init() @ray.remote def func(x): return x ** 2 # driver process if __name__ == \"__main__\": # create 2 workers, execute func remotely, return 2 futures which each points to a remote op futures = [func.remote(i) for i in range(2)] # blocking results = ray.get(futures)) print(f\"The final result is: {results}\") # [0, 1] Ray Actor Declare an actor by annotating a class with @ray.remote, just like declaring a task from a function. Ray actor is different from python class in the following ways:\nAdd accessor methods for any data members that you need to read or write, because using direct access, such as my_game.state, doesn’t work for actors. Construct actor instances with my_instance = MyClass.remote(…). Call methods with my_instance.some_method.remote(…). Use ray.get() and ray.wait() to retrieve results, just like you do for task results. import ray @ray.remote class Counter: def __init__(self): self.value = 0 def increment(self): self.value += 1 return self.value # define the accessor as we can't use direct access def get_counter(self): return self.value # Create an actor from this class. counter = Counter.remote() # Call the actor. obj_ref = counter.increment.remote() print(ray.get(obj_ref)) # Create ten Counter actors. counters = [Counter.remote() for _ in range(10)] # Increment each Counter once and get the results. These tasks all happen in # parallel. results = ray.get([c.increment.remote() for c in counters]) print(results) # Increment the first Counter five times. These tasks are executed serially # and share state. results = ray.get([counters[0].increment.remote() for _ in range(5)]) print(results) Ray Architecture Ray is a general-purpose framework for parallel programming on a cluster.\nRay Architecture Ray Tips Delay ray.get() With Ray, the invocation of every remote operation (e.g., task, actor method) is asynchronous. This means that the operation returns immediately a promise/future, which is essentially an identifier (ID) of the operation’s result. This is key to achieve parallelism, as it allows the driver program to launch multiple operations in parallel. To get the actual results, the programmer needs to call ray.get() on the IDs of the results. This call blocks until the results are available. As a side effect, this operation also blocks the driver program from invoking other operations, which can hurt parallelism.\nimport time def do_some_work(x): time.sleep(1) # Replace this with work you need to do. return x start = time.time() results = [do_some_work(x) for x in range(4)] print(\"duration =\", time.time() - start, \"\\nresults = \", results) # Above output, the program takes around 4 ses: # duration = 4.0149290561676025 # results = [0, 1, 2, 3] start = time.time() results = [do_some_work.remote(x) for x in range(4)] print(\"duration =\", time.time() - start, \"\\nresults = \", results) # Above output, # duration = 0.0003619194030761719 # results = [ObjectID(0100000000bdf683fc3e45db42685232b19d2a61), ObjectID(01000000da69c40e1c2f43b391443ce23de46cda), ObjectID(010000007fe0954ac2b3c0ab991538043e8f37e0), ObjectID(01000000cf47d5ecd1e26b42624454c795abe89b)] start = time.time() results = [ray.get(do_some_work.remote(x)) for x in range(4)] print(\"duration =\", time.time() - start, \"\\nresults = \", results) # Above output, # duration = 4.018050909042358 # results = [0, 1, 2, 3] results = ray.get([do_some_work.remote(x) for x in range(4)]) # output # duration = 1.0064549446105957 # results = [0, 1, 2, 3] Note that ray.get() is blocking, so calling it after each remote operation means that we wait for that operation to complete, which essentially means that we execute one operation at a time, hence no parallelism! To enable parallelism, we need to call ray.get() after invoking all tasks. We can easily do so in our example by replacing line “results = [do_some_work.remote(x) for x in range(4)]” with:\nAvoid passing same object repeatedly to remote tasks When we pass a large object as an argument to a remote function, Ray calls ray.put() under the hood to store that object in the local object store. This can significantly improve the performance of a remote task invocation when the remote task is executed locally, as all local tasks share the object store. However, there are cases when automatically calling ray.put() on a task invocation leads to performance issues. One example is passing the same large object as an argument repeatedly, as illustrated by the program below:\nimport time import numpy as np import ray ray.init(num_cpus = 4) @ray.remote def no_work(a): return start = time.time() a = np.zeros((10000, 2000)) result_ids = [no_work.remote(a) for x in range(10)] results = ray.get(result_ids) print(\"duration =\", time.time() - start) # output # duration = 1.0699057579040527 The right way is to put the shared object in the object store as shown below.\nimport time import numpy as np import ray ray.init(num_cpus = 4) @ray.remote def no_work(a): return start = time.time() a_id = ray.put(np.zeros((10000, 2000))) result_ids = [no_work.remote(a_id) for x in range(10)] results = ray.get(result_ids) print(\"duration =\", time.time() - start) # output # duration = 0.12425804138183594 Pipeline data processing If we use ray.get() on the results of multiple tasks we will have to wait until the last one of these tasks finishes. This can be an issue if tasks take widely different amounts of time. To illustrate this issue, consider the following example where we run four do_some_work() tasks in parallel, with each task taking a time uniformly distributed between 0 and 4 sec. Next, assume the results of these tasks are processed by process_results(), which takes 1 sec per result. The expected running time is then (1) the time it takes to execute the slowest of the do_some_work() tasks, plus (2) 4 sec which is the time it takes to execute process_results().\nimport time import random import ray ray.init(num_cpus = 4) @ray.remote def do_some_work(x): time.sleep(random.uniform(0, 4)) # Replace this with work you need to do. return x def process_results(results): sum = 0 for x in results: time.sleep(1) # Replace this with some processing code. sum += x return sum start = time.time() data_list = ray.get([do_some_work.remote(x) for x in range(4)]) sum = process_results(data_list) print(\"duration =\", time.time() - start, \"\\nresult = \", sum) # output # duration = 7.82636022567749 # result = 6 In order to process data as soon as it’s available, we can use the pipeline to remove the waiting time. Ray allows us to do exactly this by calling ray.wait() on a list of object IDs. Without specifying any other parameters, this function returns as soon as an object in its argument list is ready. This call has two returns: (1) the ID of the ready object, and (2) the list containing the IDs of the objects not ready yet. The modified program is below.\nimport time import random import ray ray.init(num_cpus = 4) @ray.remote def do_some_work(x): time.sleep(random.uniform(0, 4)) # Replace this with work you need to do. return x def process_incremental(sum, result): time.sleep(1) # Replace this with some processing code. return sum + result start = time.time() result_ids = [do_some_work.remote(x) for x in range(4)] sum = 0 while len(result_ids): done_id, result_ids = ray.wait(result_ids) sum = process_incremental(sum, ray.get(done_id[0])) print(\"duration =\", time.time() - start, \"\\nresult = \", sum) #output # duration = 4.852453231811523 # result = 6 Pipeline execution, image from 1 A full example on how to use Ray for distributed neural network training\nimport torch import torch.nn as nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor from ray import train from ray.train.torch import TorchTrainer from ray.train import ScalingConfig def get_dataset(): return datasets.FashionMNIST( root=\"/tmp/data\", train=True, download=True, transform=ToTensor(), ) class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, inputs): inputs = self.flatten(inputs) logits = self.linear_relu_stack(inputs) return logits # without distributed training, pure pytorch def train_func(): num_epochs = 3 batch_size = 64 dataset = get_dataset() dataloader = DataLoader(dataset, batch_size=batch_size) model = NeuralNetwork() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(num_epochs): for inputs, labels in dataloader: optimizer.zero_grad() pred = model(inputs) loss = criterion(pred, labels) loss.backward() optimizer.step() print(f\"epoch: {epoch}, loss: {loss.item()}\") # train_func() # distributed training def train_func_distributed(): num_epochs = 3 batch_size = 64 dataset = get_dataset() dataloader = DataLoader(dataset, batch_size=batch_size) dataloader = train.torch.prepare_data_loader(dataloader) model = NeuralNetwork() model = train.torch.prepare_model(model) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(num_epochs): for inputs, labels in dataloader: optimizer.zero_grad() pred = model(inputs) loss = criterion(pred, labels) loss.backward() optimizer.step() print(f\"epoch: {epoch}, loss: {loss.item()}\") # For GPU Training, set `use_gpu` to True. use_gpu = False trainer = TorchTrainer( train_func_distributed, scaling_config=ScalingConfig(num_workers=4, use_gpu=use_gpu) ) results = trainer.fit() References https://rise.cs.berkeley.edu/blog/ray-tips-for-first-time-users/ Ray: A Distributed Framework for Emerging AI Applications https://github.com/dmatrix/ray-core-tutorial Ray Whitepaper ","wordCount":"2067","inLanguage":"en-us","datePublished":"2024-06-18T00:18:23+08:00","dateModified":"2024-06-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/rl/ray/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="🏠 Home"><span>🏠 Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="🙋🏻‍♂️ About"><span>🙋🏻‍♂️ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="📚 Posts"><span>📚 Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="🧩 Tags"><span>🧩 Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="⏱️ Archives"><span>⏱️ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="🔍 Search (Alt + /)" accesskey=/><span>🔍 Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>🏠 Home</a>&nbsp;»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>📚Articles</a>&nbsp;»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>👨🏻‍💻 Tech</a></div><h1 class=post-title>Ray</h1><div class=post-description>Async Ops in Ray</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-06-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>2067 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>5 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/ml/ style=color:var(--secondary)!important>ML</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#promise-and-future aria-label="Promise and Future">Promise and Future</a></li><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#ray-actor aria-label="Ray Actor">Ray Actor</a><ul><li><a href=#ray-architecture aria-label="Ray Architecture">Ray Architecture</a></li></ul></li><li><a href=#ray-tips aria-label="Ray Tips">Ray Tips</a><ul><li><a href=#delay-rayget aria-label="Delay ray.get()">Delay ray.get()</a></li><li><a href=#avoid-passing-same-object-repeatedly-to-remote-tasks aria-label="Avoid passing same object repeatedly to remote tasks">Avoid passing same object repeatedly to remote tasks</a></li><li><a href=#pipeline-data-processing aria-label="Pipeline data processing">Pipeline data processing</a></li><li><a href=#references aria-label=References>References</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=promise-and-future>Promise and Future<a hidden class=anchor aria-hidden=true href=#promise-and-future>#</a></h2><p>Before diving deep into Ray, I&rsquo;ll first give a brief introduction to the async ops in programming in C++.
An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread&rsquo;s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task&rsquo;s result. The result of the delegated task is only required at some point in the future when it is needed.</p><p>An asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.</p><p align=center><img alt="Promise and future" src=images/image.png width=90%>
<em>Promise and future: worker gets the promise instance and main driver gets the future</em><br></p><p>When an asynchronous call is created, an instance of std::future is returned to the creator of the asynchronous call (receiver). Meanwhile, the executor of the asynchronous call (sender) holds an instance of std::promise. The executor uses std::promise to fulfill its promise (a commitment to deliver the result at a future point in time after the operation is completed), while the creator uses std::future to obtain this future value (the result corresponding to the promise fulfilled in the future). The std::promise instance held by the executor and the std::future instance held by the creator are both connected to a shared object. This shared object establishes a communication channel for information synchronization between the creator and the executor of the asynchronous call, enabling both parties to exchange information about the execution status of the asynchronous operation through this channel.</p><p>The executor accesses this channel via its std::promise instance to write values into the channel. The creator uses its std::future instance to retrieve values from the channel. Once the executor completes the execution of the asynchronous operation, it writes the result of the operation into the channel via the std::promise instance. The creator then retrieves the result of the asynchronous operation through its std::future instance.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;iostream&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;thread&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;future&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;vector&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;numeric&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;chrono&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 创建一个promise对象实例
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>promise<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span> _promise;
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 从promise对象实例中获取对应的future对象实例
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>future<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span> _future <span style=color:#f92672>=</span> _promise.get_future();
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 构建测试对象
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span> test_data <span style=color:#f92672>=</span> {<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>};
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 创建一个任务，该任务对容器内的所有元素求和，求和完成之后通过promise来返回结果
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>auto</span> sum_of_data <span style=color:#f92672>=</span> [](<span style=color:#66d9ef>const</span> std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;&amp;</span> data, std<span style=color:#f92672>::</span>promise<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span> prom) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>int</span> sum <span style=color:#f92672>=</span> std<span style=color:#f92672>::</span>accumulate(data.begin(), data.end(), <span style=color:#ae81ff>0</span>);
</span></span><span style=display:flex><span>        std<span style=color:#f92672>::</span>this_thread<span style=color:#f92672>::</span>sleep_for(std<span style=color:#f92672>::</span>chrono<span style=color:#f92672>::</span>milliseconds(<span style=color:#ae81ff>500</span>)); <span style=color:#75715e>// 休眠500ms
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        prom.set_value(sum); <span style=color:#75715e>// 完成承诺，将结果写入通道中
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    };
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 将这个任务交给一个线程，进行异步执行
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span><span style=color:#66d9ef>thread</span> work_thread(sum_of_data, test_data, std<span style=color:#f92672>::</span>move(_promise));
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 通过future的方法来获取异步调用的结果
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>cout <span style=color:#f92672>&lt;&lt;</span> <span style=color:#e6db74>&#34;async result is &#34;</span> <span style=color:#f92672>&lt;&lt;</span> _future.get() <span style=color:#f92672>&lt;&lt;</span> std<span style=color:#f92672>::</span>endl;
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 等待work_thread退出销毁完成
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    work_thread.join();
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// output
</span></span></span><span style=display:flex><span><span style=color:#75715e>// async result is 21
</span></span></span></code></pre></div><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Ray was designed with training RL system in mind. There are three kinds of tasks in RL system training</p><ul><li>Simulation where agents interact with environments and environments omit responses</li><li>Evaluation where agents generate rollout trajectory</li><li>Training where policy is updated for improvement</li></ul><p>To handle these heterogenous tasks, Ray abstracts two kinds of computation:</p><ul><li>Stateless task: functions decorated with <code>@ray.remote</code> in python</li><li>Stateful actor: classes decorated with <code>@ray.remote</code> in python</li></ul><p>Below is one example of task computation in Ray.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Ray task example</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>func</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># driver process</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># create 2 workers, execute func remotely, return 2 futures which each points to a remote op</span>
</span></span><span style=display:flex><span>    futures <span style=color:#f92672>=</span> [func<span style=color:#f92672>.</span>remote(i) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>2</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># blocking</span>
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get(futures)) 
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;The final result is: </span><span style=color:#e6db74>{</span>results<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># [0, 1]</span>
</span></span></code></pre></div><h2 id=ray-actor>Ray Actor<a hidden class=anchor aria-hidden=true href=#ray-actor>#</a></h2><p>Declare an actor by annotating a class with @ray.remote, just like declaring a task from a function. Ray actor is different from python class in the following ways:</p><blockquote><ol><li>Add accessor methods for any data members that you need to read or write, because using direct access, such as my_game.state, doesn&rsquo;t work for actors.</li><li>Construct actor instances with my_instance = MyClass.remote(&mldr;).</li><li>Call methods with my_instance.some_method.remote(&mldr;).</li><li>Use ray.get() and ray.wait() to retrieve results, just like you do for task results.</li></ol></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> ray
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Counter</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>value <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>increment</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>value <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>value
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># define the accessor as we can&#39;t use direct access</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_counter</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>value
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create an actor from this class.</span>
</span></span><span style=display:flex><span>counter <span style=color:#f92672>=</span> Counter<span style=color:#f92672>.</span>remote()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Call the actor.</span>
</span></span><span style=display:flex><span>obj_ref <span style=color:#f92672>=</span> counter<span style=color:#f92672>.</span>increment<span style=color:#f92672>.</span>remote()
</span></span><span style=display:flex><span>print(ray<span style=color:#f92672>.</span>get(obj_ref))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create ten Counter actors.</span>
</span></span><span style=display:flex><span>counters <span style=color:#f92672>=</span> [Counter<span style=color:#f92672>.</span>remote() <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Increment each Counter once and get the results. These tasks all happen in</span>
</span></span><span style=display:flex><span><span style=color:#75715e># parallel.</span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get([c<span style=color:#f92672>.</span>increment<span style=color:#f92672>.</span>remote() <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> counters])
</span></span><span style=display:flex><span>print(results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Increment the first Counter five times. These tasks are executed serially</span>
</span></span><span style=display:flex><span><span style=color:#75715e># and share state.</span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get([counters[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>increment<span style=color:#f92672>.</span>remote() <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>5</span>)])
</span></span><span style=display:flex><span>print(results)
</span></span></code></pre></div><h3 id=ray-architecture>Ray Architecture<a hidden class=anchor aria-hidden=true href=#ray-architecture>#</a></h3><p>Ray is a general-purpose framework for parallel programming on a cluster.</p><p align=center><img alt="Ray Architecture" src=images/ray_architecture.png width=90%>
Ray Architecture<br></p><h2 id=ray-tips>Ray Tips<a hidden class=anchor aria-hidden=true href=#ray-tips>#</a></h2><h3 id=delay-rayget>Delay ray.get()<a hidden class=anchor aria-hidden=true href=#delay-rayget>#</a></h3><p>With Ray, the invocation of every remote operation (e.g., task, actor method) is asynchronous. This means that the operation returns <em>immediately</em> a promise/future, which is essentially an identifier (ID) of the operation’s result. This is key to achieve parallelism, as it allows the driver program to launch multiple operations in parallel. To get the actual results, the programmer needs to call ray.get() on the IDs of the results. This call blocks until the results are available. As a side effect, this operation also blocks the driver program from invoking other operations, which can hurt parallelism.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>do_some_work</span>(x):
</span></span><span style=display:flex><span>    time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>1</span>) <span style=color:#75715e># Replace this with work you need to do.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> [do_some_work(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>results = &#34;</span>, results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Above output, the program takes around 4 ses:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 4.0149290561676025 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># results =  [0, 1, 2, 3]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> [do_some_work<span style=color:#f92672>.</span>remote(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>results = &#34;</span>, results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Above output, </span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 0.0003619194030761719 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># results =  [ObjectID(0100000000bdf683fc3e45db42685232b19d2a61), ObjectID(01000000da69c40e1c2f43b391443ce23de46cda), ObjectID(010000007fe0954ac2b3c0ab991538043e8f37e0), ObjectID(01000000cf47d5ecd1e26b42624454c795abe89b)]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> [ray<span style=color:#f92672>.</span>get(do_some_work<span style=color:#f92672>.</span>remote(x)) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>results = &#34;</span>, results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Above output, </span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 4.018050909042358 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># results =  [0, 1, 2, 3]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get([do_some_work<span style=color:#f92672>.</span>remote(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)])
</span></span><span style=display:flex><span><span style=color:#75715e># output</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 1.0064549446105957 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># results =  [0, 1, 2, 3]</span>
</span></span></code></pre></div><p>Note that <code>ray.get()</code> is blocking, so calling it after each remote operation means that we wait for that operation to complete, which essentially means that we execute one operation at a time, hence no parallelism!
To enable parallelism, we need to call ray.get() <em>after</em> invoking all tasks. We can easily do so in our example by replacing line “results = [do_some_work.remote(x) for x in range(4)]” with:</p><h3 id=avoid-passing-same-object-repeatedly-to-remote-tasks>Avoid passing same object repeatedly to remote tasks<a hidden class=anchor aria-hidden=true href=#avoid-passing-same-object-repeatedly-to-remote-tasks>#</a></h3><p>When we pass a large object as an argument to a remote function, Ray calls ray.put() under the hood to store that object in the local object store. This can significantly improve the performance of a remote task invocation when the remote task is executed locally, as all local tasks share the object store. However, there are cases when automatically calling ray.put() on a task invocation leads to performance issues. One example is passing the same large object as an argument <em>repeatedly</em>, as illustrated by the program below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init(num_cpus <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>no_work</span>(a): 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time() 
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>2000</span>)) 
</span></span><span style=display:flex><span>result_ids <span style=color:#f92672>=</span> [no_work<span style=color:#f92672>.</span>remote(a) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)] 
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get(result_ids) 
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start) 
</span></span><span style=display:flex><span><span style=color:#75715e># output</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 1.0699057579040527 </span>
</span></span></code></pre></div><p>The right way is to put the shared object in the object store as shown below.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init(num_cpus <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>no_work</span>(a): 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time() 
</span></span><span style=display:flex><span>a_id <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>put(np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>2000</span>))) 
</span></span><span style=display:flex><span>result_ids <span style=color:#f92672>=</span> [no_work<span style=color:#f92672>.</span>remote(a_id) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)] 
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get(result_ids) 
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start) 
</span></span><span style=display:flex><span><span style=color:#75715e># output</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 0.12425804138183594</span>
</span></span></code></pre></div><h3 id=pipeline-data-processing>Pipeline data processing<a hidden class=anchor aria-hidden=true href=#pipeline-data-processing>#</a></h3><p>If we use ray.get() on the results of multiple tasks we will have to wait until the <em>last</em> one of these tasks finishes. This can be an issue if tasks take widely different amounts of time. To illustrate this issue, consider the following example where we run four do_some_work() tasks in parallel, with each task taking a time uniformly distributed between 0 and 4 sec. Next, assume the results of these tasks are processed by process_results(), which takes 1 sec per result. The expected running time is then (1) the time it takes to execute the slowest of the do_some_work() tasks, plus (2) 4 sec which is the time it takes to execute process_results().</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init(num_cpus <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>do_some_work</span>(x): 
</span></span><span style=display:flex><span>    time<span style=color:#f92672>.</span>sleep(random<span style=color:#f92672>.</span>uniform(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>4</span>)) <span style=color:#75715e># Replace this with work you need to do. </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process_results</span>(results): 
</span></span><span style=display:flex><span>    sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> results: 
</span></span><span style=display:flex><span>        time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>1</span>) <span style=color:#75715e># Replace this with some processing code. </span>
</span></span><span style=display:flex><span>        sum <span style=color:#f92672>+=</span> x 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sum 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time() 
</span></span><span style=display:flex><span>data_list <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get([do_some_work<span style=color:#f92672>.</span>remote(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]) 
</span></span><span style=display:flex><span>sum <span style=color:#f92672>=</span> process_results(data_list) 
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>result = &#34;</span>, sum) 
</span></span><span style=display:flex><span><span style=color:#75715e># output</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 7.82636022567749</span>
</span></span><span style=display:flex><span><span style=color:#75715e># result = 6</span>
</span></span></code></pre></div><p>In order to process data as soon as it&rsquo;s available, we can use the pipeline to remove the waiting time. Ray allows us to do exactly this by calling ray.wait() on a list of object IDs. Without specifying any other parameters, this function returns as soon as an object in its argument list is ready. This call has two returns: (1) the ID of the ready object, and (2) the list containing the IDs of the objects not ready yet. The modified program is below.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init(num_cpus <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>do_some_work</span>(x): 
</span></span><span style=display:flex><span>    time<span style=color:#f92672>.</span>sleep(random<span style=color:#f92672>.</span>uniform(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>4</span>)) <span style=color:#75715e># Replace this with work you need to do. </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process_incremental</span>(sum, result): 
</span></span><span style=display:flex><span>    time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>1</span>) <span style=color:#75715e># Replace this with some processing code. </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sum <span style=color:#f92672>+</span> result 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time() 
</span></span><span style=display:flex><span>result_ids <span style=color:#f92672>=</span> [do_some_work<span style=color:#f92672>.</span>remote(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)] 
</span></span><span style=display:flex><span>sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> len(result_ids): 
</span></span><span style=display:flex><span>    done_id, result_ids <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>wait(result_ids) 
</span></span><span style=display:flex><span>    sum <span style=color:#f92672>=</span> process_incremental(sum, ray<span style=color:#f92672>.</span>get(done_id[<span style=color:#ae81ff>0</span>])) 
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>result = &#34;</span>, sum) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#output </span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 4.852453231811523 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># result = 6 </span>
</span></span></code></pre></div><p align=center><img alt="Pipeline execution" src=images/ray_pipeline.png width=90%>
Pipeline execution, image from 1<br></p><p>A full example on how to use Ray for distributed neural network training</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision <span style=color:#f92672>import</span> datasets
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision.transforms <span style=color:#f92672>import</span> ToTensor
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray <span style=color:#f92672>import</span> train
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.train.torch <span style=color:#f92672>import</span> TorchTrainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.train <span style=color:#f92672>import</span> ScalingConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_dataset</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> datasets<span style=color:#f92672>.</span>FashionMNIST(
</span></span><span style=display:flex><span>        root<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/tmp/data&#34;</span>,
</span></span><span style=display:flex><span>        train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        transform<span style=color:#f92672>=</span>ToTensor(),
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NeuralNetwork</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>flatten <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Flatten()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear_relu_stack <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>28</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>512</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>512</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>10</span>),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, inputs):
</span></span><span style=display:flex><span>        inputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>flatten(inputs)
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear_relu_stack(inputs)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> logits
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># without distributed training, pure pytorch</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_func</span>():
</span></span><span style=display:flex><span>    num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dataset <span style=color:#f92672>=</span> get_dataset()
</span></span><span style=display:flex><span>    dataloader <span style=color:#f92672>=</span> DataLoader(dataset, batch_size<span style=color:#f92672>=</span>batch_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> NeuralNetwork()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> inputs, labels <span style=color:#f92672>in</span> dataloader:
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>            pred <span style=color:#f92672>=</span> model(inputs)
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> criterion(pred, labels)
</span></span><span style=display:flex><span>            loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;epoch: </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>item()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># train_func()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># distributed training</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_func_distributed</span>():
</span></span><span style=display:flex><span>    num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dataset <span style=color:#f92672>=</span> get_dataset()
</span></span><span style=display:flex><span>    dataloader <span style=color:#f92672>=</span> DataLoader(dataset, batch_size<span style=color:#f92672>=</span>batch_size)
</span></span><span style=display:flex><span>    dataloader <span style=color:#f92672>=</span> train<span style=color:#f92672>.</span>torch<span style=color:#f92672>.</span>prepare_data_loader(dataloader)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> NeuralNetwork()
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> train<span style=color:#f92672>.</span>torch<span style=color:#f92672>.</span>prepare_model(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> inputs, labels <span style=color:#f92672>in</span> dataloader:
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>            pred <span style=color:#f92672>=</span> model(inputs)
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> criterion(pred, labels)
</span></span><span style=display:flex><span>            loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;epoch: </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>item()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># For GPU Training, set `use_gpu` to True.</span>
</span></span><span style=display:flex><span>use_gpu <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> TorchTrainer(
</span></span><span style=display:flex><span>    train_func_distributed,
</span></span><span style=display:flex><span>    scaling_config<span style=color:#f92672>=</span>ScalingConfig(num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, use_gpu<span style=color:#f92672>=</span>use_gpu)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> trainer<span style=color:#f92672>.</span>fit()
</span></span></code></pre></div><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ol><li><a href=https://rise.cs.berkeley.edu/blog/ray-tips-for-first-time-users/>https://rise.cs.berkeley.edu/blog/ray-tips-for-first-time-users/</a></li><li><a href=https://arxiv.org/abs/1712.05889>Ray: A Distributed Framework for Emerging AI Applications</a></li><li><a href=https://github.com/dmatrix/ray-core-tutorial>https://github.com/dmatrix/ray-core-tutorial</a></li><li><a href="https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview?tab=t.0#heading=h.iyrm5j2gcdoq">Ray Whitepaper</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/diffusion/><span class=title>«</span><br><span>Diffusion Probabilistic Models</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/programming_language/python/pipe/><span class=title>»</span><br><span>Pipe in Multiprocessing</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'👉Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'👇Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

————————————————\r
版权声明：本文为「Jun's Blog」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\r
原文链接：`+location.href,s=window.getSelection().toString()+`\r

————————————————\r
版权声明：本文为「Jun's Blog」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\r
原文链接：`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
————————————————\r
版权声明：本文为「Jun's Blog」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\r
原文链接：`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>