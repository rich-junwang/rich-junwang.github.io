<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>VLM | Jun's Blog</title><meta name=keywords content><meta name=description content="ViT
ViT (Vision Transformer) flattens the input image into a 2D sequence of patches (e.g., 16x16), and then uses a linear projection layer to convert each patch into a fixed-length feature vector‚Äîsimilar to word embeddings in natural language processing. Additionally, each patch is assigned a positional index, which is mapped to a positional vector through a positional embedding layer. The patch embeddings and positional embeddings are summed together to form the input to the Transformer encoder, resembling the design of the BERT model.
  Figure 1. ViT 
ViT has a BERT encoder based architecture and it uses a learnable [CLS] token to obtain a representation of the entire image, which is then passed through a fully connected layer to serve downstream classification tasks. When pretrained on large-scale datasets and transferred to medium- or small-scale image recognition benchmarks (e.g., ImageNet, CIFAR-100, VTAB), ViT achieves better performance than CNN-based models, while significantly reducing computational resource requirements during training."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vlm/><link crossorigin=anonymous href=../../../../../../assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../../../../assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vlm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="VLM"><meta property="og:description" content="ViT
ViT (Vision Transformer) flattens the input image into a 2D sequence of patches (e.g., 16x16), and then uses a linear projection layer to convert each patch into a fixed-length feature vector‚Äîsimilar to word embeddings in natural language processing. Additionally, each patch is assigned a positional index, which is mapped to a positional vector through a positional embedding layer. The patch embeddings and positional embeddings are summed together to form the input to the Transformer encoder, resembling the design of the BERT model.
  Figure 1. ViT 
ViT has a BERT encoder based architecture and it uses a learnable [CLS] token to obtain a representation of the entire image, which is then passed through a fully connected layer to serve downstream classification tasks. When pretrained on large-scale datasets and transferred to medium- or small-scale image recognition benchmarks (e.g., ImageNet, CIFAR-100, VTAB), ViT achieves better performance than CNN-based models, while significantly reducing computational resource requirements during training."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vlm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-18T00:18:23+08:00"><meta property="article:modified_time" content="2024-05-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="VLM"><meta name=twitter:description content="ViT
ViT (Vision Transformer) flattens the input image into a 2D sequence of patches (e.g., 16x16), and then uses a linear projection layer to convert each patch into a fixed-length feature vector‚Äîsimilar to word embeddings in natural language processing. Additionally, each patch is assigned a positional index, which is mapped to a positional vector through a positional embedding layer. The patch embeddings and positional embeddings are summed together to form the input to the Transformer encoder, resembling the design of the BERT model.
  Figure 1. ViT 
ViT has a BERT encoder based architecture and it uses a learnable [CLS] token to obtain a representation of the entire image, which is then passed through a fully connected layer to serve downstream classification tasks. When pretrained on large-scale datasets and transferred to medium- or small-scale image recognition benchmarks (e.g., ImageNet, CIFAR-100, VTAB), ViT achieves better performance than CNN-based models, while significantly reducing computational resource requirements during training."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"VLM","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vlm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"VLM","name":"VLM","description":"ViT ViT (Vision Transformer) flattens the input image into a 2D sequence of patches (e.g., 16x16), and then uses a linear projection layer to convert each patch into a fixed-length feature vector‚Äîsimilar to word embeddings in natural language processing. Additionally, each patch is assigned a positional index, which is mapped to a positional vector through a positional embedding layer. The patch embeddings and positional embeddings are summed together to form the input to the Transformer encoder, resembling the design of the BERT model.\nFigure 1. ViT ViT has a BERT encoder based architecture and it uses a learnable [CLS] token to obtain a representation of the entire image, which is then passed through a fully connected layer to serve downstream classification tasks. When pretrained on large-scale datasets and transferred to medium- or small-scale image recognition benchmarks (e.g., ImageNet, CIFAR-100, VTAB), ViT achieves better performance than CNN-based models, while significantly reducing computational resource requirements during training.\n","keywords":[],"articleBody":"ViT ViT (Vision Transformer) flattens the input image into a 2D sequence of patches (e.g., 16x16), and then uses a linear projection layer to convert each patch into a fixed-length feature vector‚Äîsimilar to word embeddings in natural language processing. Additionally, each patch is assigned a positional index, which is mapped to a positional vector through a positional embedding layer. The patch embeddings and positional embeddings are summed together to form the input to the Transformer encoder, resembling the design of the BERT model.\nFigure 1. ViT ViT has a BERT encoder based architecture and it uses a learnable [CLS] token to obtain a representation of the entire image, which is then passed through a fully connected layer to serve downstream classification tasks. When pretrained on large-scale datasets and transferred to medium- or small-scale image recognition benchmarks (e.g., ImageNet, CIFAR-100, VTAB), ViT achieves better performance than CNN-based models, while significantly reducing computational resource requirements during training.\nThe key to ViT‚Äôs success lies in large-scale pretraining. Without this step, directly training a ViT model on standard open datasets would still result in performance inferior to CNNs, which benefit from stronger inductive biases.\nTraining datasets are supervised datasets including: ImageNet 1.3M images, ImageNet-21k, 21k classes and 14 images, JFT dataset, 18k classes and 303M images. The [CLS] token is used for classification task at training time.\nCLIP The core idea of CLIP is to align visual and natural language representations through contrastive learning. CLIP first extracts features separately from text and images. The text encoder is a pretrained BERT, while the image encoder can be a traditional CNN model or a ViT (Vision Transformer) model. After obtaining the image and text embeddings, their features are normalized, and cosine similarity is calculated between all image-text pairs in the batch. A contrastive loss function‚Äîsuch as triple Loss or InfoNCE Loss is used to pull together positive pairs and push apart negative pairs.\nFigure 2. CLIP After pretraining on a large number of image-text pairs, CLIP learns a shared representation space for both the text and image encoders. There are typically two types of downstream applications:\nFine-tuning the model on downstream tasks to adapt it for specific image-text matching tasks. In this case, the text or image encoder can also be used for unimodal tasks. Zero-shot learning, where the pretrained image-text representations are directly used without any further training. Zero-shot usage of CLIP is shown in diagrams (2) and (3). For an image classification task, each candidate class is converted into a textual prompt using a template like ‚ÄúA photo of a {object}‚Äù, where object is the candidate class. For a given image to be classified, CLIP extracts a visual embedding via the image encoder and compares it to the embeddings of all candidate text prompts obtained from the text encoder. The class with the highest similarity is selected as the prediction.\nDue to its simple architecture and outstanding performance, CLIP has been widely adopted in many subsequent works, with its pretrained backbone often used as an initialization for visual representation modules.\nBLIP-2 The core idea of BLIP-2 is to enhance multimodal performance and reduce training costs by leveraging pretrained vision and language models. The architecture consists of a vision encoder, a vision-language adapter (Q-Former), and a large language model (LLM) layer.\nFigure 3. Blip-2 Architecture Vision Encoder: Uses a ViT model with weights initialized through CLIP pretraining. The final enhancement layer (which enriches output features) is removed. During training, the weights are frozen and not updated. LLM: Early versions of BLIP-2 used OPT/FlanT5 to experiment with decoder-based and encoder-decoder-based LLMs. This part is also frozen during training and not updated. Q-Former: The Q-Former is a module to compress the visual feature sequence and to extract key visual information via learned query vectors and feeds it to the LLM. The size of learnable queries is like 32 x 768 in the paper. This component holds most of the trainable parameters during multimodal model training. Figure 4. Blip-2 Q-former VLM Qwen-VL Qwen-VL is a classic large multimodal models. Qwen-VL uses the Qwen-7B LLM as the language backbone, OpenCLIP-pretrained ViT-bigG as the visual feature encoder, and a randomly initialized single-layer cross-attention module as the adapter between vision and language. The total number of parameters is approximately 9.6 billion.\nAs shown in the figure, Qwen-VL‚Äôs training process is divided into three stages:\nFigure 5. Qwen-vl stage 1: Pretraining ‚Äì The goal is to align the visual encoder and the LLM using large-scale image-text pairs. During this stage, the LLM‚Äôs parameters are frozen, only training adaptor and ViT. Training data is 1.4B weakly labeled, web-crawled set of image-text pairs. The training objective is to minimize the cross-entropy of the text tokens. stage 2: Multitask Pretraining ‚Äì Uses higher-quality, multitask image-text data (mainly from open-source vision-language datasets and some in-house data), higher image resolution, and involves full model fine-tuning. stage 3: Instruction Tuning ‚Äì The visual encoder is frozen; training data is mostly generated via self-instruction from the LLM. This phase aims to improve instruction following and multi-turn dialogue capabilities. Another important insight from Qwen-VL is that stages 2 and 3 incorporate not only vision-language data but also pure-text data. This helps preserve the LLM‚Äôs capabilities and avoid catastrophic forgetting. This strategy has proven effective in other models as well. Compared to models such as InstructBLIP, Qwen-VL simplifies the adapter between vision and language, using only a shallow attention pooling module nad achieving better performance.\nLLaVA Below shows an overview of LLaVA 1.5‚Äôs data and architecture. Compared to Qwen-VL, LLaVA 1.5 uses much less pretraining and instruction tuning data (with both Stage 2 and Stage 3 of Qwen-VL viewed as instruction tuning for comparison). In terms of architecture, both the vision encoder and the LLM use different base models, and the adapter between vision and language is an even simpler MLP layer. On several evaluation benchmarks, LLaVA 1.5 outperforms Qwen-VL, suggesting that with the right optimizations, it‚Äôs possible to achieve strong multimodal understanding with less data and simpler architecture. As shown in the data comparison, improvements can come from:\nFigure 6. LlaVA-1.5 Adding high-quality, fine-grained VL data Enriching instruction templates Using pure-text instruction tuning Increasing image resolution Scaling up the LLM parameter size Qwen2-VL Qwen2-VL represents a contemporary architecture of VLM. The model architecture is shown below. It features several innovations comparing with previous version:\nNative Dynamic Resolution mechanism: The model adapts to varying image resolutions by generating different numbers of visual tokens on-the-fly‚Äîmoving away from traditional fixed-resolution processing. Qwen2-VL pretrained their own ViT model based on this mechanism. Multimodal Rotary Position Embedding: Introduces rotary-style positional embeddings capable of jointly encoding positional information across text, images, and video. This unified encoding supports seamless fusion of positional cues across modalities Unified Image \u0026 Video Processing Pipeline: The Visual Transformer (ViT) backbone (675M parameters) is structured to process both static images and video frames in a single, coherent paradigm. Simplifies multimodal modeling and demonstrates strong performance on both image and video tasks . Notice that there is no explicit vision adaptor outside the vision module.\nFigure 7. Qwen2-VL Training comprises three stages:\nFirst stage: ViT pretraining. Note that the training only optimizes ViT module, but LLM is used in the training process which differs from the original ViT pretraining. Second stage: unfreeze all parameters and train with a wider range of data for more comprehensive learning. Third stage: lock the ViT parameters and perform exclusive fine-tuning of the LLM using instructional datasets Multimodal Rotary Position Embedding Extending RoPE from 1D to 2D: For a given position, split the input vector of dimension $d$ into two halves. Use a 1D RoPE matrix of size $\\frac{d}{2}$ (denoted as $R_x$) to process the first half of the vector, and use another 1D RoPE matrix of size $\\frac{d}{2}$ (denoted as $R_y$) to process the second half. Then, concatenate the two processed halves together ‚Äî this completes the 2D RoPE processing.\n3D-RoPE mapping is similar to 2D-RoPE. For a given position $(x, y, z)$, split the input vector of dimension $d$ into three equal parts. Use a 1D RoPE matrix for $x$ (denoted as $\\mathcal{R}_x$) to process the first part of the vector, use a 1D RoPE matrix for $y$ (denoted as $\\mathcal{R}_y$) to process the second part, and use a 1D RoPE matrix for $z$ (denoted as $\\mathcal{R}_z$) to process the third part. Finally, concatenate the three processed parts together.\nLWM LWM‚Äôs core goal is to create multimodal large model capable of ultra-long context understanding. With support for 1 million token inputs, LWM can comprehend videos over an hour long. The key highlights of LWM‚Äôs work include:\nSupport for ultra-long contexts, capable of handling extended text, image sequences, or video content; Solutions for key technical challenges: Mixed-length input using a Masked Sequence Packing method; balancing visual and textual modalities via loss weighting; automatic generation of long-sequence Q\u0026A datasets for model training; Implementation of high-performance optimizations such as RingAttention and Masked Sequence Packing, enabling training on multimodal sequences of million-token scale; Open-sourcing of a 7B-parameter large model, including long-context text-only models (LWM-Text, LWM-Text-Chat) and multimodal models (LWM, LWM-Chat). LWM uses a Transformer-based structure and extends the context length limit on top of LLaMA 2 (7B). LWM uses VQGAN as the visual encoder, which encodes 256 √ó 256 input images into 16 √ó 16 discrete tokens. This enables LWM not only to generate text, but also to generate image tokens from text, which can be reconstructed into video. For multiple images or video frames, visual features can be extracted individually and input into the LLM along with textual modalities.\nThe model training process consists of two main stages:\nExtend the context length of the text-only LLM to 1M using the Books dataset; Multimodal training with long context, using mixed image-text data, video-text data, and pure-text Books data. There are two core challenges across these stages:\nScalable training of long documents; How to stably extend the LLM‚Äôs context length. The first focuses on training efficiency and computational cost, while the second emphasizes the effectiveness of long-context extension. To address challenge 1, LWM implements efficient RingAttention, combined with FlashAttention. For challenge 2, on one hand, both training stages adopt multi-round training to gradually increase context length. On the other hand, the model‚Äôs positional encoding capability for long texts is enhanced by simply adjusting the $\\theta$ parameter in RoPE.\nReferences World Model on Million-Length Video And Language With RingAttention CLIP: Learning Transferable Visual Models From Natural Language Supervision Improved Baselines with Visual Instruction Tuning BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond Qwen2-VL: Enhancing Vision-Language Model‚Äôs Perception of the World at Any Resolution Qwen2.5-VL Technical Report An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale World Model on Million-Length Video And Language With Blockwise RingAttention ","wordCount":"1893","inLanguage":"en-us","datePublished":"2024-05-18T00:18:23+08:00","dateModified":"2024-05-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vlm/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>VLM</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-05-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1893 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>4 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#vit aria-label=ViT>ViT</a></li><li><a href=#clip aria-label=CLIP>CLIP</a></li><li><a href=#blip-2 aria-label=BLIP-2>BLIP-2</a></li><li><a href=#vlm aria-label=VLM>VLM</a><ul><li><a href=#qwen-vl aria-label=Qwen-VL>Qwen-VL</a></li><li><a href=#llava aria-label=LLaVA>LLaVA</a></li><li><a href=#qwen2-vl aria-label=Qwen2-VL>Qwen2-VL</a><ul><li><a href=#multimodal-rotary-position-embedding aria-label="Multimodal Rotary Position Embedding">Multimodal Rotary Position Embedding</a></li></ul></li></ul></li><li><a href=#lwm aria-label=LWM>LWM</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=vit>ViT<a hidden class=anchor aria-hidden=true href=#vit>#</a></h2><p>ViT (Vision Transformer) flattens the input image into a 2D sequence of patches (e.g., 16x16), and then uses a linear projection layer to convert each patch into a fixed-length feature vector‚Äîsimilar to word embeddings in natural language processing. Additionally, each patch is assigned a positional index, which is mapped to a positional vector through a positional embedding layer. The patch embeddings and positional embeddings are summed together to form the input to the Transformer encoder, resembling the design of the BERT model.</p><div align=center><img src=images/vit.png style=width:70%;height:auto> Figure 1. ViT</div><p>ViT has a BERT encoder based architecture and it uses a learnable [CLS] token to obtain a representation of the entire image, which is then passed through a fully connected layer to serve downstream classification tasks. When pretrained on large-scale datasets and transferred to medium- or small-scale image recognition benchmarks (e.g., ImageNet, CIFAR-100, VTAB), ViT achieves better performance than CNN-based models, while significantly reducing computational resource requirements during training.</p><p>The key to ViT‚Äôs success lies in large-scale pretraining. Without this step, directly training a ViT model on standard open datasets would still result in performance inferior to CNNs, which benefit from stronger inductive biases.</p><p>Training datasets are supervised datasets including: ImageNet 1.3M images, ImageNet-21k, 21k classes and 14 images, JFT dataset, 18k classes and 303M images. The <code>[CLS]</code> token is used for classification task at training time.</p><h2 id=clip>CLIP<a hidden class=anchor aria-hidden=true href=#clip>#</a></h2><p>The core idea of CLIP is to align visual and natural language representations through contrastive learning. CLIP first extracts features separately from text and images. The text encoder is a pretrained BERT, while the image encoder can be a traditional CNN model or a ViT (Vision Transformer) model. After obtaining the image and text embeddings, their features are normalized, and cosine similarity is calculated between all image-text pairs in the batch. A contrastive loss function‚Äîsuch as triple Loss or InfoNCE Loss is used to pull together positive pairs and push apart negative pairs.</p><div align=center><img src=images/clip.png style=width:100%;height:auto> Figure 2. CLIP</div><p>After pretraining on a large number of image-text pairs, CLIP learns a shared representation space for both the text and image encoders. There are typically two types of downstream applications:</p><p>Fine-tuning the model on downstream tasks to adapt it for specific image-text matching tasks. In this case, the text or image encoder can also be used for unimodal tasks. Zero-shot learning, where the pretrained image-text representations are directly used without any further training. Zero-shot usage of CLIP is shown in diagrams (2) and (3). For an image classification task, each candidate class is converted into a textual prompt using a template like ‚ÄúA photo of a {object}‚Äù, where object is the candidate class. For a given image to be classified, CLIP extracts a visual embedding via the image encoder and compares it to the embeddings of all candidate text prompts obtained from the text encoder. The class with the highest similarity is selected as the prediction.</p><p>Due to its simple architecture and outstanding performance, CLIP has been widely adopted in many subsequent works, with its pretrained backbone often used as an initialization for visual representation modules.</p><h2 id=blip-2>BLIP-2<a hidden class=anchor aria-hidden=true href=#blip-2>#</a></h2><p>The core idea of BLIP-2 is to enhance multimodal performance and reduce training costs by leveraging pretrained vision and language models. The architecture consists of a vision encoder, a vision-language adapter (Q-Former), and a large language model (LLM) layer.</p><div align=center><img src=images/blip2.png style=width:65%;height:auto> Figure 3. Blip-2 Architecture</div><ul><li>Vision Encoder: Uses a ViT model with weights initialized through CLIP pretraining. The final enhancement layer (which enriches output features) is removed. During training, the weights are frozen and not updated.</li><li>LLM: Early versions of BLIP-2 used OPT/FlanT5 to experiment with decoder-based and encoder-decoder-based LLMs. This part is also frozen during training and not updated.</li><li>Q-Former: The Q-Former is a module to compress the visual feature sequence and to extract key visual information via learned query vectors and feeds it to the LLM. The size of learnable queries is like 32 x 768 in the paper. This component holds most of the trainable parameters during multimodal model training.</li></ul><div align=center><img src=images/blip2_qformer.png style=width:100%;height:auto> Figure 4. Blip-2 Q-former</div><h2 id=vlm>VLM<a hidden class=anchor aria-hidden=true href=#vlm>#</a></h2><h3 id=qwen-vl>Qwen-VL<a hidden class=anchor aria-hidden=true href=#qwen-vl>#</a></h3><p>Qwen-VL is a classic large multimodal models. Qwen-VL uses the Qwen-7B LLM as the language backbone, OpenCLIP-pretrained ViT-bigG as the visual feature encoder, and a randomly initialized single-layer cross-attention module as the adapter between vision and language. The total number of parameters is approximately 9.6 billion.</p><p>As shown in the figure, Qwen-VL‚Äôs training process is divided into three stages:</p><div align=center><img src=images/qwen1.png style=width:90%;height:auto> Figure 5. Qwen-vl</div><ul><li>stage 1: Pretraining ‚Äì The goal is to align the visual encoder and the LLM using large-scale image-text pairs. During this stage, the LLM&rsquo;s parameters are frozen, only training adaptor and ViT. Training data is 1.4B weakly labeled, web-crawled set of image-text pairs. The training objective is to minimize the cross-entropy of the text tokens.</li><li>stage 2: Multitask Pretraining ‚Äì Uses higher-quality, multitask image-text data (mainly from open-source vision-language datasets and some in-house data), higher image resolution, and involves full model fine-tuning.</li><li>stage 3: Instruction Tuning ‚Äì The visual encoder is frozen; training data is mostly generated via self-instruction from the LLM. This phase aims to improve instruction following and multi-turn dialogue capabilities.</li></ul><p>Another important insight from Qwen-VL is that stages 2 and 3 incorporate not only vision-language data but also pure-text data. This helps preserve the LLM‚Äôs capabilities and avoid catastrophic forgetting. This strategy has proven effective in other models as well. Compared to models such as InstructBLIP, Qwen-VL simplifies the adapter between vision and language, using only a shallow attention pooling module nad achieving better performance.</p><h3 id=llava>LLaVA<a hidden class=anchor aria-hidden=true href=#llava>#</a></h3><p>Below shows an overview of LLaVA 1.5‚Äôs data and architecture. Compared to Qwen-VL, LLaVA 1.5 uses much less pretraining and instruction tuning data (with both Stage 2 and Stage 3 of Qwen-VL viewed as instruction tuning for comparison). In terms of architecture, both the vision encoder and the LLM use different base models, and the adapter between vision and language is an even simpler MLP layer. On several evaluation benchmarks, LLaVA 1.5 outperforms Qwen-VL, suggesting that with the right optimizations, it&rsquo;s possible to achieve strong multimodal understanding with less data and simpler architecture. As shown in the data comparison, improvements can come from:</p><div align=center><img src=images/llava1.5.png style=width:90%;height:auto> Figure 6. LlaVA-1.5</div><ul><li>Adding high-quality, fine-grained VL data</li><li>Enriching instruction templates</li><li>Using pure-text instruction tuning</li><li>Increasing image resolution</li><li>Scaling up the LLM parameter size</li></ul><h3 id=qwen2-vl>Qwen2-VL<a hidden class=anchor aria-hidden=true href=#qwen2-vl>#</a></h3><p>Qwen2-VL represents a contemporary architecture of VLM. The model architecture is shown below. It features several innovations comparing with previous version:</p><ol><li>Native Dynamic Resolution mechanism: The model adapts to varying image resolutions by generating different numbers of visual tokens on-the-fly‚Äîmoving away from traditional fixed-resolution processing. Qwen2-VL pretrained their own ViT model based on this mechanism.</li><li>Multimodal Rotary Position Embedding: Introduces rotary-style positional embeddings capable of jointly encoding positional information across text, images, and video. This unified encoding supports seamless fusion of positional cues across modalities</li><li>Unified Image & Video Processing Pipeline: The Visual Transformer (ViT) backbone (675M parameters) is structured to process both static images and video frames in a single, coherent paradigm. Simplifies multimodal modeling and demonstrates strong performance on both image and video tasks .</li></ol><p>Notice that there is no explicit vision adaptor outside the vision module.</p><div align=center><img src=images/qwen2.png style=width:100%;height:auto> Figure 7. Qwen2-VL</div><p>Training comprises three stages:</p><ul><li>First stage: ViT pretraining. Note that the training only optimizes ViT module, but LLM is used in the training process which differs from the original ViT pretraining.</li><li>Second stage: unfreeze all parameters and train with a wider range of data for more comprehensive learning.</li><li>Third stage: lock the ViT parameters and perform exclusive fine-tuning of the LLM using instructional datasets</li></ul><h4 id=multimodal-rotary-position-embedding>Multimodal Rotary Position Embedding<a hidden class=anchor aria-hidden=true href=#multimodal-rotary-position-embedding>#</a></h4><p>Extending RoPE from 1D to 2D: For a given position, split the input vector of dimension $d$ into two halves. Use a 1D RoPE matrix of size $\frac{d}{2}$ (denoted as $R_x$) to process the first half of the vector, and use another 1D RoPE matrix of size $\frac{d}{2}$ (denoted as $R_y$) to process the second half. Then, concatenate the two processed halves together ‚Äî this completes the 2D RoPE processing.</p><p><strong>3D-RoPE</strong> mapping is similar to 2D-RoPE. For a given position $(x, y, z)$, split the input vector of dimension $d$ into three equal parts. Use a 1D RoPE matrix for $x$ (denoted as $\mathcal{R}_x$) to process the first part of the vector, use a 1D RoPE matrix for $y$ (denoted as $\mathcal{R}_y$) to process the second part, and use a 1D RoPE matrix for $z$ (denoted as $\mathcal{R}_z$) to process the third part. Finally, concatenate the three processed parts together.</p><h2 id=lwm>LWM<a hidden class=anchor aria-hidden=true href=#lwm>#</a></h2><p>LWM&rsquo;s core goal is to create <strong>multimodal large model capable of ultra-long context understanding</strong>. With support for 1 million token inputs, LWM can comprehend videos over an hour long. The key highlights of LWM‚Äôs work include:</p><ul><li>Support for ultra-long contexts, capable of handling extended text, image sequences, or video content;</li><li>Solutions for key technical challenges: Mixed-length input using a <em>Masked Sequence Packing</em> method; balancing visual and textual modalities via <em>loss weighting</em>; automatic generation of long-sequence Q&amp;A datasets for model training;</li><li>Implementation of high-performance optimizations such as <em>RingAttention</em> and <em>Masked Sequence Packing</em>, enabling training on multimodal sequences of million-token scale;</li><li>Open-sourcing of a 7B-parameter large model, including long-context text-only models (LWM-Text, LWM-Text-Chat) and multimodal models (LWM, LWM-Chat).</li></ul><p>LWM uses a Transformer-based structure and extends the context length limit on top of LLaMA 2 (7B). LWM uses VQGAN as the visual encoder, which encodes 256 √ó 256 input images into 16 √ó 16 discrete tokens. This enables LWM not only to generate text, but also to generate image tokens from text, which can be reconstructed into video. For multiple images or video frames, visual features can be extracted individually and input into the LLM along with textual modalities.</p><p>The model training process consists of two main stages:</p><ul><li>Extend the context length of the text-only LLM to 1M using the Books dataset;</li><li>Multimodal training with long context, using mixed image-text data, video-text data, and pure-text Books data.</li></ul><p>There are two core challenges across these stages:</p><ol><li>Scalable training of long documents;</li><li>How to stably extend the LLM‚Äôs context length.</li></ol><p>The first focuses on training efficiency and computational cost, while the second emphasizes the effectiveness of long-context extension. To address challenge 1, LWM implements efficient RingAttention, combined with FlashAttention. For challenge 2, on one hand, both training stages adopt multi-round training to gradually increase context length. On the other hand, the model‚Äôs positional encoding capability for long texts is enhanced by simply adjusting the $\theta$ parameter in RoPE.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>World Model on Million-Length Video And Language With RingAttention</li><li>CLIP: Learning Transferable Visual Models From Natural Language Supervision</li><li>Improved Baselines with Visual Instruction Tuning</li><li>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</li><li><a href=https://arxiv.org/abs/2308.12966>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a></li><li><a href=https://arxiv.org/pdf/2409.12191>Qwen2-VL: Enhancing Vision-Language Model‚Äôs Perception of the World at Any Resolution</a></li><li><a href=https://arxiv.org/abs/2502.13923>Qwen2.5-VL Technical Report</a></li><li>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</li><li>World Model on Million-Length Video And Language With Blockwise RingAttention</li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/ray/><span class=title>¬´</span><br><span>Ray</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/><span class=title>¬ª</span><br><span>VQ-VAE</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share VLM on twitter" href="https://twitter.com/intent/tweet/?text=VLM&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvlm%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VLM on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvlm%2f&amp;title=VLM&amp;summary=VLM&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvlm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VLM on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvlm%2f&title=VLM"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VLM on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvlm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VLM on whatsapp" href="https://api.whatsapp.com/send?text=VLM%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvlm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VLM on telegram" href="https://telegram.me/share/url?text=VLM&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvlm%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>