<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>VQ-VAE | Jun's Blog</title><meta name=keywords content><meta name=description content="Multimodality"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="VQ-VAE"><meta property="og:description" content="Multimodality"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-18T00:18:23+08:00"><meta property="article:modified_time" content="2024-05-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="VQ-VAE"><meta name=twitter:description content="Multimodality"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"VQ-VAE","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"VQ-VAE","name":"VQ-VAE","description":"Multimodality","keywords":[""],"articleBody":"Basics Here we have a short recap about KL-divergence. The materials are mostly from [6].\nInformation Information is defined as the log probability of event $$ I(p) = -logp(x) $$\nMinus sign here is to get positive value. If an event is rare (low p), then it carries a lot of information when it happens.\nEntropy Shannon entropy is the average of information - the expectation of $I(p)$ with respect to the distribution $p(x)$ $$ \\begin{aligned} H(p) \u0026= \\mathbb{E}_{x \\sim P}[I(p)] \\\\ \u0026= \\sum p(x)I(p) \\\\ \u0026= -\\sum p(x)\\log p(x) \\end{aligned} $$\nShannon entropy is the average(expected) information under the same distribution.\nCross-Entropy The average of $I(q)$ with respect to the distribution $p(x)$ $$ \\begin{aligned} H(p, q) \u0026= \\mathbb{E}_{x \\sim P}[I(q)] \\\\ \u0026= \\sum p(x)I(q) \\\\ \u0026= -\\sum p(x)\\log q(x) \\end{aligned} $$\nCross entropy is the average(expected) information under the different distribution.\nKL-divergence KL divergence is the relative entropy or information gain.\n$$ \\begin{aligned} D_{KL}(p||q) \u0026= H(p, q) - H(p) \\\\ \u0026= -\\sum p(x)\\log q(x) + \\sum p(x)\\log p(x) \\\\ \u0026= -\\sum p(x)\\log \\frac{q(x)}{p(x)} \\\\ \u0026= \\sum p(x)\\log \\frac{p(x)}{q(x)} \\\\ \u0026= \\mathbb{E}_{x \\sim p(x)}[\\log p(x) - \\log q(x)] \\end{aligned} $$\nRelative entropy is the difference between cross entropy and shannon entropy. Based on Jensenâ€™s inequality, we have $$ \\begin{aligned} -D_{KL}(p||q) \u0026= \\sum p(x)\\log \\frac{q(x)}{p(x)} \\\\ \u0026\u003c= \\log \\sum p(x) \\frac{q(x)}{p(x)} \\\\ \u0026= log 1 \\\\ \u0026= 0 \\end{aligned} $$ Thus, KL-divergence is always positive.\nForward and Reverse KL-divergence Normally we say $D_{KL}(p||q)$ is forward KL and $D_{KL}(q||p)$ is reverse KL.\nMinimizing KL-divergence In machine learning, we generally believe that data distribution $p_{d}$ is the real distribution. Modelâ€™s output $q_{m}$ is what is used to approximate $p_d$.\n$$ \\begin{aligned} D_{KL}(p_d \\| q_m) \u0026= -\\sum_{i=1}^{n} p_d(x_i)\\cdot\\log q_m(x_i) + \\sum_{i=1}^{n} p_d(x_i)\\cdot\\log p_d(x_i) \\\\ \u0026= \\mathbb{E}_{x \\sim p_d(x)} [-\\log q_m(x)] - \\mathbb{E}_{x \\sim p_d(x)} [-\\log p_d(x)] \\\\ \u0026= H(p_d, q_m) - H(p_d). \\end{aligned} $$ $H(p_d)$ is data entropy, thus a constant. Thus minimizing KL divergence is equivalent to maximizing cross entropy.\nAE An autoencoder has two main parts, encoder and decoder. Encoder compresses the input data into a smaller, lower-dimensional representation called a latent vector. For example, a 784-dimensional image (like a 28x28 pixel MNIST image) might be compressed into a 32-dimensional vector. Decoder attempts to reconstruct the original input from the encoded (compressed) representation. This process in illustrated in the figure below.\nFigure 1. Autoencoder Mathematically, this process can be represented as two transformations:\n$$ \\begin{aligned} z \u0026= g(X) , z \\in \\mathbb{R}^d\\\\ \\hat{X} \u0026= f(z) \\end{aligned} $$\nThe loss function is defined as the reconstruction loss. $$ \\mathcal{L}_{AE}(x, \\hat{x}) = |x - \\hat{x}|^2 $$\nThe decoder here promises us that we can input low dimension vector $z$ to get high-dimensional image data. Can we directly use this model as a generative model? i.e. randomly sample some latent vectors $ z $ in a low-dimensional space $ \\mathbb{R}^d $, and then feed them into the decoder $ f(z) $ to generate images?\nThe answer is that no. Why? Itâ€™s because we havenâ€™t explicitly modeled the distribution $p(z)$. We donâ€™t know which $ z $ can generate useful images. The data that decoder is trained on is limited. But $ z $ lies in a vast space ($ \\mathbb{R}^d $), and if we just randomly sample in this space, we naturally cannot expect to produce useful images.\nWhy AE Decoder Can Generate Images The decoder in AE is trained for reconstruction, not generation. The decoder in a vanilla autoencoder only learns to map valid latent codes (produced by the encoder) back to images.\nIf we feed the decoder a random latent vector, the decoder doesnâ€™t know how to interpret it â€” most likely weâ€™ll get garbage or noise. In contrast, generative models (like VAEs, GANs, diffusion models) train the latent space to follow a structured distribution, so random samples make sense.\nNo structured latent space. Autoencoders donâ€™t enforce any probability distribution over the latent codes. This means the latent space is irregular and discontinuous. Only codes near actual training examples reconstruct to meaningful images. Generative models like VAEs add a regularization term (KL divergence) so the latent space follows, e.g., a Gaussian distribution. Thatâ€™s what makes sampling possible.\nTo summarize, why AE canâ€™t be a generative model:\nAE doesnâ€™t model the distribution of latent variables $p(z)$ . If you randomly pick $z \\in \\mathbb{R}^d $ and decode, it usually produces junk. Thatâ€™s because AE never learns what region of latent space corresponds to real data.\nA decoder becomes generative when it can take latent codes sampled from a known prior distribution (e.g., Gaussian) â€” not just from the encoder â€” and map them to meaningful, diverse outputs.\nVAE Remember our objective is to find the distribution $p(X)$ such that we can generate images. From bayes rule, $$ p(X) = \\sum_z{p(X|z)p(z)} $$\nIf we explicitly model the $p(z)$, we might be able to get a good generative model which is the variational autoencoder. In practice, this is unrealistic because $z$ is in a big space, itâ€™s very hard to sample $z_i$ which is strongly correlated to $x_i$.\nThe solution is to put constaints on $z$â€™s distribution. Letâ€™s assume $p(z)$ follows a normal distribution.\nIn practice we use encoder to approximate the posterior $p_{\\theta}(z | x_i)$, the approximate distribution is $q_{\\phi}(z \\mid x)$.\nThen the generation process is as follows:\nFeed data sample $x_i$ to encoder and get posterior $p_{\\theta}(z | x_i)$, which is a normal distribution $$ q_{\\phi}(z \\mid x) = \\mathcal{N}\\big(z;, \\mu_{\\phi}(x),, \\sigma_{\\phi}^2(x) I \\big) $$ It is a multivariate Gaussian distribution with independent dimensions. Why we want it to be a Gaussian distribution, because we can let encoder to output $\\mu, \\sigma$ to model it. Note that the notation here $q_{\\phi}$ is encoder fitting posterior parameterized by $\\phi$ and $p_{\\theta}$ the real posterior.\nFrom the posterior, we sample $z_i$ which is the latent representation of $x_i$ Sampling $z_i$ from distribution $\\mathcal{N}(\\mu, \\sigma^2)$ï¼Œis equivalent to sampling $\\varepsilon$ from $\\mathcal{N}(0, I)$. Thus, we get a constant from normal distribution. This is the so-called Reparameterization Trick. $$ z_i = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$\nFeed $x_i$ to decoder, we get the distribution of $p(X | z_i)$. We think the generation of decoder (e.g. $\\mu_i$, the mean is the recovered $x_i$). The core idea of a VAE is to treat the latent vector as a probability distribution. The difference between AE and VAE is in step 2. Instead of directly using encoding as the input, we sample a vector $z_i$ as the input to the decoder. The smart part of this approach is that each sampling result $z_i$ is correlated to input $x_i$, thus we donâ€™t have to go through enormous sampling process.\nFigure 2. Variational Autoencoder Now that we know each element in the latent vector is a normal distribution, normally we would want to put some constraints on its distribution. For instance, we donâ€™t want it to have very small variance such that it collapses into a constant distribution. In this case, the VAE becomes AE. Since the objective is to constrain the output distribution to follow a normal distribution, the Kullbackâ€“Leibler (KL) divergence is utilized for regularization. From here we have the two components of VAE loss function: reconstruction loss and KL divergence regularization loss.\nVAE Loss From MLE perspective, to get a generative model, we want to learn a model to maximize\n$$ \\begin{aligned} L(\\theta) \u0026= \\log \\prod_{x \\in X} p(x; \\theta) \\\\ \u0026= \\sum_{x \\in X} \\log p(x; \\theta) \\end{aligned} $$ To simplify, we can consider single example case. Considering the latent variable $z$, we can rewrite the above as a joint probability: $$ \\log p(x; \\theta) = \\log \\sum_z p(x, z; \\theta) $$\nUsing Jensen inequality, we can have:\n$$ \\begin{aligned} \\log p(x; \\theta) \u0026= \\log \\sum_z p(x, z; \\theta) \\\\ \u0026=\\log \\sum_z Q(z) \\frac{p(x, z; \\theta)}{Q(z)} \\\\ \u0026=\\log E_{z \\in Q(z)} \\left[ \\frac{p(x, z; \\theta)}{Q(z)} \\right] \\\\ \u0026\\geq E_{z \\in Q(z)} \\left[ \\log \\frac{p(x, z; \\theta)}{Q(z)} \\right] \\end{aligned} $$ This is what is called Evidence Lower Bound (ELBO).\nSince encoder approximates the posterior, we can use $q(z|x; \\phi)$ to replace the $Q(z)$ in the above equation.\n$$ \\begin{aligned} ELBO(\\theta, \\phi) \u0026= E_{z \\in Q(z)} \\left[ \\log \\frac{p(x, z; \\theta)}{Q(z)} \\right] \\\\ \u0026= E_{z \\in q(z|x; \\phi)} \\left[ \\log \\frac{p(x, z; \\theta)}{q(z|x; \\phi)} \\right] \\\\ \u0026= E_{z \\in q(z|x)} \\left[ \\log \\frac{p(x|z; \\theta) \\times p(z)}{q(z|x; \\phi)} \\right] \\\\ \u0026= E_{z \\in q(z|x)} \\left[ \\log p(x|z; \\theta) \\right] + E_{z \\in q(z|x; \\phi)} \\left[ \\log \\frac{p(z)}{q(z|x; \\phi)} \\right] \\\\ \u0026= E_{z \\in q(z|x)} \\left[ \\log p(x|z; \\theta) \\right] - D_{KL}(q(z | x; \\phi) || p(z)) \\\\ \\end{aligned} $$ $E_{z \\in q(z|x)} \\left[ \\log p(x|z; \\theta) \\right]$ is called reconstruction loss and is usually computed by sampling from $q(z|x; \\phi)$\n$$ E_{z \\in q(z|x)} \\left[ \\log p(x|z; \\theta) \\right] \\approx \\frac{1}{m} \\sum_{i=1}^{m} \\log p (X | z_i; \\theta) $$\nIn practice, $m$ is often chosen to be 1. When variance is a constant, the MLE becomes MSE. This part can use MSE to compute.\nSimilarly for continuous variable we can also use MLE to maximize $log(p(X))$, we have\n$$ \\begin{aligned} \\log p_\\theta(X) \u0026= \\int_z q_\\phi(z \\mid X) \\log p_\\theta(X) dz \\quad \\\\ \u0026= \\int_z q_\\phi(z \\mid X) \\log \\frac{p_\\theta(X, z)}{p_\\theta(z \\mid X)} dz \\quad \\\\ \u0026= \\int_z q_\\phi(z \\mid X) \\log \\left( \\frac{p_\\theta(X, z)}{q_\\phi(z \\mid X)} \\cdot \\frac{q_\\phi(z \\mid X)}{p_\\theta(z \\mid X)} \\right) dz \\\\ \u0026= \\int_z q_\\phi(z \\mid X) \\log \\frac{p_\\theta(X, z)}{q_\\phi(z \\mid X)} , dz + \\int_z q_\\phi(z \\mid X) \\log \\frac{q_\\phi(z \\mid X)}{p_\\theta(z \\mid X)} dz \\\\ \u0026= \\ell(p_\\theta, q_\\phi) + D_{\\mathrm{KL}}(q_\\phi | p_\\theta) \\\\ \u0026\\geq \\ell(p_\\theta, q_\\phi) \\quad \\end{aligned} $$\nHere $q_\\phi(z \\mid X)$ is the posterior.\nVQ-VAE A VAE can encode an image into a vector that follows a standard Gaussian distribution. The reason for making the vector follow a standard Gaussian distribution is to facilitate random sampling. Note that the vectors encoded by a VAE are continuous vectors, meaning each dimension of the vector is a floating-point number. If you slightly change one dimension of the vector, the decoder can still recognize the vector and generate an image that is very similar to the one corresponding to the original vector.\nContrary to VAE, in VQ-VAE, the latent representation is discrete. The intuition is that in nature, where is male and female, limited number of colors etc.\nFigure 3. VQ-VAE The process is like the follows:\nInput image $x$ into the encoder to obtain $z_e$:\n$$ z_e = \\text{encoder}(x) $$\nThe codebook is a $K \\times D$ table (purple blocks):\n$$ E = [e_1, e_2, \\ldots, e_K] $$\nEach dimension in $z_e$ is mapped to one of the $K$ embeddings in the codebook:\n$$ z_q(x) = e_k, \\quad \\text{where } k = \\arg\\min_j | z_e(x) - e_j |_2 $$\nAfter replacing all green parts in the image with the purple $z_q$, reconstruction is performed.\nVQVAE uses codebook to replace the latent distribution in VAE. However in this way, the input to decoder becomes a fixed vector again like an autoencoder. To solve this issue, VQVAE trained a autoregressive model to fit the codebook. in this way, we can do generation again.\nHow VQ-VAE trains an autoregressive model VQ-VAE needs something like an autoregressive prior to regain the generative capability that a VAE naturally has via sampling. The missing piece is the prior over the discrete latent codes. VQ-VAE learns this in two stages:\nStep 1. Train the VQ-VAE Train the encoder, codebook, and decoder using the reconstruction loss + codebook commitment loss. After training, we have a dataset of latent codes (discrete indices into the codebook) corresponding to the training data.\nStep 2: Learn a prior with an autoregressive model Take the sequence of latent code indices produced by the encoder for your training data.\nTrain a powerful autoregressive model (PixelCNN, PixelSNAIL, or Transformer) on these code sequences.\nIf the latent map is 2D (like an image grid of discrete codes), you flatten or use masked convolutions to model the spatial dependencies. The autoregressive model learns $P(z) = \\prod_i P(z_i | z_{","wordCount":"2156","inLanguage":"en-us","datePublished":"2024-05-18T00:18:23+08:00","dateModified":"2024-05-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>VQ-VAE</h1><div class=post-description>Multimodality</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-05-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>2156 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>5 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/multimodality/ style=color:var(--secondary)!important>Multimodality</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#basics aria-label=Basics>Basics</a></li><li><a href=#ae aria-label=AE>AE</a><ul><li><a href=#why-ae-decoder-can-generate-images aria-label="Why AE Decoder Can Generate Images">Why AE Decoder Can Generate Images</a></li></ul></li><li><a href=#vae aria-label=VAE>VAE</a><ul><li><a href=#vae-loss aria-label="VAE Loss">VAE Loss</a></li></ul></li><li><a href=#vq-vae aria-label=VQ-VAE>VQ-VAE</a><ul><li><a href=#how-vq-vae-trains-an-autoregressive-model aria-label="How VQ-VAE trains an autoregressive model">How VQ-VAE trains an autoregressive model</a><ul><li><a href=#step-1--train-the-vq-vae aria-label="Step 1.  Train the VQ-VAE">Step 1. Train the VQ-VAE</a></li><li><a href=#step-2-learn-a-prior-with-an-autoregressive-model aria-label="Step 2: Learn a prior with an autoregressive model">Step 2: Learn a prior with an autoregressive model</a></li><li><a href=#step-3-generation aria-label="Step 3: Generation">Step 3: Generation</a></li></ul></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=basics>Basics<a hidden class=anchor aria-hidden=true href=#basics>#</a></h2><p>Here we have a short recap about KL-divergence. The materials are mostly from [6].</p><ol><li>Information</li></ol><p>Information is defined as the log probability of event
$$
I(p) = -logp(x)
$$</p><p>Minus sign here is to get positive value. If an event is rare (low p), then it carries a lot of information when it happens.</p><ol start=2><li>Entropy</li></ol><p>Shannon entropy is the average of information - the expectation of $I(p)$ with respect to the distribution $p(x)$
$$
\begin{aligned}
H(p) &= \mathbb{E}_{x \sim P}[I(p)] \\
&= \sum p(x)I(p) \\
&= -\sum p(x)\log p(x)
\end{aligned}
$$</p><p>Shannon entropy is the average(expected) information under the same distribution.</p><ol start=3><li>Cross-Entropy</li></ol><p>The average of $I(q)$ with respect to the distribution $p(x)$
$$
\begin{aligned}
H(p, q) &= \mathbb{E}_{x \sim P}[I(q)] \\
&= \sum p(x)I(q) \\
&= -\sum p(x)\log q(x)
\end{aligned}
$$</p><p>Cross entropy is the average(expected) information under the different distribution.</p><ol start=4><li>KL-divergence</li></ol><p>KL divergence is the relative entropy or information gain.</p><p>$$
\begin{aligned}
D_{KL}(p||q) &= H(p, q) - H(p) \\
&= -\sum p(x)\log q(x) + \sum p(x)\log p(x) \\
&= -\sum p(x)\log \frac{q(x)}{p(x)} \\
&= \sum p(x)\log \frac{p(x)}{q(x)} \\
&= \mathbb{E}_{x \sim p(x)}[\log p(x) - \log q(x)]
\end{aligned}
$$</p><p>Relative entropy is the difference between cross entropy and shannon entropy.
Based on <strong>Jensen&rsquo;s inequality</strong>, we have
$$
\begin{aligned}
-D_{KL}(p||q) &= \sum p(x)\log \frac{q(x)}{p(x)} \\
&&lt;= \log \sum p(x) \frac{q(x)}{p(x)} \\
&= log 1 \\
&= 0
\end{aligned}
$$
Thus, KL-divergence is always positive.</p><ol start=5><li>Forward and Reverse KL-divergence</li></ol><p>Normally we say $D_{KL}(p||q)$ is forward KL and $D_{KL}(q||p)$ is reverse KL.</p><ol start=6><li>Minimizing KL-divergence</li></ol><p>In machine learning, we generally believe that data distribution $p_{d}$ is the real distribution. Model&rsquo;s output $q_{m}$ is what is used to approximate $p_d$.</p><p>$$
\begin{aligned}
D_{KL}(p_d \| q_m) &= -\sum_{i=1}^{n} p_d(x_i)\cdot\log q_m(x_i) + \sum_{i=1}^{n} p_d(x_i)\cdot\log p_d(x_i) \\
&= \mathbb{E}_{x \sim p_d(x)} [-\log q_m(x)] - \mathbb{E}_{x \sim p_d(x)} [-\log p_d(x)] \\
&= H(p_d, q_m) - H(p_d).
\end{aligned}
$$</p><p>$H(p_d)$ is data entropy, thus a constant. Thus minimizing KL divergence is equivalent to maximizing cross entropy.</p><h2 id=ae>AE<a hidden class=anchor aria-hidden=true href=#ae>#</a></h2><p>An autoencoder has two main parts, encoder and decoder. Encoder compresses the input data into a smaller, lower-dimensional representation called a latent vector. For example, a 784-dimensional image (like a 28x28 pixel MNIST image) might be compressed into a 32-dimensional vector. Decoder attempts to reconstruct the original input from the encoded (compressed) representation. This process in illustrated in the figure below.</p><p align=center><img alt=Autoencoder src=images/ae.png width=80% height=auto/>
<em>Figure 1. Autoencoder</em></p><p>Mathematically, this process can be represented as two transformations:</p><p>$$
\begin{aligned}
z &= g(X) , z \in \mathbb{R}^d\\
\hat{X} &= f(z)
\end{aligned}
$$</p><p>The loss function is defined as the reconstruction loss.
$$
\mathcal{L}_{AE}(x, \hat{x}) = |x - \hat{x}|^2
$$</p><p>The decoder here promises us that we can input low dimension vector $z$ to get high-dimensional image data. Can we directly use this model as a generative model? i.e. randomly sample some latent vectors $ z $ in a low-dimensional space $ \mathbb{R}^d $, and then feed them into the decoder $ f(z) $ to generate images?</p><p>The answer is that no. Why? It&rsquo;s because we haven&rsquo;t explicitly modeled the distribution $p(z)$. We donâ€™t know which $ z $ can generate useful images. The data that decoder is trained on is limited. But $ z $ lies in a vast space ($ \mathbb{R}^d $), and if we just randomly sample in this space, we naturally cannot expect to produce useful images.</p><h3 id=why-ae-decoder-can-generate-images>Why AE Decoder Can Generate Images<a hidden class=anchor aria-hidden=true href=#why-ae-decoder-can-generate-images>#</a></h3><ul><li><p>The decoder in AE is trained for reconstruction, not generation. The decoder in a vanilla autoencoder only learns to map valid latent codes (produced by the encoder) back to images.</p></li><li><p>If we feed the decoder a random latent vector, the decoder doesnâ€™t know how to interpret it â€” most likely weâ€™ll get garbage or noise. In contrast, generative models (like VAEs, GANs, diffusion models) train the latent space to follow a structured distribution, so random samples make sense.</p></li><li><p>No structured latent space. Autoencoders donâ€™t enforce any probability distribution over the latent codes. This means the latent space is irregular and discontinuous. <strong>Only codes near actual training examples reconstruct to meaningful images</strong>. Generative models like VAEs add a regularization term (KL divergence) so the latent space follows, e.g., a Gaussian distribution. Thatâ€™s what makes sampling possible.</p></li></ul><p>To summarize, why AE canâ€™t be a generative model:</p><p>AE doesnâ€™t model the distribution of latent variables $p(z)$ . If you randomly pick $z \in \mathbb{R}^d $ and decode, it usually produces junk. Thatâ€™s because AE never learns what region of latent space corresponds to real data.</p><p>A decoder becomes generative when it can take latent codes sampled from a known prior distribution (e.g., Gaussian) â€” not just from the encoder â€” and map them to meaningful, diverse outputs.</p><h2 id=vae>VAE<a hidden class=anchor aria-hidden=true href=#vae>#</a></h2><p>Remember our objective is to find the distribution $p(X)$ such that we can generate images. From bayes rule,
$$
p(X) = \sum_z{p(X|z)p(z)}
$$</p><p>If we explicitly model the $p(z)$, we might be able to get a good generative model which is the variational autoencoder. In practice, this is unrealistic because $z$ is in a big space, it&rsquo;s very hard to sample $z_i$ which is strongly correlated to $x_i$.</p><p>The solution is to put constaints on $z$&rsquo;s distribution. Let&rsquo;s assume $p(z)$ follows a normal distribution.</p><p>In practice we use encoder to approximate the posterior $p_{\theta}(z | x_i)$, the approximate distribution is $q_{\phi}(z \mid x)$.</p><p>Then the generation process is as follows:</p><ol><li>Feed data sample $x_i$ to encoder and get posterior $p_{\theta}(z | x_i)$, which is a normal distribution</li></ol><p>$$
q_{\phi}(z \mid x) = \mathcal{N}\big(z;, \mu_{\phi}(x),, \sigma_{\phi}^2(x) I \big)
$$
It is a multivariate Gaussian distribution with independent dimensions. Why we want it to be a Gaussian distribution, because we can let encoder to output $\mu, \sigma$ to model it. Note that the notation here $q_{\phi}$ is encoder fitting posterior parameterized by $\phi$ and $p_{\theta}$ the real posterior.</p><ol start=2><li>From the posterior, we sample $z_i$ which is the latent representation of $x_i$</li></ol><p>Sampling $z_i$ from distribution $\mathcal{N}(\mu, \sigma^2)$ï¼Œis equivalent to sampling $\varepsilon$ from $\mathcal{N}(0, I)$. Thus, we get a constant from normal distribution. This is the so-called Reparameterization Trick.
$$
z_i = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
$$</p><ol start=3><li>Feed $x_i$ to decoder, we get the distribution of $p(X | z_i)$. We think the generation of decoder (e.g. $\mu_i$, the mean is the recovered $x_i$).</li></ol><p>The core idea of a VAE is to treat the latent vector as a probability distribution. The difference between AE and VAE is in step 2. Instead of directly using encoding as the input, we sample a vector $z_i$ as the input to the decoder. The smart part of this approach is that each sampling result $z_i$ is correlated to input $x_i$, thus we don&rsquo;t have to go through enormous sampling process.</p><p align=center><img alt=Autoencoder src=images/vae.jpg width=80% height=auto/>
<em>Figure 2. Variational Autoencoder</em></p><p>Now that we know each element in the latent vector is a normal distribution, normally we would want to put some constraints on its distribution. For instance, we don&rsquo;t want it to have very small variance such that it collapses into a constant distribution. In this case, the VAE becomes AE. Since the objective is to constrain the output distribution to follow a normal distribution, the Kullbackâ€“Leibler (KL) divergence is utilized for regularization. From here we have the two components of VAE loss function: reconstruction loss and KL divergence regularization loss.</p><h3 id=vae-loss>VAE Loss<a hidden class=anchor aria-hidden=true href=#vae-loss>#</a></h3><p>From MLE perspective, to get a generative model, we want to learn a model to maximize</p><p>$$
\begin{aligned}
L(\theta) &= \log \prod_{x \in X} p(x; \theta) \\
&= \sum_{x \in X} \log p(x; \theta)
\end{aligned}
$$</p><p>To simplify, we can consider single example case. Considering the latent variable $z$, we can rewrite the above as a joint probability:
$$
\log p(x; \theta) = \log \sum_z p(x, z; \theta)
$$</p><p>Using Jensen inequality, we can have:</p><p>$$
\begin{aligned}
\log p(x; \theta) &= \log \sum_z p(x, z; \theta) \\
&=\log \sum_z Q(z) \frac{p(x, z; \theta)}{Q(z)} \\
&=\log E_{z \in Q(z)} \left[ \frac{p(x, z; \theta)}{Q(z)} \right] \\
&\geq E_{z \in Q(z)} \left[ \log \frac{p(x, z; \theta)}{Q(z)} \right]
\end{aligned}
$$</p><p>This is what is called Evidence Lower Bound (ELBO).</p><p>Since encoder approximates the posterior, we can use $q(z|x; \phi)$ to replace the $Q(z)$ in the above equation.</p><p>$$
\begin{aligned}
ELBO(\theta, \phi) &= E_{z \in Q(z)} \left[ \log \frac{p(x, z; \theta)}{Q(z)} \right] \\
&= E_{z \in q(z|x; \phi)} \left[ \log \frac{p(x, z; \theta)}{q(z|x; \phi)} \right] \\
&= E_{z \in q(z|x)} \left[ \log \frac{p(x|z; \theta) \times p(z)}{q(z|x; \phi)} \right] \\
&= E_{z \in q(z|x)} \left[ \log p(x|z; \theta) \right] + E_{z \in q(z|x; \phi)} \left[ \log \frac{p(z)}{q(z|x; \phi)} \right] \\
&= E_{z \in q(z|x)} \left[ \log p(x|z; \theta) \right] - D_{KL}(q(z | x; \phi) || p(z)) \\
\end{aligned}
$$</p><p>$E_{z \in q(z|x)} \left[ \log p(x|z; \theta) \right]$ is called reconstruction loss and is usually computed by sampling from $q(z|x; \phi)$</p><p>$$
E_{z \in q(z|x)} \left[ \log p(x|z; \theta) \right] \approx \frac{1}{m} \sum_{i=1}^{m} \log p (X | z_i; \theta)
$$</p><p>In practice, $m$ is often chosen to be 1. When variance is a constant, the MLE becomes MSE. This part can use MSE to compute.</p><p>Similarly for continuous variable we can also use MLE to maximize $log(p(X))$, we have</p><p>$$
\begin{aligned}
\log p_\theta(X)
&= \int_z q_\phi(z \mid X) \log p_\theta(X) dz \quad \\
&= \int_z q_\phi(z \mid X) \log \frac{p_\theta(X, z)}{p_\theta(z \mid X)} dz \quad \\
&= \int_z q_\phi(z \mid X) \log \left( \frac{p_\theta(X, z)}{q_\phi(z \mid X)} \cdot \frac{q_\phi(z \mid X)}{p_\theta(z \mid X)} \right) dz \\
&= \int_z q_\phi(z \mid X) \log \frac{p_\theta(X, z)}{q_\phi(z \mid X)} , dz + \int_z q_\phi(z \mid X) \log \frac{q_\phi(z \mid X)}{p_\theta(z \mid X)} dz \\
&= \ell(p_\theta, q_\phi) + D_{\mathrm{KL}}(q_\phi | p_\theta) \\
&\geq \ell(p_\theta, q_\phi) \quad
\end{aligned}
$$</p><p>Here $q_\phi(z \mid X)$ is the posterior.</p><h2 id=vq-vae>VQ-VAE<a hidden class=anchor aria-hidden=true href=#vq-vae>#</a></h2><p>A VAE can encode an image into a vector that follows a standard Gaussian distribution. The reason for making the vector follow a standard Gaussian distribution is to facilitate random sampling. Note that the vectors encoded by a VAE are continuous vectors, meaning each dimension of the vector is a floating-point number. If you slightly change one dimension of the vector, the decoder can still recognize the vector and generate an image that is very similar to the one corresponding to the original vector.</p><p>Contrary to VAE, in VQ-VAE, the latent representation is discrete. The intuition is that in nature, where is male and female, limited number of colors etc.</p><p align=center><img alt=Autoencoder src=images/vq.png width=100% height=auto/>
<em>Figure 3. VQ-VAE</em></p><p>The process is like the follows:</p><ol><li><p>Input image $x$ into the encoder to obtain $z_e$:<br>$$
z_e = \text{encoder}(x)
$$</p></li><li><p>The codebook is a $K \times D$ table (purple blocks):<br>$$
E = [e_1, e_2, \ldots, e_K]
$$</p></li><li><p>Each dimension in $z_e$ is mapped to one of the $K$ embeddings in the codebook:<br>$$
z_q(x) = e_k, \quad \text{where } k = \arg\min_j | z_e(x) - e_j |_2
$$</p></li><li><p>After replacing all green parts in the image with the purple $z_q$, reconstruction is performed.</p></li></ol><p>VQVAE uses codebook to replace the latent distribution in VAE. However in this way, the input to decoder becomes a fixed vector again like an autoencoder. To solve this issue, VQVAE trained a autoregressive model to fit the codebook. in this way, we can do generation again.</p><h3 id=how-vq-vae-trains-an-autoregressive-model>How VQ-VAE trains an autoregressive model<a hidden class=anchor aria-hidden=true href=#how-vq-vae-trains-an-autoregressive-model>#</a></h3><p>VQ-VAE needs something like an autoregressive prior to regain the generative capability that a VAE naturally has via sampling. The missing piece is the <strong>prior</strong> over the discrete latent codes. VQ-VAE learns this in two stages:</p><h4 id=step-1--train-the-vq-vae>Step 1. Train the VQ-VAE<a hidden class=anchor aria-hidden=true href=#step-1--train-the-vq-vae>#</a></h4><p>Train the encoder, codebook, and decoder using the reconstruction loss + codebook commitment loss. After training, we have a dataset of latent codes (discrete indices into the codebook) corresponding to the training data.</p><h4 id=step-2-learn-a-prior-with-an-autoregressive-model>Step 2: Learn a prior with an autoregressive model<a hidden class=anchor aria-hidden=true href=#step-2-learn-a-prior-with-an-autoregressive-model>#</a></h4><ul><li><p>Take the sequence of latent code indices produced by the encoder for your training data.</p></li><li><p>Train a powerful autoregressive model (PixelCNN, PixelSNAIL, or Transformer) on these code sequences.</p><ul><li>If the latent map is 2D (like an image grid of discrete codes), you flatten or use masked convolutions to model the spatial dependencies.</li><li>The autoregressive model learns $P(z) = \prod_i P(z_i | z_{&lt;i})$, where each $z_i$ is a code index.</li></ul></li><li><p>This prior learns the distribution of code sequences across your dataset.</p></li></ul><h4 id=step-3-generation>Step 3: Generation<a hidden class=anchor aria-hidden=true href=#step-3-generation>#</a></h4><ul><li>Sample a sequence of latent codes from the autoregressive prior.</li><li>Pass these codes through the decoder to generate a new image (or audio, depending on domain).</li></ul><p>Thatâ€™s how VQ-VAE restores the generative power: instead of sampling from a Gaussian prior (like in a VAE), it samples from a learned discrete autoregressive prior.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://arxiv.org/abs/1312.6114>Auto-Encoding Variational Bayes</a></li><li><a href=https://arxiv.org/abs/1711.00937>Neural Discrete Representation Learning</a></li><li><a href=https://zhuanlan.zhihu.com/p/348498294>https://zhuanlan.zhihu.com/p/348498294</a></li><li><a href=https://zhuanlan.zhihu.com/p/34998569>https://zhuanlan.zhihu.com/p/34998569</a></li><li><a href=https://zhuanlan.zhihu.com/p/2433292582>https://zhuanlan.zhihu.com/p/2433292582</a></li><li><a href=https://zhuanlan.zhihu.com/p/425693597>https://zhuanlan.zhihu.com/p/425693597</a></li><li><a href=https://zhuanlan.zhihu.com/p/719968411>https://zhuanlan.zhihu.com/p/719968411</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vlm/><span class=title>Â«</span><br><span>VLM</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/triton/><span class=title>Â»</span><br><span>Triton, Cuda and GPU</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>