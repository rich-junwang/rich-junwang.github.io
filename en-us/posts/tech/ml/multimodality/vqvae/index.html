<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>VQ-VAE | Jun's Blog</title><meta name=keywords content><meta name=description content="Basics
Here we have a short recap about KL-divergence. The materials are mostly from [6].

Information

Information is defined as the log probability of event

$$
I(p) = -logp(x)
$$Minus sign here is to get positive value. If an event is rare (low p), then it carries a lot of information when it happens.

Entropy

Shannon entropy is the average of information - the expectation of $I(p)$ with respect to the distribution $p(x)$

$$
\begin{aligned}
H(p) &= \mathbb{E}_{x \sim P}[I(p)] \\\
     &= \sum p(x)I(p) \\\
     &= -\sum p(x)\log p(x)
\end{aligned}
$$Shannon entropy is the average(expected) information under the same distribution.

Cross-Entropy

The average of $I(q)$ with respect to the distribution $p(x)$

$$
\begin{aligned}
H(p, q) &= \mathbb{E}_{x \sim P}[I(q)] \\\
        &= \sum p(x)I(q) \\\
        &= -\sum p(x)\log q(x)
\end{aligned}
$$Cross entropy is the average(expected) information under the different distribution.

KL-divergence

KL divergence is the relative entropy or information gain."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="VQ-VAE"><meta property="og:description" content="Basics
Here we have a short recap about KL-divergence. The materials are mostly from [6].

Information

Information is defined as the log probability of event

$$
I(p) = -logp(x)
$$Minus sign here is to get positive value. If an event is rare (low p), then it carries a lot of information when it happens.

Entropy

Shannon entropy is the average of information - the expectation of $I(p)$ with respect to the distribution $p(x)$

$$
\begin{aligned}
H(p) &= \mathbb{E}_{x \sim P}[I(p)] \\\
     &= \sum p(x)I(p) \\\
     &= -\sum p(x)\log p(x)
\end{aligned}
$$Shannon entropy is the average(expected) information under the same distribution.

Cross-Entropy

The average of $I(q)$ with respect to the distribution $p(x)$

$$
\begin{aligned}
H(p, q) &= \mathbb{E}_{x \sim P}[I(q)] \\\
        &= \sum p(x)I(q) \\\
        &= -\sum p(x)\log q(x)
\end{aligned}
$$Cross entropy is the average(expected) information under the different distribution.

KL-divergence

KL divergence is the relative entropy or information gain."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-18T00:18:23+08:00"><meta property="article:modified_time" content="2024-05-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="VQ-VAE"><meta name=twitter:description content="Basics
Here we have a short recap about KL-divergence. The materials are mostly from [6].

Information

Information is defined as the log probability of event

$$
I(p) = -logp(x)
$$Minus sign here is to get positive value. If an event is rare (low p), then it carries a lot of information when it happens.

Entropy

Shannon entropy is the average of information - the expectation of $I(p)$ with respect to the distribution $p(x)$

$$
\begin{aligned}
H(p) &= \mathbb{E}_{x \sim P}[I(p)] \\\
     &= \sum p(x)I(p) \\\
     &= -\sum p(x)\log p(x)
\end{aligned}
$$Shannon entropy is the average(expected) information under the same distribution.

Cross-Entropy

The average of $I(q)$ with respect to the distribution $p(x)$

$$
\begin{aligned}
H(p, q) &= \mathbb{E}_{x \sim P}[I(q)] \\\
        &= \sum p(x)I(q) \\\
        &= -\sum p(x)\log q(x)
\end{aligned}
$$Cross entropy is the average(expected) information under the different distribution.

KL-divergence

KL divergence is the relative entropy or information gain."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"VQ-VAE","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"VQ-VAE","name":"VQ-VAE","description":"Basics Here we have a short recap about KL-divergence. The materials are mostly from [6].\nInformation Information is defined as the log probability of event $$ I(p) = -logp(x) $$Minus sign here is to get positive value. If an event is rare (low p), then it carries a lot of information when it happens.\nEntropy Shannon entropy is the average of information - the expectation of $I(p)$ with respect to the distribution $p(x)$ $$ \\begin{aligned} H(p) \u0026= \\mathbb{E}_{x \\sim P}[I(p)] \\\\\\ \u0026= \\sum p(x)I(p) \\\\\\ \u0026= -\\sum p(x)\\log p(x) \\end{aligned} $$Shannon entropy is the average(expected) information under the same distribution.\nCross-Entropy The average of $I(q)$ with respect to the distribution $p(x)$ $$ \\begin{aligned} H(p, q) \u0026= \\mathbb{E}_{x \\sim P}[I(q)] \\\\\\ \u0026= \\sum p(x)I(q) \\\\\\ \u0026= -\\sum p(x)\\log q(x) \\end{aligned} $$Cross entropy is the average(expected) information under the different distribution.\nKL-divergence KL divergence is the relative entropy or information gain.\n","keywords":[],"articleBody":"Basics Here we have a short recap about KL-divergence. The materials are mostly from [6].\nInformation Information is defined as the log probability of event $$ I(p) = -logp(x) $$Minus sign here is to get positive value. If an event is rare (low p), then it carries a lot of information when it happens.\nEntropy Shannon entropy is the average of information - the expectation of $I(p)$ with respect to the distribution $p(x)$ $$ \\begin{aligned} H(p) \u0026= \\mathbb{E}_{x \\sim P}[I(p)] \\\\\\ \u0026= \\sum p(x)I(p) \\\\\\ \u0026= -\\sum p(x)\\log p(x) \\end{aligned} $$Shannon entropy is the average(expected) information under the same distribution.\nCross-Entropy The average of $I(q)$ with respect to the distribution $p(x)$ $$ \\begin{aligned} H(p, q) \u0026= \\mathbb{E}_{x \\sim P}[I(q)] \\\\\\ \u0026= \\sum p(x)I(q) \\\\\\ \u0026= -\\sum p(x)\\log q(x) \\end{aligned} $$Cross entropy is the average(expected) information under the different distribution.\nKL-divergence KL divergence is the relative entropy or information gain.\n$$ \\begin{aligned} D_{KL}(p||q) \u0026= H(p, q) - H(p) \\\\\\ \u0026= -\\sum p(x)\\log q(x) + \\sum p(x)\\log p(x) \\\\\\ \u0026= -\\sum p(x)\\log \\frac{q(x)}{p(x)} \\\\\\ \u0026= \\sum p(x)\\log \\frac{p(x)}{q(x)} \\\\\\ \u0026= \\mathbb{E}_{x \\sim p(x)}[\\log p(x) - \\log q(x)] \\end{aligned} $$Relative entropy is the difference between cross entropy and shannon entropy. Based on Jensen‚Äôs inequality, we have $$ \\begin{aligned} -D_{KL}(p||q) \u0026= \\sum p(x)\\log \\frac{q(x)}{p(x)} \\\\\\ \u0026\u003c= \\log \\sum p(x) \\frac{q(x)}{p(x)} \\\\\\ \u0026= log 1 \\\\\\ \u0026= 0 \\end{aligned} $$ Thus, KL-divergence is always positive.\nForward and Reverse KL-divergence Normally we say $D_{KL}(p||q)$ is forward KL and $D_{KL}(q||p)$ is reverse KL.\nMinimizing KL-divergence In machine learning, we generally believe that data distribution $p_{d}$ is the real distribution. Model‚Äôs output $q_{m}$ is what is used to approximate $p_d$.\n$$ \\begin{aligned} D_{KL}(p_d \\| q_m) \u0026= -\\sum_{i=1}^{n} p_d(x_i)\\cdot\\log q_m(x_i) + \\sum_{i=1}^{n} p_d(x_i)\\cdot\\log p_d(x_i) \\\\ \u0026= \\mathbb{E}_{x \\sim p_d(x)} [-\\log q_m(x)] - \\mathbb{E}_{x \\sim p_d(x)} [-\\log p_d(x)] \\\\ \u0026= H(p_d, q_m) - H(p_d). \\end{aligned} $$ $H(p_d)$ is data entropy, thus a constant. Thus minimizing KL divergence is equivalent to maximizing cross entropy.\nAE An autoencoder has two main parts, encoder and decoder. Encoder compresses the input data into a smaller, lower-dimensional representation called a latent vector. For example, a 784-dimensional image (like a 28x28 pixel MNIST image) might be compressed into a 32-dimensional vector. Decoder attempts to reconstruct the original input from the encoded (compressed) representation. This process in illustrated in the figure below.\nFigure 1. Autoencoder Mathematically, this process can be represented as two transformations:\n$$ \\begin{aligned} z \u0026= g(X) , z \\in \\mathbb{R}^d\\\\\\ \\hat{X} \u0026= f(z) \\end{aligned} $$The loss function is defined as the reconstruction loss. $$ \\mathcal{L}_{AE}(x, \\hat{x}) = \\|x - \\hat{x}\\|^2 $$The decoder here promises us that we can input low dimension vector $z$ to get high-dimensional image data. Can we directly use this model as a generative model? i.e. randomly sample some latent vectors $ z $ in a low-dimensional space $ \\mathbb{R}^d $, and then feed them into the decoder $ f(z) $ to generate images?\nThe answer is that no. Why? It‚Äôs because we haven‚Äôt explicitly modeled the distribution $p(z)$. We don‚Äôt know which $ z $ can generate useful images. The data that decoder is trained on is limited. But $ z $ lies in a vast space ($ \\mathbb{R}^d $), and if we just randomly sample in this space, we naturally cannot expect to produce useful images.\nWhy AE Decoder Can Generate Images The decoder in AE is trained for reconstruction, not generation. The decoder in a vanilla autoencoder only learns to map valid latent codes (produced by the encoder) back to images.\nIf we feed the decoder a random latent vector, the decoder doesn‚Äôt know how to interpret it ‚Äî most likely we‚Äôll get garbage or noise. In contrast, generative models (like VAEs, GANs, diffusion models) train the latent space to follow a structured distribution, so random samples make sense.\nNo structured latent space. Autoencoders don‚Äôt enforce any probability distribution over the latent codes. This means the latent space is irregular and discontinuous. Only codes near actual training examples reconstruct to meaningful images. Generative models like VAEs add a regularization term (KL divergence) so the latent space follows, e.g., a Gaussian distribution. That‚Äôs what makes sampling possible.\nTo summarize, why AE can‚Äôt be a generative model:\nAE doesn‚Äôt model the distribution of latent variables $p(z)$ . If you randomly pick $z \\in \\mathbb{R}^d $ and decode, it usually produces junk. That‚Äôs because AE never learns what region of latent space corresponds to real data.\nA decoder becomes generative when it can take latent codes sampled from a known prior distribution (e.g., Gaussian) ‚Äî not just from the encoder ‚Äî and map them to meaningful, diverse outputs.\nVAE Remember our objective is to find the distribution $p(X)$ such that we can generate images. From bayes rule, $$ p(X) = \\sum_z{p(X|z)p(z)} $$If we explicitly model the $p(z)$, we might be able to get a good generative model which is the variational autoencoder. In practice, this is unrealistic because $z$ is in a big space, it‚Äôs very hard to sample $z_i$ which is strongly correlated to $x_i$.\nThe solution is to put constaints on $z$‚Äôs distribution. Let‚Äôs assume $p(z)$ follows a normal distribution.\nIn practice we use encoder to approximate the posterior $p_{\\theta}(z | x_i)$, the approximate distribution is $q_{\\phi}(z \\mid x)$.\nThen the generation process is as follows:\nFeed data sample $x_i$ to encoder and get posterior $p_{\\theta}(z | x_i)$, which is a normal distribution $$ q_{\\phi}(z \\mid x) = \\mathcal{N}\\big(z;\\, \\mu_{\\phi}(x),\\, \\sigma_{\\phi}^2(x) I \\big) $$ It is a multivariate Gaussian distribution with independent dimensions. Why we want it to be a Gaussian distribution, because we can let encoder to output $\\mu, \\sigma$ to model it. Note that the notation here $q_{\\phi}$ is encoder fitting posterior parameterized by $\\phi$ and $p_{\\theta}$ the real posterior.\nFrom the posterior, we sample $z_i$ which is the latent representation of $x_i$ Sampling $z_i$ from distribution $\\mathcal{N}(\\mu, \\sigma^2)$Ôºåis equivalent to sampling $\\varepsilon$ from $\\mathcal{N}(0, I)$. Thus, we get a constant from normal distribution. This is the so-called Reparameterization Trick. $$ z_i = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ Feed $x_i$ to decoder, we get the distribution of $p(X | z_i)$. We think the generation of decoder (e.g. $\\mu_i$, the mean is the recovered $x_i$). The core idea of a VAE is to treat the latent vector as a probability distribution. The difference between AE and VAE is in step 2. Instead of directly using encoding as the input, we sample a vector $z_i$ as the input to the decoder. The smart part of this approach is that each sampling result $z_i$ is correlated to input $x_i$, thus we don‚Äôt have to go through enormous sampling process.\nFigure 2. Variational Autoencoder Now that we know each element in the latent vector is a normal distribution, normally we would want to put some constraints on its distribution. For instance, we don‚Äôt want it to have very small variance such that it collapses into a constant distribution. In this case, the VAE becomes AE. Since the objective is to constrain the output distribution to follow a normal distribution, the Kullback‚ÄìLeibler (KL) divergence is utilized for regularization. From here we have the two components of VAE loss function: reconstruction loss and KL divergence regularization loss.\nVAE Loss From MLE perspective, to get a generative model, we want to learn a model to maximize\n$$ \\begin{aligned} L(\\theta) \u0026= \\log \\prod_{x \\in X} p(x; \\theta) \\\\ \u0026= \\sum_{x \\in X} \\log p(x; \\theta) \\end{aligned} $$ To simplify, we can consider single example case. Considering the latent variable $z$, we can rewrite the above as a joint probability: $$ \\log p(x; \\theta) = \\log \\sum_z p(x, z; \\theta) $$Using Jensen inequality, we can have:\n$$ \\begin{aligned} \\log p(x; \\theta) \u0026= \\log \\sum_z p(x, z; \\theta) \\\\ \u0026=\\log \\sum_z Q(z) \\frac{p(x, z; \\theta)}{Q(z)} \\\\ \u0026=\\log E_{z \\in Q(z)} \\left[ \\frac{p(x, z; \\theta)}{Q(z)} \\right] \\\\ \u0026\\geq E_{z \\in Q(z)} \\left[ \\log \\frac{p(x, z; \\theta)}{Q(z)} \\right] \\end{aligned} $$ This is what is called Evidence Lower Bound (ELBO).\nSince encoder approximates the posterior, we can use $q(z|x; \\phi)$ to replace the $Q(z)$ in the above equation.\n$$ \\begin{aligned} ELBO(\\theta, \\phi) \u0026= E_{z \\in Q(z)} \\left[ \\log \\frac{p(x, z; \\theta)}{Q(z)} \\right] \\\\ \u0026= E_{z \\in q(z|x; \\phi)} \\left[ \\log \\frac{p(x, z; \\theta)}{q(z|x; \\phi)} \\right] \\\\ \u0026= E_{z \\in q(z|x)} \\left[ \\log \\frac{p(x|z; \\theta) \\times p(z)}{q(z|x; \\phi)} \\right] \\\\ \u0026= E_{z \\in q(z|x)} \\left[ \\log p(x|z; \\theta) \\right] + E_{z \\in q(z|x; \\phi)} \\left[ \\log \\frac{p(z)}{q(z|x; \\phi)} \\right] \\\\ \u0026= E_{z \\in q(z|x)} \\left[ \\log p(x|z; \\theta) \\right] - D_{KL}(q(z | x; \\phi) || p(z)) \\\\ \\end{aligned} $$ $E_{z \\in q(z|x)} \\left[ \\log p(x|z; \\theta) \\right]$ is called reconstruction loss and is usually computed by sampling from $q(z|x; \\phi)$\n$$ E_{z \\in q(z|x)} \\left[ \\log p(x|z; \\theta) \\right] \\approx \\frac{1}{m} \\sum_{i=1}^{m} \\log p (X | z_i; \\theta) $$In practice, $m$ is often chosen to be 1. When variance is a constant, the MLE becomes MSE. This part can use MSE to compute.\nSimilarly for continuous variable we can also use MLE to maximize $log(p(X))$, we have\n$$ \\begin{aligned} \\log p_\\theta(X) \u0026= \\int_z q_\\phi(z \\mid X) \\log p_\\theta(X) dz \\quad \\\\\\ \u0026= \\int_z q_\\phi(z \\mid X) \\log \\frac{p_\\theta(X, z)}{p_\\theta(z \\mid X)} dz \\quad \\\\\\ \u0026= \\int_z q_\\phi(z \\mid X) \\log \\left( \\frac{p_\\theta(X, z)}{q_\\phi(z \\mid X)} \\cdot \\frac{q_\\phi(z \\mid X)}{p_\\theta(z \\mid X)} \\right) dz \\\\\\ \u0026= \\int_z q_\\phi(z \\mid X) \\log \\frac{p_\\theta(X, z)}{q_\\phi(z \\mid X)} \\, dz + \\int_z q_\\phi(z \\mid X) \\log \\frac{q_\\phi(z \\mid X)}{p_\\theta(z \\mid X)} dz \\\\\\ \u0026= \\ell(p_\\theta, q_\\phi) + D_{\\mathrm{KL}}(q_\\phi \\| p_\\theta) \\\\\\ \u0026\\geq \\ell(p_\\theta, q_\\phi) \\quad \\end{aligned} $$Here $q_\\phi(z \\mid X)$ is the posterior.\nVQ-VAE A VAE can encode an image into a vector that follows a standard Gaussian distribution. The reason for making the vector follow a standard Gaussian distribution is to facilitate random sampling. Note that the vectors encoded by a VAE are continuous vectors, meaning each dimension of the vector is a floating-point number. If you slightly change one dimension of the vector, the decoder can still recognize the vector and generate an image that is very similar to the one corresponding to the original vector.\nContrary to VAE, in VQ-VAE, the latent representation is discrete. The intuition is that in nature, where is male and female, limited number of colors etc.\nFigure 3. VQ-VAE The process is like the follows:\nInput image $x$ into the encoder to obtain $z_e$:\n$$ z_e = \\text{encoder}(x) $$ The codebook is a $K \\times D$ table (purple blocks):\n$$ E = [e_1, e_2, \\ldots, e_K] $$ Each dimension in $z_e$ is mapped to one of the $K$ embeddings in the codebook:\n$$ z_q(x) = e_k, \\quad \\text{where } k = \\arg\\min_j \\| z_e(x) - e_j \\|_2 $$ After replacing all green parts in the image with the purple $z_q$, reconstruction is performed.\nVQVAE uses codebook to replace the latent distribution in VAE. However in this way, the input to decoder becomes a fixed vector again like an autoencoder. To solve this issue, VQVAE trained a autoregressive model to fit the codebook. in this way, we can do generation again.\nHow VQ-VAE trains an autoregressive model VQ-VAE needs something like an autoregressive prior to regain the generative capability that a VAE naturally has via sampling. The missing piece is the prior over the discrete latent codes. VQ-VAE learns this in two stages:\nStep 1. Train the VQ-VAE Train the encoder, codebook, and decoder using the reconstruction loss + codebook commitment loss. After training, we have a dataset of latent codes (discrete indices into the codebook) corresponding to the training data.\nStep 2: Learn a prior with an autoregressive model Take the sequence of latent code indices produced by the encoder for your training data.\nTrain a powerful autoregressive model (PixelCNN, PixelSNAIL, or Transformer) on these code sequences.\nIf the latent map is 2D (like an image grid of discrete codes), you flatten or use masked convolutions to model the spatial dependencies. The autoregressive model learns $P(z) = \\prod_i P(z_i | z_{","wordCount":"2055","inLanguage":"en-us","datePublished":"2024-05-18T00:18:23+08:00","dateModified":"2024-05-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>VQ-VAE</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-05-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>2055 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>5 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#basics aria-label=Basics>Basics</a></li><li><a href=#ae aria-label=AE>AE</a><ul><li><a href=#why-ae-decoder-can-generate-images aria-label="Why AE Decoder Can Generate Images">Why AE Decoder Can Generate Images</a></li></ul></li><li><a href=#vae aria-label=VAE>VAE</a><ul><li><a href=#vae-loss aria-label="VAE Loss">VAE Loss</a></li></ul></li><li><a href=#vq-vae aria-label=VQ-VAE>VQ-VAE</a><ul><li><a href=#how-vq-vae-trains-an-autoregressive-model aria-label="How VQ-VAE trains an autoregressive model">How VQ-VAE trains an autoregressive model</a><ul><li><a href=#step-1--train-the-vq-vae aria-label="Step 1.  Train the VQ-VAE">Step 1. Train the VQ-VAE</a></li><li><a href=#step-2-learn-a-prior-with-an-autoregressive-model aria-label="Step 2: Learn a prior with an autoregressive model">Step 2: Learn a prior with an autoregressive model</a></li><li><a href=#step-3-generation aria-label="Step 3: Generation">Step 3: Generation</a></li></ul></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=basics>Basics<a hidden class=anchor aria-hidden=true href=#basics>#</a></h2><p>Here we have a short recap about KL-divergence. The materials are mostly from [6].</p><ol><li>Information</li></ol><p>Information is defined as the log probability of event</p>$$
I(p) = -logp(x)
$$<p>Minus sign here is to get positive value. If an event is rare (low p), then it carries a lot of information when it happens.</p><ol start=2><li>Entropy</li></ol><p>Shannon entropy is the average of information - the expectation of $I(p)$ with respect to the distribution $p(x)$</p>$$
\begin{aligned}
H(p) &= \mathbb{E}_{x \sim P}[I(p)] \\\
&= \sum p(x)I(p) \\\
&= -\sum p(x)\log p(x)
\end{aligned}
$$<p>Shannon entropy is the average(expected) information under the same distribution.</p><ol start=3><li>Cross-Entropy</li></ol><p>The average of $I(q)$ with respect to the distribution $p(x)$</p>$$
\begin{aligned}
H(p, q) &= \mathbb{E}_{x \sim P}[I(q)] \\\
&= \sum p(x)I(q) \\\
&= -\sum p(x)\log q(x)
\end{aligned}
$$<p>Cross entropy is the average(expected) information under the different distribution.</p><ol start=4><li>KL-divergence</li></ol><p>KL divergence is the relative entropy or information gain.</p>$$
\begin{aligned}
D_{KL}(p||q) &= H(p, q) - H(p) \\\
&= -\sum p(x)\log q(x) + \sum p(x)\log p(x) \\\
&= -\sum p(x)\log \frac{q(x)}{p(x)} \\\
&= \sum p(x)\log \frac{p(x)}{q(x)} \\\
&= \mathbb{E}_{x \sim p(x)}[\log p(x) - \log q(x)]
\end{aligned}
$$<p>Relative entropy is the difference between cross entropy and shannon entropy.
Based on <strong>Jensen&rsquo;s inequality</strong>, we have</p>$$
\begin{aligned}
-D_{KL}(p||q) &= \sum p(x)\log \frac{q(x)}{p(x)} \\\
&<= \log \sum p(x) \frac{q(x)}{p(x)} \\\
&= log 1 \\\
&= 0
\end{aligned}
$$<p>Thus, KL-divergence is always positive.</p><ol start=5><li>Forward and Reverse KL-divergence</li></ol><p>Normally we say $D_{KL}(p||q)$ is forward KL and $D_{KL}(q||p)$ is reverse KL.</p><ol start=6><li>Minimizing KL-divergence</li></ol><p>In machine learning, we generally believe that data distribution $p_{d}$ is the real distribution. Model&rsquo;s output $q_{m}$ is what is used to approximate $p_d$.</p><p>$$
\begin{aligned}
D_{KL}(p_d \| q_m) &= -\sum_{i=1}^{n} p_d(x_i)\cdot\log q_m(x_i) + \sum_{i=1}^{n} p_d(x_i)\cdot\log p_d(x_i) \\
&= \mathbb{E}_{x \sim p_d(x)} [-\log q_m(x)] - \mathbb{E}_{x \sim p_d(x)} [-\log p_d(x)] \\
&= H(p_d, q_m) - H(p_d).
\end{aligned}
$$</p><p>$H(p_d)$ is data entropy, thus a constant. Thus minimizing KL divergence is equivalent to maximizing cross entropy.</p><h2 id=ae>AE<a hidden class=anchor aria-hidden=true href=#ae>#</a></h2><p>An autoencoder has two main parts, encoder and decoder. Encoder compresses the input data into a smaller, lower-dimensional representation called a latent vector. For example, a 784-dimensional image (like a 28x28 pixel MNIST image) might be compressed into a 32-dimensional vector. Decoder attempts to reconstruct the original input from the encoded (compressed) representation. This process in illustrated in the figure below.</p><p align=center><img alt=Autoencoder src=images/ae.png width=80% height=auto/>
<em>Figure 1. Autoencoder</em></p><p>Mathematically, this process can be represented as two transformations:</p>$$
\begin{aligned}
z &= g(X) , z \in \mathbb{R}^d\\\
\hat{X} &= f(z)
\end{aligned}
$$<p>The loss function is defined as the reconstruction loss.</p>$$
\mathcal{L}_{AE}(x, \hat{x}) = \|x - \hat{x}\|^2
$$<p>The decoder here promises us that we can input low dimension vector $z$ to get high-dimensional image data. Can we directly use this model as a generative model? i.e. randomly sample some latent vectors $ z $ in a low-dimensional space $ \mathbb{R}^d $, and then feed them into the decoder $ f(z) $ to generate images?</p><p>The answer is that no. Why? It&rsquo;s because we haven&rsquo;t explicitly modeled the distribution $p(z)$. We don‚Äôt know which $ z $ can generate useful images. The data that decoder is trained on is limited. But $ z $ lies in a vast space ($ \mathbb{R}^d $), and if we just randomly sample in this space, we naturally cannot expect to produce useful images.</p><h3 id=why-ae-decoder-can-generate-images>Why AE Decoder Can Generate Images<a hidden class=anchor aria-hidden=true href=#why-ae-decoder-can-generate-images>#</a></h3><ul><li><p>The decoder in AE is trained for reconstruction, not generation. The decoder in a vanilla autoencoder only learns to map valid latent codes (produced by the encoder) back to images.</p></li><li><p>If we feed the decoder a random latent vector, the decoder doesn‚Äôt know how to interpret it ‚Äî most likely we‚Äôll get garbage or noise. In contrast, generative models (like VAEs, GANs, diffusion models) train the latent space to follow a structured distribution, so random samples make sense.</p></li><li><p>No structured latent space. Autoencoders don‚Äôt enforce any probability distribution over the latent codes. This means the latent space is irregular and discontinuous. <strong>Only codes near actual training examples reconstruct to meaningful images</strong>. Generative models like VAEs add a regularization term (KL divergence) so the latent space follows, e.g., a Gaussian distribution. That‚Äôs what makes sampling possible.</p></li></ul><p>To summarize, why AE can‚Äôt be a generative model:</p><p>AE doesn‚Äôt model the distribution of latent variables $p(z)$ . If you randomly pick $z \in \mathbb{R}^d $ and decode, it usually produces junk. That‚Äôs because AE never learns what region of latent space corresponds to real data.</p><p>A decoder becomes generative when it can take latent codes sampled from a known prior distribution (e.g., Gaussian) ‚Äî not just from the encoder ‚Äî and map them to meaningful, diverse outputs.</p><h2 id=vae>VAE<a hidden class=anchor aria-hidden=true href=#vae>#</a></h2><p>Remember our objective is to find the distribution $p(X)$ such that we can generate images. From bayes rule,</p>$$
p(X) = \sum_z{p(X|z)p(z)}
$$<p>If we explicitly model the $p(z)$, we might be able to get a good generative model which is the variational autoencoder. In practice, this is unrealistic because $z$ is in a big space, it&rsquo;s very hard to sample $z_i$ which is strongly correlated to $x_i$.</p><p>The solution is to put constaints on $z$&rsquo;s distribution. Let&rsquo;s assume $p(z)$ follows a normal distribution.</p><p>In practice we use encoder to approximate the posterior $p_{\theta}(z | x_i)$, the approximate distribution is $q_{\phi}(z \mid x)$.</p><p>Then the generation process is as follows:</p><ol><li>Feed data sample $x_i$ to encoder and get posterior $p_{\theta}(z | x_i)$, which is a normal distribution</li></ol>$$
q_{\phi}(z \mid x) = \mathcal{N}\big(z;\, \mu_{\phi}(x),\, \sigma_{\phi}^2(x) I \big)
$$<p>It is a multivariate Gaussian distribution with independent dimensions. Why we want it to be a Gaussian distribution, because we can let encoder to output $\mu, \sigma$ to model it. Note that the notation here $q_{\phi}$ is encoder fitting posterior parameterized by $\phi$ and $p_{\theta}$ the real posterior.</p><ol start=2><li>From the posterior, we sample $z_i$ which is the latent representation of $x_i$</li></ol><p>Sampling $z_i$ from distribution $\mathcal{N}(\mu, \sigma^2)$Ôºåis equivalent to sampling $\varepsilon$ from $\mathcal{N}(0, I)$. Thus, we get a constant from normal distribution. This is the so-called Reparameterization Trick.</p>$$
z_i = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
$$<ol start=3><li>Feed $x_i$ to decoder, we get the distribution of $p(X | z_i)$. We think the generation of decoder (e.g. $\mu_i$, the mean is the recovered $x_i$).</li></ol><p>The core idea of a VAE is to treat the latent vector as a probability distribution. The difference between AE and VAE is in step 2. Instead of directly using encoding as the input, we sample a vector $z_i$ as the input to the decoder. The smart part of this approach is that each sampling result $z_i$ is correlated to input $x_i$, thus we don&rsquo;t have to go through enormous sampling process.</p><p align=center><img alt=Autoencoder src=images/vae.jpg width=80% height=auto/>
<em>Figure 2. Variational Autoencoder</em></p><p>Now that we know each element in the latent vector is a normal distribution, normally we would want to put some constraints on its distribution. For instance, we don&rsquo;t want it to have very small variance such that it collapses into a constant distribution. In this case, the VAE becomes AE. Since the objective is to constrain the output distribution to follow a normal distribution, the Kullback‚ÄìLeibler (KL) divergence is utilized for regularization. From here we have the two components of VAE loss function: reconstruction loss and KL divergence regularization loss.</p><h3 id=vae-loss>VAE Loss<a hidden class=anchor aria-hidden=true href=#vae-loss>#</a></h3><p>From MLE perspective, to get a generative model, we want to learn a model to maximize</p><p>$$
\begin{aligned}
L(\theta) &= \log \prod_{x \in X} p(x; \theta) \\
&= \sum_{x \in X} \log p(x; \theta)
\end{aligned}
$$</p><p>To simplify, we can consider single example case. Considering the latent variable $z$, we can rewrite the above as a joint probability:</p>$$
\log p(x; \theta) = \log \sum_z p(x, z; \theta)
$$<p>Using Jensen inequality, we can have:</p><p>$$
\begin{aligned}
\log p(x; \theta) &= \log \sum_z p(x, z; \theta) \\
&=\log \sum_z Q(z) \frac{p(x, z; \theta)}{Q(z)} \\
&=\log E_{z \in Q(z)} \left[ \frac{p(x, z; \theta)}{Q(z)} \right] \\
&\geq E_{z \in Q(z)} \left[ \log \frac{p(x, z; \theta)}{Q(z)} \right]
\end{aligned}
$$</p><p>This is what is called Evidence Lower Bound (ELBO).</p><p>Since encoder approximates the posterior, we can use $q(z|x; \phi)$ to replace the $Q(z)$ in the above equation.</p><p>$$
\begin{aligned}
ELBO(\theta, \phi) &= E_{z \in Q(z)} \left[ \log \frac{p(x, z; \theta)}{Q(z)} \right] \\
&= E_{z \in q(z|x; \phi)} \left[ \log \frac{p(x, z; \theta)}{q(z|x; \phi)} \right] \\
&= E_{z \in q(z|x)} \left[ \log \frac{p(x|z; \theta) \times p(z)}{q(z|x; \phi)} \right] \\
&= E_{z \in q(z|x)} \left[ \log p(x|z; \theta) \right] + E_{z \in q(z|x; \phi)} \left[ \log \frac{p(z)}{q(z|x; \phi)} \right] \\
&= E_{z \in q(z|x)} \left[ \log p(x|z; \theta) \right] - D_{KL}(q(z | x; \phi) || p(z)) \\
\end{aligned}
$$</p><p>$E_{z \in q(z|x)} \left[ \log p(x|z; \theta) \right]$ is called reconstruction loss and is usually computed by sampling from $q(z|x; \phi)$</p>$$
E_{z \in q(z|x)} \left[ \log p(x|z; \theta) \right] \approx \frac{1}{m} \sum_{i=1}^{m} \log p (X | z_i; \theta)
$$<p>In practice, $m$ is often chosen to be 1. When variance is a constant, the MLE becomes MSE. This part can use MSE to compute.</p><p>Similarly for continuous variable we can also use MLE to maximize $log(p(X))$, we have</p>$$
\begin{aligned}
\log p_\theta(X)
&= \int_z q_\phi(z \mid X) \log p_\theta(X) dz \quad \\\
&= \int_z q_\phi(z \mid X) \log \frac{p_\theta(X, z)}{p_\theta(z \mid X)} dz \quad \\\
&= \int_z q_\phi(z \mid X) \log \left( \frac{p_\theta(X, z)}{q_\phi(z \mid X)} \cdot \frac{q_\phi(z \mid X)}{p_\theta(z \mid X)} \right) dz \\\
&= \int_z q_\phi(z \mid X) \log \frac{p_\theta(X, z)}{q_\phi(z \mid X)} \, dz + \int_z q_\phi(z \mid X) \log \frac{q_\phi(z \mid X)}{p_\theta(z \mid X)} dz \\\
&= \ell(p_\theta, q_\phi) + D_{\mathrm{KL}}(q_\phi \| p_\theta) \\\
&\geq \ell(p_\theta, q_\phi) \quad
\end{aligned}
$$<p>Here $q_\phi(z \mid X)$ is the posterior.</p><h2 id=vq-vae>VQ-VAE<a hidden class=anchor aria-hidden=true href=#vq-vae>#</a></h2><p>A VAE can encode an image into a vector that follows a standard Gaussian distribution. The reason for making the vector follow a standard Gaussian distribution is to facilitate random sampling. Note that the vectors encoded by a VAE are continuous vectors, meaning each dimension of the vector is a floating-point number. If you slightly change one dimension of the vector, the decoder can still recognize the vector and generate an image that is very similar to the one corresponding to the original vector.</p><p>Contrary to VAE, in VQ-VAE, the latent representation is discrete. The intuition is that in nature, where is male and female, limited number of colors etc.</p><p align=center><img alt=Autoencoder src=images/vq.png width=100% height=auto/>
<em>Figure 3. VQ-VAE</em></p><p>The process is like the follows:</p><ol><li><p>Input image $x$ into the encoder to obtain $z_e$:<br></p>$$
z_e = \text{encoder}(x)
$$</li><li><p>The codebook is a $K \times D$ table (purple blocks):<br></p>$$
E = [e_1, e_2, \ldots, e_K]
$$</li><li><p>Each dimension in $z_e$ is mapped to one of the $K$ embeddings in the codebook:<br></p>$$
z_q(x) = e_k, \quad \text{where } k = \arg\min_j \| z_e(x) - e_j \|_2
$$</li><li><p>After replacing all green parts in the image with the purple $z_q$, reconstruction is performed.</p></li></ol><p>VQVAE uses codebook to replace the latent distribution in VAE. However in this way, the input to decoder becomes a fixed vector again like an autoencoder. To solve this issue, VQVAE trained a autoregressive model to fit the codebook. in this way, we can do generation again.</p><h3 id=how-vq-vae-trains-an-autoregressive-model>How VQ-VAE trains an autoregressive model<a hidden class=anchor aria-hidden=true href=#how-vq-vae-trains-an-autoregressive-model>#</a></h3><p>VQ-VAE needs something like an autoregressive prior to regain the generative capability that a VAE naturally has via sampling. The missing piece is the <strong>prior</strong> over the discrete latent codes. VQ-VAE learns this in two stages:</p><h4 id=step-1--train-the-vq-vae>Step 1. Train the VQ-VAE<a hidden class=anchor aria-hidden=true href=#step-1--train-the-vq-vae>#</a></h4><p>Train the encoder, codebook, and decoder using the reconstruction loss + codebook commitment loss. After training, we have a dataset of latent codes (discrete indices into the codebook) corresponding to the training data.</p><h4 id=step-2-learn-a-prior-with-an-autoregressive-model>Step 2: Learn a prior with an autoregressive model<a hidden class=anchor aria-hidden=true href=#step-2-learn-a-prior-with-an-autoregressive-model>#</a></h4><ul><li><p>Take the sequence of latent code indices produced by the encoder for your training data.</p></li><li><p>Train a powerful autoregressive model (PixelCNN, PixelSNAIL, or Transformer) on these code sequences.</p><ul><li>If the latent map is 2D (like an image grid of discrete codes), you flatten or use masked convolutions to model the spatial dependencies.</li><li>The autoregressive model learns $P(z) = \prod_i P(z_i | z_{<i})$, where each $z_i$ is a code index.</li></ul></li><li><p>This prior learns the distribution of code sequences across your dataset.</p></li></ul><h4 id=step-3-generation>Step 3: Generation<a hidden class=anchor aria-hidden=true href=#step-3-generation>#</a></h4><ul><li>Sample a sequence of latent codes from the autoregressive prior.</li><li>Pass these codes through the decoder to generate a new image (or audio, depending on domain).</li></ul><p>That‚Äôs how VQ-VAE restores the generative power: instead of sampling from a Gaussian prior (like in a VAE), it samples from a learned discrete autoregressive prior.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://arxiv.org/abs/1312.6114>Auto-Encoding Variational Bayes</a></li><li><a href=https://arxiv.org/abs/1711.00937>Neural Discrete Representation Learning</a></li><li><a href=https://zhuanlan.zhihu.com/p/348498294>https://zhuanlan.zhihu.com/p/348498294</a></li><li><a href=https://zhuanlan.zhihu.com/p/34998569>https://zhuanlan.zhihu.com/p/34998569</a></li><li><a href=https://zhuanlan.zhihu.com/p/2433292582>https://zhuanlan.zhihu.com/p/2433292582</a></li><li><a href=https://zhuanlan.zhihu.com/p/425693597>https://zhuanlan.zhihu.com/p/425693597</a></li><li><a href=https://zhuanlan.zhihu.com/p/719968411>https://zhuanlan.zhihu.com/p/719968411</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vlm/><span class=title>¬´</span><br><span>VLM</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/triton/><span class=title>¬ª</span><br><span>Triton, Cuda and GPU</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share VQ-VAE on twitter" href="https://twitter.com/intent/tweet/?text=VQ-VAE&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvqvae%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VQ-VAE on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvqvae%2f&amp;title=VQ-VAE&amp;summary=VQ-VAE&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvqvae%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VQ-VAE on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvqvae%2f&title=VQ-VAE"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VQ-VAE on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvqvae%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VQ-VAE on whatsapp" href="https://api.whatsapp.com/send?text=VQ-VAE%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvqvae%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share VQ-VAE on telegram" href="https://telegram.me/share/url?text=VQ-VAE&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmultimodality%2fvqvae%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>