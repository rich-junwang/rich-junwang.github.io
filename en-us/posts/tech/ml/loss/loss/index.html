<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Loss in ML | Jun's Blog</title><meta name=keywords content><meta name=description content="Sigmoid
Sigmoid is one of the most used activation functions.

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$
It has nice mathematical proprities:

$$
\sigma^\prime(x) = \sigma(x) \left[ 1 - \sigma(x) \right]
$$
and

$$
\left[log\sigma(x)\right]^\prime = 1 - \sigma(x) \\\
\left[log\left(1 - \sigma(x)\right)\right]^\prime = - \sigma(x)
$$Logistic Regression
For a binary classification problem, for an example $x = (x_1, x_2, \dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as:

$$
\begin{aligned}
h_{\theta}(1|x) &= \sigma(\theta_0 + \theta_1x_1 + \theta_2x_2 + \dotsb + \theta_nx_n) \\\
&= \sigma(\theta^{\mathrm{T}}x) \\\
&= \frac{1}{1+e^{-\theta^{\mathrm{T}}x}}
\end{aligned}
$$
Consequently, for the negative class,

$$
\begin{aligned}
h_{\theta}(0|x) &= 1 - \frac{1}{1+e^{-\theta^{\mathrm{T}}x}} \\\
&= \frac{1}{1+e^{\theta^{\mathrm{T}}x}} \\\
&= \sigma(-\theta^{\mathrm{T}}x)
\end{aligned}
$$Single sample cost function of logistic regression is expressed as:

$$
L(\theta) = -y_i \cdot \log(h_\theta(x_i)) -  (1-y_i) \cdot \log(1 - h_\theta(x_i))
$$
Notice that in the second term $1 - h_\theta(x_i)$ is the negative class probability"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/loss/loss/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/loss/loss/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="Loss in ML"><meta property="og:description" content="Sigmoid
Sigmoid is one of the most used activation functions.

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$
It has nice mathematical proprities:

$$
\sigma^\prime(x) = \sigma(x) \left[ 1 - \sigma(x) \right]
$$
and

$$
\left[log\sigma(x)\right]^\prime = 1 - \sigma(x) \\\
\left[log\left(1 - \sigma(x)\right)\right]^\prime = - \sigma(x)
$$Logistic Regression
For a binary classification problem, for an example $x = (x_1, x_2, \dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as:

$$
\begin{aligned}
h_{\theta}(1|x) &= \sigma(\theta_0 + \theta_1x_1 + \theta_2x_2 + \dotsb + \theta_nx_n) \\\
&= \sigma(\theta^{\mathrm{T}}x) \\\
&= \frac{1}{1+e^{-\theta^{\mathrm{T}}x}}
\end{aligned}
$$
Consequently, for the negative class,

$$
\begin{aligned}
h_{\theta}(0|x) &= 1 - \frac{1}{1+e^{-\theta^{\mathrm{T}}x}} \\\
&= \frac{1}{1+e^{\theta^{\mathrm{T}}x}} \\\
&= \sigma(-\theta^{\mathrm{T}}x)
\end{aligned}
$$Single sample cost function of logistic regression is expressed as:

$$
L(\theta) = -y_i \cdot \log(h_\theta(x_i)) -  (1-y_i) \cdot \log(1 - h_\theta(x_i))
$$
Notice that in the second term $1 - h_\theta(x_i)$ is the negative class probability"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/loss/loss/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-18T00:18:23+08:00"><meta property="article:modified_time" content="2019-07-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Loss in ML"><meta name=twitter:description content="Sigmoid
Sigmoid is one of the most used activation functions.

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$
It has nice mathematical proprities:

$$
\sigma^\prime(x) = \sigma(x) \left[ 1 - \sigma(x) \right]
$$
and

$$
\left[log\sigma(x)\right]^\prime = 1 - \sigma(x) \\\
\left[log\left(1 - \sigma(x)\right)\right]^\prime = - \sigma(x)
$$Logistic Regression
For a binary classification problem, for an example $x = (x_1, x_2, \dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as:

$$
\begin{aligned}
h_{\theta}(1|x) &= \sigma(\theta_0 + \theta_1x_1 + \theta_2x_2 + \dotsb + \theta_nx_n) \\\
&= \sigma(\theta^{\mathrm{T}}x) \\\
&= \frac{1}{1+e^{-\theta^{\mathrm{T}}x}}
\end{aligned}
$$
Consequently, for the negative class,

$$
\begin{aligned}
h_{\theta}(0|x) &= 1 - \frac{1}{1+e^{-\theta^{\mathrm{T}}x}} \\\
&= \frac{1}{1+e^{\theta^{\mathrm{T}}x}} \\\
&= \sigma(-\theta^{\mathrm{T}}x)
\end{aligned}
$$Single sample cost function of logistic regression is expressed as:

$$
L(\theta) = -y_i \cdot \log(h_\theta(x_i)) -  (1-y_i) \cdot \log(1 - h_\theta(x_i))
$$
Notice that in the second term $1 - h_\theta(x_i)$ is the negative class probability"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Loss in ML","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/loss/loss/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Loss in ML","name":"Loss in ML","description":"Sigmoid Sigmoid is one of the most used activation functions. $$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$ It has nice mathematical proprities: $$ \\sigma^\\prime(x) = \\sigma(x) \\left[ 1 - \\sigma(x) \\right] $$ and $$ \\left[log\\sigma(x)\\right]^\\prime = 1 - \\sigma(x) \\\\\\ \\left[log\\left(1 - \\sigma(x)\\right)\\right]^\\prime = - \\sigma(x) $$Logistic Regression For a binary classification problem, for an example $x = (x_1, x_2, \\dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as: $$ \\begin{aligned} h_{\\theta}(1|x) \u0026= \\sigma(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dotsb + \\theta_nx_n) \\\\\\ \u0026= \\sigma(\\theta^{\\mathrm{T}}x) \\\\\\ \u0026= \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\end{aligned} $$ Consequently, for the negative class, $$ \\begin{aligned} h_{\\theta}(0|x) \u0026= 1 - \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\\\\\ \u0026= \\frac{1}{1+e^{\\theta^{\\mathrm{T}}x}} \\\\\\ \u0026= \\sigma(-\\theta^{\\mathrm{T}}x) \\end{aligned} $$Single sample cost function of logistic regression is expressed as: $$ L(\\theta) = -y_i \\cdot \\log(h_\\theta(x_i)) - (1-y_i) \\cdot \\log(1 - h_\\theta(x_i)) $$ Notice that in the second term $1 - h_\\theta(x_i)$ is the negative class probability\n","keywords":[],"articleBody":"Sigmoid Sigmoid is one of the most used activation functions. $$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$ It has nice mathematical proprities: $$ \\sigma^\\prime(x) = \\sigma(x) \\left[ 1 - \\sigma(x) \\right] $$ and $$ \\left[log\\sigma(x)\\right]^\\prime = 1 - \\sigma(x) \\\\\\ \\left[log\\left(1 - \\sigma(x)\\right)\\right]^\\prime = - \\sigma(x) $$Logistic Regression For a binary classification problem, for an example $x = (x_1, x_2, \\dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as: $$ \\begin{aligned} h_{\\theta}(1|x) \u0026= \\sigma(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dotsb + \\theta_nx_n) \\\\\\ \u0026= \\sigma(\\theta^{\\mathrm{T}}x) \\\\\\ \u0026= \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\end{aligned} $$ Consequently, for the negative class, $$ \\begin{aligned} h_{\\theta}(0|x) \u0026= 1 - \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\\\\\ \u0026= \\frac{1}{1+e^{\\theta^{\\mathrm{T}}x}} \\\\\\ \u0026= \\sigma(-\\theta^{\\mathrm{T}}x) \\end{aligned} $$Single sample cost function of logistic regression is expressed as: $$ L(\\theta) = -y_i \\cdot \\log(h_\\theta(x_i)) - (1-y_i) \\cdot \\log(1 - h_\\theta(x_i)) $$ Notice that in the second term $1 - h_\\theta(x_i)$ is the negative class probability\nCross Entropy Cross entropy defines the distance between model output distribution and the groudtruth distribution. $$ H(y,p) = -\\sum_{i}y_i \\log(p_i) $$ Since the $y_i$ is the class label (1 for positive class, 0 for negative), essentially here we‚Äôre summing up the negative log probably of the positive label. What is the reason why we say that negative log likehood and cross entropy is equivalent.\nWhen normalization function (we can say activation function of last layer) is softmax function, namely, for each class $s_i$ the probability is given by $$ f(s)_i = \\frac{e^{s_i}}{ \\sum_j ^{C} e^{s_j}} $$ Given the above cross entropy equation, and there is only one positive class, the softmax cross entropy loss is: $$ L = -log(\\frac{e^{s_p}}{ \\sum_j ^{C} e^{s_j}}) $$ here $p$ stands for positive class. If we want to get the gradient of loss with respect to the logits ($s_i$ here), for positive class we can have $$ \\frac{\\partial{L}}{\\partial{s_p}} = \\left(\\frac{e^{s_p}}{ \\sum_j ^{C} e^{s_j}} - 1 \\right) \\\\\\ \\frac{\\partial{L}}{\\partial{s_n}} = \\left(\\frac{e^{s_n}}{ \\sum_j ^{C} e^{s_j}} - 1 \\right) $$ We can put this in one equation, which is what we commonly see as the graident of cross entropy loss for softmax $$ \\frac{\\partial{L}}{\\partial{s_i}} = p_i - y_i $$ $p_i$ is the probability and $y_i$ is the label, 1 for positive class and 0 for negative class.\nBinary Cross Entropy In the above section, we talked about softmax cross entropy loss, here we talk about binary cross entropy loss which is also called Sigmoid cross entropy loss.\nWe apply sigmoid function to the output logits before BCE. Notice that here we apply the function to each element in the tensor, all the elements are not related to each other, this is why BCE is widely used for multi-label classification task.\nFor each label, we can calculate the loss in the same way as the logistic regression loss.\nimport torch import numpy as np pred = np.array([[-0.4089, -1.2471, 0.5907], [-0.4897, -0.8267, -0.7349], [0.5241, -0.1246, -0.4751]]) # after sigmod, pred becomes # [[0.3992, 0.2232, 0.6435], # [0.3800, 0,3044, 0.3241], # [0.6281, 0.4689, 0.3834]] label = np.array([[0, 1, 1], [0, 0, 1], [1, 0, 1]]) # after cross entropy, pred becomes # [[-0.5095, -1.4997, -0.4408], take neg and avg 0.8167 # [-0.4780, -0.3630, -1.1267], take neg and avg 0.6559 # [-0.4651, -0.6328, -0.9587]] take neg and avg 0.6855 # 0 * ln0.3992 + (1-0) * ln(1-0.3992) = -0.5095 # (0.8167 + 0.6559 + 0.6855) / 3. = 0.7194 pred = torch.from_numpy(pred).float() label = torch.from_numpy(label).float() crition1 = torch.nn.BCEWithLogitsLoss() loss1 = crition1(pred, label) print(loss1) # 0.7193 crition2 = torch.nn.MultiLabelSoftMarginLoss() loss2 = crition2(pred, label) print(loss2) # 0.7193 Noise Contrastive Estimation Noise contrastive estimation or negative sampling is a commonly used computation trick in ML to deal with expansive softmax computation or intractable partition function in computation.\nThe derivation of NCE loss sometimes can be bewildering, but the idea is actually very simple. For example, in word2vec implementation, the negative sampling is to choose 1 positive target and 5 negative target, and calculate the binary cross entropy loss (binary logistic loss) and then do backward propagation.\nContrastive Loss CLIP Loss CLIP loss is the same with the paper from [3]. The negatives here are used for contrastive learning. However, they‚Äôre not using NCE method like word2vec. It‚Äôs more like softmax cross entropy.\nimport torch from torch import nn import torch.nn.functional as F import config as CFG from modules import ImageEncoder, TextEncoder, ProjectionHead class CLIPModel(nn.Module): def __init__( self, temperature=CFG.temperature, image_embedding=CFG.image_embedding, text_embedding=CFG.text_embedding, ): super().__init__() self.image_encoder = ImageEncoder() self.text_encoder = TextEncoder() self.image_projection = ProjectionHead(embedding_dim=image_embedding) self.text_projection = ProjectionHead(embedding_dim=text_embedding) self.temperature = temperature def forward(self, batch): # Getting Image and Text Features image_features = self.image_encoder(batch[\"image\"]) text_features = self.text_encoder( input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"] ) # Getting Image and Text Embeddings (with same dimension) image_embeddings = self.image_projection(image_features) text_embeddings = self.text_projection(text_features) # Calculating the Loss logits = (text_embeddings @ image_embeddings.T) / self.temperature images_similarity = image_embeddings @ image_embeddings.T texts_similarity = text_embeddings @ text_embeddings.T targets = F.softmax( (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1 ) texts_loss = cross_entropy(logits, targets, reduction='none') images_loss = cross_entropy(logits.T, targets.T, reduction='none') loss = (images_loss + texts_loss) / 2.0 # shape: (batch_size) return loss.mean() def cross_entropy(preds, targets, reduction='none'): log_softmax = nn.LogSoftmax(dim=-1) loss = (-targets * log_softmax(preds)).sum(1) if reduction == \"none\": return loss elif reduction == \"mean\": return loss.mean() if __name__ == '__main__': images = torch.randn(8, 3, 224, 224) input_ids = torch.randint(5, 300, size=(8, 25)) attention_mask = torch.ones(8, 25) batch = { 'image': images, 'input_ids': input_ids, 'attention_mask': attention_mask } CLIP = CLIPModel() loss = CLIP(batch) print(\"\") From MLE to Logistic Loss Model and likelihood We have binary outcomes $y_i\\in{0,1}$ and feature vectors $x_i\\in\\mathbb{R}^d$. The logistic model assumes $$ P(y_i=1\\mid x_i,w) = \\sigma(z_i),\\qquad z_i = x_i^\\top w, $$ where $\\sigma(z)=\\dfrac{1}{1+e^{-z}}$ is the logistic (sigmoid) function. Let $$ \\mu_i \\equiv \\sigma(z_i) = \\sigma(x_i^\\top w). $$Because $y_i$ is Bernoulli with parameter $\\mu_i$, the likelihood for one example is $$ p(y_i\\mid x_i,w) = \\mu_i^{y_i}(1-\\mu_i)^{1-y_i}. $$ Assuming independent samples, the full likelihood is $$ L(w)=\\prod_{i=1}^n \\mu_i^{y_i}(1-\\mu_i)^{1-y_i}. $$ Log-likelihood Take logs to get the log-likelihood, this turns product of probability to summation of probability which is much easier to handle. $$ \\ell(w) = \\log L(w) = \\sum_{i=1}^n \\big( y_i\\log\\mu_i + (1-y_i)\\log(1-\\mu_i)\\big). $$ Using $\\mu_i=\\sigma(x_i^\\top w)$ and the identity $\\log\\mu = -\\log(1+e^{-z})$, $\\log(1-\\mu) = -\\log(1+e^{z})$, this can be written as $$ \\ell(w)=\\sum_{i=1}^n \\big( y_i x_i^\\top w - \\log(1+e^{x_i^\\top w})\\big). $$ Maximize $\\ell(w)$ (or equivalently minimize the negative log-likelihood).\nGradient of the log-likelihood Differentiate $\\ell(w)=\\sum_i [y_i x_i^\\top w - \\log(1+e^{x_i^\\top w})]$ with respect to $w$.\nFor a single term: $$ \\frac{\\partial}{\\partial w}\\big(y_i x_i^\\top w\\big)=y_i x_i, \\qquad \\frac{\\partial}{\\partial w}\\log(1+e^{x_i^\\top w})=\\frac{e^{x_i^\\top w}}{1+e^{x_i^\\top w}},x_i=\\sigma(x_i^\\top w),x_i=\\mu_i x_i. $$ So $$ \\nabla \\ell(w) = \\sum_{i=1}^n (y_i - \\mu_i),x_i. $$Vector/matrix form: let $X$ be the $n\\times d$ design matrix $rows (x_i^\\top)$, $y$ the $n$-vector of labels, and $\\mu$ the $n$-vector of $\\mu_i$. Then $$ \\nabla \\ell(w) = X^\\top (y - \\mu). $$Setting $\\nabla\\ell(w)=0$ gives the score equations $X^\\top (y-\\mu)=0$, which are nonlinear in $w$ (because $\\mu$ depends on $w$). There is no closed-form solution ‚Äî we solve numerically.\nRelationship to cross-entropy loss Maximizing $\\ell(w)$ is equivalent to minimizing the negative log-likelihood (cross-entropy): $$ \\mathcal{L}(w) = -\\ell(w) = \\sum_{i=1}^n \\big( -y_i x_i^\\top w + \\log(1+e^{x_i^\\top w})\\big). $$ The gradient of this loss is $-\\nabla\\ell = X^\\top(\\mu - y)$, which is the form used in gradient descent.\nReference Learning Transferable Visual Models From Natural Language Supervision http://www.cnblogs.com/peghoty/p/3857839.html contrastive learning of medical visual representations from paired images and text https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/CLIP.py ","wordCount":"1207","inLanguage":"en-us","datePublished":"2019-06-18T00:18:23+08:00","dateModified":"2019-07-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/loss/loss/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>Loss in ML</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2019-06-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1207 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>3 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#sigmoid aria-label=Sigmoid>Sigmoid</a></li><li><a href=#logistic-regression aria-label="Logistic Regression">Logistic Regression</a></li><li><a href=#cross-entropy aria-label="Cross Entropy">Cross Entropy</a></li><li><a href=#binary-cross-entropy aria-label="Binary Cross Entropy">Binary Cross Entropy</a></li><li><a href=#noise-contrastive-estimation aria-label="Noise Contrastive Estimation">Noise Contrastive Estimation</a></li><li><a href=#contrastive-loss aria-label="Contrastive Loss">Contrastive Loss</a></li><li><a href=#clip-loss aria-label="CLIP Loss">CLIP Loss</a></li><li><a href=#from-mle-to-logistic-loss aria-label="From MLE to Logistic Loss">From MLE to Logistic Loss</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=sigmoid>Sigmoid<a hidden class=anchor aria-hidden=true href=#sigmoid>#</a></h2><p>Sigmoid is one of the most used activation functions.</p>$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$<p>It has nice mathematical proprities:</p>$$
\sigma^\prime(x) = \sigma(x) \left[ 1 - \sigma(x) \right]
$$<p>and</p>$$
\left[log\sigma(x)\right]^\prime = 1 - \sigma(x) \\\
\left[log\left(1 - \sigma(x)\right)\right]^\prime = - \sigma(x)
$$<h2 id=logistic-regression>Logistic Regression<a hidden class=anchor aria-hidden=true href=#logistic-regression>#</a></h2><p>For a binary classification problem, for an example $x = (x_1, x_2, \dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as:</p>$$
\begin{aligned}
h_{\theta}(1|x) &= \sigma(\theta_0 + \theta_1x_1 + \theta_2x_2 + \dotsb + \theta_nx_n) \\\
&= \sigma(\theta^{\mathrm{T}}x) \\\
&= \frac{1}{1+e^{-\theta^{\mathrm{T}}x}}
\end{aligned}
$$<p>Consequently, for the negative class,</p>$$
\begin{aligned}
h_{\theta}(0|x) &= 1 - \frac{1}{1+e^{-\theta^{\mathrm{T}}x}} \\\
&= \frac{1}{1+e^{\theta^{\mathrm{T}}x}} \\\
&= \sigma(-\theta^{\mathrm{T}}x)
\end{aligned}
$$<p>Single sample cost function of logistic regression is expressed as:</p>$$
L(\theta) = -y_i \cdot \log(h_\theta(x_i)) - (1-y_i) \cdot \log(1 - h_\theta(x_i))
$$<p>Notice that in the second term $1 - h_\theta(x_i)$ is the negative class probability</p><h2 id=cross-entropy>Cross Entropy<a hidden class=anchor aria-hidden=true href=#cross-entropy>#</a></h2><p>Cross entropy defines the distance between model output distribution and the groudtruth distribution.</p>$$
H(y,p) = -\sum_{i}y_i \log(p_i)
$$<p>Since the $y_i$ is the class label (1 for positive class, 0 for negative), essentially here we&rsquo;re summing up the negative log probably of the positive label. What is the reason why we say that negative log likehood and cross entropy is equivalent.</p><p>When normalization function (we can say activation function of last layer) is softmax function, namely, for each class $s_i$ the probability is given by</p>$$
f(s)_i = \frac{e^{s_i}}{ \sum_j ^{C} e^{s_j}}
$$<p>Given the above cross entropy equation, and there is only one positive class, the softmax cross entropy loss is:</p>$$
L = -log(\frac{e^{s_p}}{ \sum_j ^{C} e^{s_j}})
$$<p>here $p$ stands for positive class.
If we want to get the gradient of loss with respect to the logits ($s_i$ here), for positive class we can have</p>$$
\frac{\partial{L}}{\partial{s_p}} = \left(\frac{e^{s_p}}{ \sum_j ^{C} e^{s_j}} - 1 \right) \\\
\frac{\partial{L}}{\partial{s_n}} = \left(\frac{e^{s_n}}{ \sum_j ^{C} e^{s_j}} - 1 \right)
$$<p>We can put this in one equation, which is what we commonly see as the graident of cross entropy loss for softmax</p>$$
\frac{\partial{L}}{\partial{s_i}} = p_i - y_i
$$<p>$p_i$ is the probability and $y_i$ is the label, 1 for positive class and 0 for negative class.</p><h2 id=binary-cross-entropy>Binary Cross Entropy<a hidden class=anchor aria-hidden=true href=#binary-cross-entropy>#</a></h2><p>In the above section, we talked about softmax cross entropy loss, here we talk about binary cross entropy loss which is also called Sigmoid cross entropy loss.</p><p>We apply sigmoid function to the output logits before BCE. Notice that here we apply the function to each element in the tensor, all the elements are not related to each other, this is why BCE is widely used for <strong>multi-label</strong> classification task.</p><p>For each label, we can calculate the loss in the same way as the logistic regression loss.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.4089</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.2471</span>, <span style=color:#ae81ff>0.5907</span>],
</span></span><span style=display:flex><span>                [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.4897</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8267</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7349</span>],
</span></span><span style=display:flex><span>                [<span style=color:#ae81ff>0.5241</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1246</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4751</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># after sigmod, pred becomes</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [[0.3992, 0.2232, 0.6435],</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [0.3800, 0,3044, 0.3241],</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [0.6281, 0.4689, 0.3834]]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>label <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>                  [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>                  [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># after cross entropy, pred becomes</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [[-0.5095, -1.4997, -0.4408], take neg and avg 0.8167</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [-0.4780, -0.3630, -1.1267], take neg and avg 0.6559</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [-0.4651, -0.6328, -0.9587]] take neg and avg 0.6855</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 0 * ln0.3992 + (1-0) * ln(1-0.3992) = -0.5095</span>
</span></span><span style=display:flex><span><span style=color:#75715e># (0.8167 + 0.6559 + 0.6855) / 3. = 0.7194</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pred <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(pred)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>label <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(label)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>crition1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>BCEWithLogitsLoss()
</span></span><span style=display:flex><span>loss1 <span style=color:#f92672>=</span> crition1(pred, label)
</span></span><span style=display:flex><span>print(loss1) <span style=color:#75715e># 0.7193</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>crition2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>MultiLabelSoftMarginLoss()
</span></span><span style=display:flex><span>loss2 <span style=color:#f92672>=</span> crition2(pred, label)
</span></span><span style=display:flex><span>print(loss2) <span style=color:#75715e># 0.7193</span>
</span></span></code></pre></div><h2 id=noise-contrastive-estimation>Noise Contrastive Estimation<a hidden class=anchor aria-hidden=true href=#noise-contrastive-estimation>#</a></h2><p>Noise contrastive estimation or negative sampling is a commonly used computation trick in ML to deal with expansive softmax computation or intractable partition function in computation.</p><p>The derivation of NCE loss sometimes can be bewildering, but the idea is actually very simple. For example, in word2vec implementation, the negative sampling is to choose 1 positive target and 5 negative target, and calculate the binary cross entropy loss (binary logistic loss) and then do backward propagation.</p><h2 id=contrastive-loss>Contrastive Loss<a hidden class=anchor aria-hidden=true href=#contrastive-loss>#</a></h2><h2 id=clip-loss>CLIP Loss<a hidden class=anchor aria-hidden=true href=#clip-loss>#</a></h2><p>CLIP loss is the same with the paper from [3]. The negatives here are used for contrastive learning. However, they&rsquo;re not using NCE method like word2vec. It&rsquo;s more like softmax cross entropy.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> config <span style=color:#66d9ef>as</span> CFG
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> modules <span style=color:#f92672>import</span> ImageEncoder, TextEncoder, ProjectionHead
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CLIPModel</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(
</span></span><span style=display:flex><span>        self,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span>CFG<span style=color:#f92672>.</span>temperature,
</span></span><span style=display:flex><span>        image_embedding<span style=color:#f92672>=</span>CFG<span style=color:#f92672>.</span>image_embedding,
</span></span><span style=display:flex><span>        text_embedding<span style=color:#f92672>=</span>CFG<span style=color:#f92672>.</span>text_embedding,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_encoder <span style=color:#f92672>=</span> ImageEncoder()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text_encoder <span style=color:#f92672>=</span> TextEncoder()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_projection <span style=color:#f92672>=</span> ProjectionHead(embedding_dim<span style=color:#f92672>=</span>image_embedding)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text_projection <span style=color:#f92672>=</span> ProjectionHead(embedding_dim<span style=color:#f92672>=</span>text_embedding)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>temperature <span style=color:#f92672>=</span> temperature
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, batch):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Getting Image and Text Features</span>
</span></span><span style=display:flex><span>        image_features <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>image_encoder(batch[<span style=color:#e6db74>&#34;image&#34;</span>])
</span></span><span style=display:flex><span>        text_features <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>text_encoder(
</span></span><span style=display:flex><span>            input_ids<span style=color:#f92672>=</span>batch[<span style=color:#e6db74>&#34;input_ids&#34;</span>], attention_mask<span style=color:#f92672>=</span>batch[<span style=color:#e6db74>&#34;attention_mask&#34;</span>]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># Getting Image and Text Embeddings (with same dimension)</span>
</span></span><span style=display:flex><span>        image_embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>image_projection(image_features)
</span></span><span style=display:flex><span>        text_embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>text_projection(text_features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Calculating the Loss</span>
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> (text_embeddings <span style=color:#f92672>@</span> image_embeddings<span style=color:#f92672>.</span>T) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>temperature
</span></span><span style=display:flex><span>        images_similarity <span style=color:#f92672>=</span> image_embeddings <span style=color:#f92672>@</span> image_embeddings<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>        texts_similarity <span style=color:#f92672>=</span> text_embeddings <span style=color:#f92672>@</span> text_embeddings<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>        targets <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(
</span></span><span style=display:flex><span>            (images_similarity <span style=color:#f92672>+</span> texts_similarity) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>temperature, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        texts_loss <span style=color:#f92672>=</span> cross_entropy(logits, targets, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;none&#39;</span>)
</span></span><span style=display:flex><span>        images_loss <span style=color:#f92672>=</span> cross_entropy(logits<span style=color:#f92672>.</span>T, targets<span style=color:#f92672>.</span>T, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;none&#39;</span>)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span>  (images_loss <span style=color:#f92672>+</span> texts_loss) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2.0</span> <span style=color:#75715e># shape: (batch_size)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy</span>(preds, targets, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;none&#39;</span>):
</span></span><span style=display:flex><span>    log_softmax <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LogSoftmax(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> (<span style=color:#f92672>-</span>targets <span style=color:#f92672>*</span> log_softmax(preds))<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> reduction <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;none&#34;</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> reduction <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;mean&#34;</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    images <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>)
</span></span><span style=display:flex><span>    input_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>300</span>, size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>25</span>))
</span></span><span style=display:flex><span>    attention_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>25</span>)
</span></span><span style=display:flex><span>    batch <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;image&#39;</span>: images,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;input_ids&#39;</span>: input_ids,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;attention_mask&#39;</span>: attention_mask
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    CLIP <span style=color:#f92672>=</span> CLIPModel()
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> CLIP(batch)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;&#34;</span>)
</span></span></code></pre></div><h2 id=from-mle-to-logistic-loss>From MLE to Logistic Loss<a hidden class=anchor aria-hidden=true href=#from-mle-to-logistic-loss>#</a></h2><ol><li>Model and likelihood</li></ol><p>We have binary outcomes $y_i\in{0,1}$ and feature vectors $x_i\in\mathbb{R}^d$. The logistic model assumes</p>$$
P(y_i=1\mid x_i,w) = \sigma(z_i),\qquad z_i = x_i^\top w,
$$<p>where $\sigma(z)=\dfrac{1}{1+e^{-z}}$ is the logistic (sigmoid) function. Let</p>$$
\mu_i \equiv \sigma(z_i) = \sigma(x_i^\top w).
$$<p>Because $y_i$ is Bernoulli with parameter $\mu_i$, the likelihood for one example is</p>$$
p(y_i\mid x_i,w) = \mu_i^{y_i}(1-\mu_i)^{1-y_i}.
$$<p>Assuming independent samples, the full likelihood is</p>$$
L(w)=\prod_{i=1}^n \mu_i^{y_i}(1-\mu_i)^{1-y_i}.
$$<ol start=2><li>Log-likelihood</li></ol><p>Take logs to get the log-likelihood, this turns product of probability to summation of probability which is much easier to handle.</p>$$
\ell(w) = \log L(w)
= \sum_{i=1}^n \big( y_i\log\mu_i + (1-y_i)\log(1-\mu_i)\big).
$$<p>Using $\mu_i=\sigma(x_i^\top w)$ and the identity $\log\mu = -\log(1+e^{-z})$, $\log(1-\mu) = -\log(1+e^{z})$, this can be written as</p>$$
\ell(w)=\sum_{i=1}^n \big( y_i x_i^\top w - \log(1+e^{x_i^\top w})\big).
$$<p>Maximize $\ell(w)$ (or equivalently minimize the negative log-likelihood).</p><ol start=3><li>Gradient of the log-likelihood</li></ol><p>Differentiate $\ell(w)=\sum_i [y_i x_i^\top w - \log(1+e^{x_i^\top w})]$ with respect to $w$.</p><p>For a single term:</p>$$
\frac{\partial}{\partial w}\big(y_i x_i^\top w\big)=y_i x_i,
\qquad
\frac{\partial}{\partial w}\log(1+e^{x_i^\top w})=\frac{e^{x_i^\top w}}{1+e^{x_i^\top w}},x_i=\sigma(x_i^\top w),x_i=\mu_i x_i.
$$<p>So</p>$$
\nabla \ell(w) = \sum_{i=1}^n (y_i - \mu_i),x_i.
$$<p>Vector/matrix form: let $X$ be the $n\times d$ design matrix $rows (x_i^\top)$, $y$ the $n$-vector of labels, and $\mu$ the $n$-vector of $\mu_i$. Then</p>$$
\nabla \ell(w) = X^\top (y - \mu).
$$<p>Setting $\nabla\ell(w)=0$ gives the score equations $X^\top (y-\mu)=0$, which are nonlinear in $w$ (because $\mu$ depends on $w$). There is no closed-form solution ‚Äî we solve numerically.</p><ol start=4><li>Relationship to cross-entropy loss</li></ol><p>Maximizing $\ell(w)$ is equivalent to minimizing the negative log-likelihood (cross-entropy):</p>$$
\mathcal{L}(w) = -\ell(w) = \sum_{i=1}^n \big( -y_i x_i^\top w + \log(1+e^{x_i^\top w})\big).
$$<p>The gradient of this loss is $-\nabla\ell = X^\top(\mu - y)$, which is the form used in gradient descent.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ol><li><a href=https://arxiv.org/pdf/2103.00020.pdf>Learning Transferable Visual Models From Natural Language Supervision</a></li><li><a href=http://www.cnblogs.com/peghoty/p/3857839.html>http://www.cnblogs.com/peghoty/p/3857839.html</a></li><li><a href=https://arxiv.org/abs/2010.00747>contrastive learning of medical visual representations from paired images and text</a></li><li><a href=https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/CLIP.py>https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/CLIP.py</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/makefile/><span class=title>¬´</span><br><span>Makefile</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/git/git/><span class=title>¬ª</span><br><span>Git</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Loss in ML on twitter" href="https://twitter.com/intent/tweet/?text=Loss%20in%20ML&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2floss%2floss%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Loss in ML on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2floss%2floss%2f&amp;title=Loss%20in%20ML&amp;summary=Loss%20in%20ML&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2floss%2floss%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Loss in ML on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2floss%2floss%2f&title=Loss%20in%20ML"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Loss in ML on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2floss%2floss%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Loss in ML on whatsapp" href="https://api.whatsapp.com/send?text=Loss%20in%20ML%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2floss%2floss%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Loss in ML on telegram" href="https://telegram.me/share/url?text=Loss%20in%20ML&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2floss%2floss%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>