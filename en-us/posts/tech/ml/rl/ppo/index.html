<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PPO and Its Implementation | Jun's Blog</title><meta name=keywords content><meta name=description content="In this blog, I&rsquo;ll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.
Basics
Monte Carlo Approximation
Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as
$$
\mathbb{E_{x\sim p(x)}}\left(f(x)\right) = \int{f(x)p(x)} dx
$$
when it&rsquo;s a continuous random variable with a probability density function of $p$, or

$$
\mathbb{E}\left(f(x)\right) = \sum_x{f(x)p(x)}
$$
when it&rsquo;s a discrete random variable with probability mass function of $p$.
Then the Monte Carlo approximation says that the expectation is:

$$
\mathbb{E}\left(f(x)\right) \approx \frac{1}{N}\sum_{i=1}^{N}{f(x_i)}
$$assuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/rl/ppo/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/rl/ppo/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="PPO and Its Implementation"><meta property="og:description" content="In this blog, I&rsquo;ll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.
Basics
Monte Carlo Approximation
Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as
$$
\mathbb{E_{x\sim p(x)}}\left(f(x)\right) = \int{f(x)p(x)} dx
$$
when it&rsquo;s a continuous random variable with a probability density function of $p$, or

$$
\mathbb{E}\left(f(x)\right) = \sum_x{f(x)p(x)}
$$
when it&rsquo;s a discrete random variable with probability mass function of $p$.
Then the Monte Carlo approximation says that the expectation is:

$$
\mathbb{E}\left(f(x)\right) \approx \frac{1}{N}\sum_{i=1}^{N}{f(x_i)}
$$assuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/rl/ppo/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-05T00:18:23-07:00"><meta property="article:modified_time" content="2024-07-05T00:18:23-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="PPO and Its Implementation"><meta name=twitter:description content="In this blog, I&rsquo;ll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.
Basics
Monte Carlo Approximation
Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as
$$
\mathbb{E_{x\sim p(x)}}\left(f(x)\right) = \int{f(x)p(x)} dx
$$
when it&rsquo;s a continuous random variable with a probability density function of $p$, or

$$
\mathbb{E}\left(f(x)\right) = \sum_x{f(x)p(x)}
$$
when it&rsquo;s a discrete random variable with probability mass function of $p$.
Then the Monte Carlo approximation says that the expectation is:

$$
\mathbb{E}\left(f(x)\right) \approx \frac{1}{N}\sum_{i=1}^{N}{f(x_i)}
$$assuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"PPO and Its Implementation","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/rl/ppo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PPO and Its Implementation","name":"PPO and Its Implementation","description":"In this blog, I\u0026rsquo;ll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.\nBasics Monte Carlo Approximation Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as\n$$ \\mathbb{E_{x\\sim p(x)}}\\left(f(x)\\right) = \\int{f(x)p(x)} dx $$ when it\u0026rsquo;s a continuous random variable with a probability density function of $p$, or $$ \\mathbb{E}\\left(f(x)\\right) = \\sum_x{f(x)p(x)} $$ when it\u0026rsquo;s a discrete random variable with probability mass function of $p$. Then the Monte Carlo approximation says that the expectation is: $$ \\mathbb{E}\\left(f(x)\\right) \\approx \\frac{1}{N}\\sum_{i=1}^{N}{f(x_i)} $$assuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution.\n","keywords":[],"articleBody":"In this blog, I‚Äôll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.\nBasics Monte Carlo Approximation Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as\n$$ \\mathbb{E_{x\\sim p(x)}}\\left(f(x)\\right) = \\int{f(x)p(x)} dx $$ when it‚Äôs a continuous random variable with a probability density function of $p$, or $$ \\mathbb{E}\\left(f(x)\\right) = \\sum_x{f(x)p(x)} $$ when it‚Äôs a discrete random variable with probability mass function of $p$. Then the Monte Carlo approximation says that the expectation is: $$ \\mathbb{E}\\left(f(x)\\right) \\approx \\frac{1}{N}\\sum_{i=1}^{N}{f(x_i)} $$assuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution.\nImportance Sampling In reality, it could be very challenging to sample data according to the distribution $p(x)$ as it is usually unknown to us. A workaround is to have another known distribution $q(x)$, and define the expectation as: $$ \\mathbb{E_{x\\sim p(x)}}[f] = \\int{q(x)\\frac{p(x)}{q(x)}f(x)} dx $$ This can be seen as the expectation of function $\\frac{p(x)}{q(x)}f(x)$ according to the distribution of $q(x)$. The distribution is sometimes called the proposal distribution. Then the expectation can be estimated as $$ \\mathbb{E_{x\\sim q(x)}}[f] \\approx \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{p(x_i)}{q(x_i)}f(x_i)} $$ Here the ratios $\\frac{p(x_i)}{q(x_i)}$ are referred sa the importance weights. The above derivation looks nice. However, we need to notice that the although the expectation is similar in both cases, the variance is different:\n$$ Var_{x\\sim p(x)}[f] = \\mathbb{E_{x\\sim p(x)}}[f(x)^2] - ({\\mathbb{E_{x\\sim p(x)}}[f(x)]})^2 $$$$ \\begin{aligned} Var_{x\\sim q(x)}[f] \u0026= \\mathbb{E_{x\\sim q(x)}}[({\\frac{p(x_i)}{q(x_i)}f(x_i)})^2] - (\\mathbb{E_{x\\sim q(x)}}[{\\frac{p(x_i)}{q(x_i)}f(x_i)}])^2 \\\\\\ \u0026= \\mathbb{E_{x\\sim p(x)}}[{\\frac{p(x_i)}{q(x_i)}f(x_i)^2}] - (\\mathbb{E_{x\\sim p(x)}}[f(x_i)])^2 \\end{aligned} $$ Notice that the second equation here, in the second step derivation, the expectation is relative to distribution of $p(x)$. From the above two equations, we can see that to make the sampling distribution as close as possible to the original distribution, the ratio $\\frac{p(x_i)}{q(x_i)}$ has to be close to 1.\nPolicy Gradient First, let‚Äôs remind ourselves some basics. The discounted return for a trajectory is defined as: $$ U_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + ... $$Consequently, the action-value function is defined as $$ Q_{\\pi}(s_t, a_t) = \\mathbb{E_t}[U_t|S_t=s_t, A_t=a_t] $$State-value function (or value function) can be calculated as: $$ V_{\\pi}(s_t) = \\mathbb{E_A}[Q_{\\pi}(s_t, A)] = \\sum_a \\pi(a|s_t) \\cdot Q_{\\pi}(s_t, a) $$In policy gradient algorithm, the policy function $\\pi(a|s_t)$ is approximated by policy network $\\pi(a|s_t; \\theta)$. $\\theta$ here is the neural network model parameters. Then the policy-based learning is to maximize the objective function $$ \\begin{aligned} J(\\theta) \u0026= \\mathbb{E_S}[V(S; \\theta)] \\\\\\ \u0026= \\sum_{s\\in S} d_{\\pi}(s) V_{\\pi}(s_t; \\theta) \\\\\\ \u0026= \\sum_{s\\in S} d_{\\pi}(s) \\sum_a \\pi(a|s_t; \\theta) \\cdot Q_{\\pi}(s_t, a) \\end{aligned} $$where $d_{\\pi}(s)$ is the stationary distribution of Markov chain for $\\pi_{\\theta}$, namely the state distribution under policy $\\pi$. Now we know the objective function of the policy-based algorithm, we can learn the parameters $\\theta$ through policy gradiet ascent.\nNow we can look at how to get the policy gradient. Since the first summation of the last step in the above equation has nothing to do with $\\theta$, so we can focus on getting the derivatives of the value function $V_{\\pi}(s; \\theta)$. Using chain rule, it‚Äôs easy to get: $$ \\begin{aligned} \\frac{\\partial{V(s; \\theta)}}{\\partial{\\theta}} \u0026= \\sum_a \\frac{\\partial{\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\\\\\ \u0026= \\sum_a \\pi(a|s_t; \\theta) \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\\\\\ \u0026= \\mathbb{E_A}\\left[ \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\right] \\end{aligned} $$ The last step assumes that $\\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a)$ follows a distribution of $\\pi(a|s_t; \\theta)$ with respect to the random variable $A$.\nLet‚Äôs take another look at the policy gradient here. First, in practice, when we calculate the expectation we can use Monte Carlo Approximation. The gradient here becomes summations as below:\n$$ \\nabla_{\\theta}(J(\\theta)) = \\sum_{t} \\nabla_{\\theta}{\\log\\pi (a|s; \\theta)} \\cdot Q_{\\pi}(s, a) $$This is also called Monte Carlo policy gradient. Since gradient is a direction, this formula shows that policy gradient estimation is the direction of the steepest increase in reward/return. When reward is larger, the policy gradient will be larger.\nTemporal Difference (TD) Learning Temporal Difference (TD) learning is one of the core concepts in Reinforcement Learning. Temporal difference algorithm always aims to bring the expected prediction and the new prediction together, thus matching expectations with reality and gradually increasing the accuracy of the entire chain of prediction.\nThe most basic version is TD(0) method. Specifically, if our agent is in a current state $s_t$, takes the action $a_t$ and receives the reward $r_t$, then we update our estimate of $V$ following\n$$ V(s_t) \\xleftarrow[]{} V(s_t) + \\alpha[r_{t+1} + \\gamma V(s_{t+1}) ‚Äì V(s_t)] $$Here $r_{t+1} + \\gamma V(s_{t+1})$ is TD target and $r_{t+1} + \\gamma V(s_{t+1}) ‚Äì V(s_t)$ is called TD error ($\\delta$).\nThere is SARSA (state-action-reward-state-action), where we replace the value function as the action-state value function.\n$$ Q(s_t, a_t) \\xleftarrow[]{} Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) ‚Äì Q(s_t, a_t)] $$And TD with Q-learning $$ Q(s_t, a_t) \\xleftarrow[]{} Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) ‚Äì Q(s_t, a_t)] $$REINFORCE Since $Q_{\\pi}(s, a)$ is the expectation of the return, we can once again use Monte Carlo approximation, $$ \\begin{aligned} Q_{\\pi}(s_t, a_t) \u0026= u_t \\\\\\ \u0026= \\sum_{i=t}^{N} {\\gamma^{i-t} \\cdot r_{i}} \\end{aligned} $$ The above MCPG actually gives us a practical algorithm to do policy gradient based RL. Let‚Äôs summarize it as follows:\nPlay one episode of game to get the trajectory: $s_1, a_1, r_1, s_2, a_2, r_2, ...$ Estimate all $q_t \\approx u_t$ using above equation Differentiate policy network to get $d_{\\theta, t}$ Compute policy gradient $g(a_t, \\theta_t) = q_t \\cdot d_{\\theta, t}$ Advantage Function and Generalized Advantage Estimation The above equation is the vanilla policy gradient method. More policy gradient algorithms are proposed later to reduce high variance of the vanilla version. John Schulman‚Äôs GAE paper summarized all the improvement methods. In the derivation, the policy gradient is represented as $$ \\frac{\\partial{V(s; \\theta)}}{\\partial{\\theta}} = \\mathbb{E_A}\\left[ \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] $$ where $\\hat{A_t}(s, a)$ is the advantage function. In implementation, we construct loss function in a way such that the policy gradient $g$ equals to the above result $$ L(\\theta) = \\mathbb{E_t}\\left[ \\log\\pi (a|s; \\theta) \\hat{A_t}(s, a) \\right] $$The idea is that the Advantage function calculates how better taking that action at a state is compared to the average value of the state. It‚Äôs subtracting the mean value of the state from the state action pair. Mathematically, $A(s_t, a_t) = Q(s_t, a_t) ‚àí V (s_t)$, where $Q(s_t, a_t)$ is the action-value function, representing the expected return after taking action at at state $s$, and $V (s_t)$ is the value function, representing the average expected return at state $s_t$.\nBased on the above advantage definition, we have $$ \\begin{aligned} \\hat{A_t^{(1)}} \u0026= r_t + \\gamma V(s_{t+1}) - V(s) \\\\\\ \\hat{A_t^{(2)}} \u0026= r_t + \\gamma r_{t+1} +\\gamma^2 V(s_{t+2}) - V(s) \\\\\\ ...\\\\\\ \\hat{A_t^{(\\infty)}} \u0026= r_t + \\gamma r_{t+1} +\\gamma^2 r_{t+2} + ... - V(s) \\end{aligned} $$Notice that $\\hat{A_t^{(1)}}$ has high bias, low variance, whilst $\\hat{A_t^{(\\infty)}}$ is unbiased, high variance. A weighted average of $\\hat{A_t^{(k)}}$ can be used to balance bias and variance. $$\\hat{A_t} = \\hat{A_t^{GAE}} = \\frac{\\sum_k w_k \\hat{A_t^{(k)}}}{\\sum_k w_k}$$ We set $w_k = \\lambda^{k-1}$, this gives clean calculation for $\\hat{A_t}$. Below we have the recursion equations. (Refer to [11] to learn how to derive the second equation here.)\n$$ \\begin{aligned} \\delta_t \u0026= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\\ \\hat{A_t} \u0026= \\delta_t + \\gamma \\lambda \\delta_{t+1} + ... + (\\gamma \\lambda)^{T - t + 1} \\delta_{T - 1} \\\\\\ \u0026= \\delta_t + \\gamma \\lambda \\hat{A_{t+1}} \\end{aligned} $$Actor-Critic Algorithm There we give a recap of how actor-critic method works. In Actor-Critic algorithm, we use one neural network $\\pi(a|s; \\theta)$ to approximate policy function $\\pi(a|s)$ and use another neural network $q(s, a; w)$ to approximate value function $Q_{\\pi}(s, a)$.\nObserve state $s_t$, and randomly sample action from policy $a_t \\sim \\pi(\\cdot | s_t; \\Theta_t)$ Let agent perform action $a_t$, and get new state $s_{t+1}$ and reward $r_t$ from environment Randomly sample $\\tilde{a}_{t+1} \\sim \\pi(\\cdot | s_t; \\Theta_t)$ without performing the action Evaluate value network: $q_t = q(s_t, a_t; W_t)$ and $q_{t+1} = q(s_{t+1}, \\tilde{a}_{t+1}; W_t)$ Compute TD error: $\\delta_t = q_t - (r_t + \\gamma \\cdot q_{t+1})$ Differentiate value network: $d_{w,t} = \\frac{\\partial{q(s_t, a_t, w)}}{\\partial{w}}$ (autograd will do this for us) Update value network: $ w_{t+1} = w_t - \\alpha \\cdot \\delta_t \\cdot d_{w, t}$ Differentiate policy network: $ d_{\\theta, t} = \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} $ (again autograd will do this for us) Update policy network: $\\theta_{t+1} = \\theta_t + \\beta \\cdot q_t \\cdot d_{\\theta, t}$. We can also use: $\\theta_{t+1} = \\theta_t + \\beta \\cdot \\delta_t \\cdot d_{\\theta, t}$ to update policy network. This is called policy gradient with baseline. Essentially, the algorithm alternates between sampling and optimization. The expectation in the above equation indicates that we need to average over a finite batch of empirical samples. Proximal Policy Optimization Vanilla policy gradient method uses on-policy update. Concretely, the algorithm samples empirical data from a policy network $\\pi_{\\theta}$ parameterized with $\\theta$. After updating the network itself, the new policy network is $\\pi_{\\theta_{new}}$ and the old policy $\\pi_{\\theta}$ is out of use and future sampling will be from $\\pi_{\\theta_{new}}$. This whole process is not efficient enough. The solution to this is to reuse the old samples to achieve off-policy training. From above importance sampling section, we know that:\n$$ \\mathbb{E_{x\\sim p(x)}}\\left[f \\right] = \\mathbb{E_{x\\sim q(x)}} \\left[ \\frac{p(x_i)}{q(x_i)}f(x_i) \\right] $$Similarly, we can make a change to the objective function of our policy gradient, and the resulting policy gradient will become $$ \\begin{aligned} g \u0026= \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta}}}\\left[ \\frac{\\partial{\\log\\pi (a_t|s_t; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] \\\\\\ \u0026= \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t; \\theta)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\frac{\\partial{\\log\\pi (a_t|s_t; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] \\end{aligned} $$ Consequently, the loss becomes\n$$ L(\\theta) = \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\hat{A_t}(s, a) \\right] $$ This is so-called surrogate objective function. In the above section, we mentioned how to use chain rule to get the expectation format of gradient, here we just to reverse the process to get the above loss function.\nIn the importance sampling section, we saw that the variance of new distribution could be large when the proposal distribution is not so close to the original distribution. Thus, to deal with this, people add KL diveragence to the loss function to limit the old and new policy difference. Using Largrangian dual method, we can add this constraint to the objective function:\n$$ L(\\theta) = \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\hat{A_t}(s, a) - \\beta KL[\\pi_{\\theta_{old}}(a_t|s_t), \\pi_{\\theta}(a_t|s_t)]\\right] $$Implementation For language generation task, generating a token is an action. Agent is the target language model we want to train.\nHere we first look at the implementation from Deepspeed-chat model. The actor-critic algorithm requires to load four model in training: actor model, critic model, reference model and reward mdoel. Actor model is the poliy network and critice model is the value network. Reference model and reward model are frozen in training. Reference model is used to contrain the actor model predictions so that they won‚Äôt divege too much. Reward model gives the current step reward.\nReferences [1] High-Dimensional Continuous Control Using Generalized Advantage Estimation [2] Proximal Policy Optimization Algorithms [3] Policy Gradient Methods for Reinforcement Learning with Function Approximation [4] Dueling Network Architectures for Deep Reinforcement Learning [5] https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/schedule.html [6] https://github.com/wangshusen/DRL [7] https://www.davidsilver.uk/teaching/ [8] Fine-Tuning Language Models from Human Preferences [9] https://zhuanlan.zhihu.com/p/677607581 [10] DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [11] Secrets of RLHF in Large Language Models Part I: PPO [12] Secrets of RLHF in Large Language Models Part II: Reward Modeling [13] The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization [14] Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study [15] Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO [16] Advanced Tricks for Training Large Language Models with Proximal Policy Optimization\n","wordCount":"2010","inLanguage":"en-us","datePublished":"2024-07-05T00:18:23-07:00","dateModified":"2024-07-05T00:18:23-07:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/rl/ppo/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>PPO and Its Implementation</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-07-05
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>2010 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>5 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#basics aria-label=Basics>Basics</a><ul><li><a href=#monte-carlo-approximation aria-label="Monte Carlo Approximation">Monte Carlo Approximation</a></li><li><a href=#importance-sampling aria-label="Importance Sampling">Importance Sampling</a></li><li><a href=#policy-gradient aria-label="Policy Gradient">Policy Gradient</a></li><li><a href=#temporal-difference-td-learning aria-label="Temporal Difference (TD) Learning">Temporal Difference (TD) Learning</a></li><li><a href=#reinforce aria-label=REINFORCE>REINFORCE</a></li><li><a href=#advantage-function-and-generalized-advantage-estimation aria-label="Advantage Function and Generalized Advantage Estimation">Advantage Function and Generalized Advantage Estimation</a></li><li><a href=#actor-critic-algorithm aria-label="Actor-Critic Algorithm">Actor-Critic Algorithm</a></li><li><a href=#proximal-policy-optimization aria-label="Proximal Policy Optimization">Proximal Policy Optimization</a></li></ul></li><li><a href=#implementation aria-label=Implementation>Implementation</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>In this blog, I&rsquo;ll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.</p><h2 id=basics>Basics<a hidden class=anchor aria-hidden=true href=#basics>#</a></h2><h3 id=monte-carlo-approximation>Monte Carlo Approximation<a hidden class=anchor aria-hidden=true href=#monte-carlo-approximation>#</a></h3><p>Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as</p>$$
\mathbb{E_{x\sim p(x)}}\left(f(x)\right) = \int{f(x)p(x)} dx
$$<p>when it&rsquo;s a continuous random variable with a probability density function of $p$, or</p>$$
\mathbb{E}\left(f(x)\right) = \sum_x{f(x)p(x)}
$$<p>when it&rsquo;s a discrete random variable with probability mass function of $p$.
Then the Monte Carlo approximation says that the expectation is:</p>$$
\mathbb{E}\left(f(x)\right) \approx \frac{1}{N}\sum_{i=1}^{N}{f(x_i)}
$$<p>assuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution.</p><h3 id=importance-sampling>Importance Sampling<a hidden class=anchor aria-hidden=true href=#importance-sampling>#</a></h3><p>In reality, it could be very challenging to sample data according to the distribution $p(x)$ as it is usually unknown to us. A workaround is to have another known distribution $q(x)$, and define the expectation as:</p>$$
\mathbb{E_{x\sim p(x)}}[f] = \int{q(x)\frac{p(x)}{q(x)}f(x)} dx
$$<p>This can be seen as the expectation of function $\frac{p(x)}{q(x)}f(x)$ according to the distribution of $q(x)$. The distribution is sometimes called the <strong>proposal distribution</strong>. Then the expectation can be estimated as</p>$$
\mathbb{E_{x\sim q(x)}}[f] \approx \frac{1}{N}\sum_{i=1}^{N}{\frac{p(x_i)}{q(x_i)}f(x_i)}
$$<p>Here the ratios $\frac{p(x_i)}{q(x_i)}$ are referred sa the importance weights.
The above derivation looks nice. However, we need to notice that the although the expectation is similar in both cases, the variance is different:</p>$$
Var_{x\sim p(x)}[f] = \mathbb{E_{x\sim p(x)}}[f(x)^2] - ({\mathbb{E_{x\sim p(x)}}[f(x)]})^2
$$$$
\begin{aligned}
Var_{x\sim q(x)}[f] &= \mathbb{E_{x\sim q(x)}}[({\frac{p(x_i)}{q(x_i)}f(x_i)})^2] - (\mathbb{E_{x\sim q(x)}}[{\frac{p(x_i)}{q(x_i)}f(x_i)}])^2 \\\
&= \mathbb{E_{x\sim p(x)}}[{\frac{p(x_i)}{q(x_i)}f(x_i)^2}] - (\mathbb{E_{x\sim p(x)}}[f(x_i)])^2
\end{aligned}
$$<p>Notice that the second equation here, in the second step derivation, the expectation is relative to distribution of $p(x)$. From the above two equations, we can see that to make the sampling distribution as close as possible to the original distribution, the ratio $\frac{p(x_i)}{q(x_i)}$ has to be close to 1.</p><h3 id=policy-gradient>Policy Gradient<a hidden class=anchor aria-hidden=true href=#policy-gradient>#</a></h3><p>First, let&rsquo;s remind ourselves some basics. The discounted return for a trajectory is defined as:</p>$$
U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + ...
$$<p>Consequently, the action-value function is defined as</p>$$
Q_{\pi}(s_t, a_t) = \mathbb{E_t}[U_t|S_t=s_t, A_t=a_t]
$$<p>State-value function (or value function) can be calculated as:</p>$$
V_{\pi}(s_t) = \mathbb{E_A}[Q_{\pi}(s_t, A)] = \sum_a \pi(a|s_t) \cdot Q_{\pi}(s_t, a)
$$<p>In policy gradient algorithm, the policy function $\pi(a|s_t)$ is approximated by policy network $\pi(a|s_t; \theta)$. $\theta$ here is the neural network model parameters. Then the policy-based learning is to maximize the objective function</p>$$
\begin{aligned}
J(\theta) &= \mathbb{E_S}[V(S; \theta)] \\\
&= \sum_{s\in S} d_{\pi}(s) V_{\pi}(s_t; \theta) \\\
&= \sum_{s\in S} d_{\pi}(s) \sum_a \pi(a|s_t; \theta) \cdot Q_{\pi}(s_t, a)
\end{aligned}
$$<p>where $d_{\pi}(s)$ is the stationary distribution of Markov chain for $\pi_{\theta}$, namely the state distribution under policy $\pi$.
Now we know the objective function of the policy-based algorithm, we can learn the parameters $\theta$ through policy gradiet ascent.</p><p>Now we can look at how to get the policy gradient. Since the first summation of the last step in the above equation has nothing to do with $\theta$, so we can focus on getting the derivatives of the value function $V_{\pi}(s; \theta)$. Using chain rule, it&rsquo;s easy to get:</p>$$
\begin{aligned}
\frac{\partial{V(s; \theta)}}{\partial{\theta}} &= \sum_a \frac{\partial{\pi (a|s; \theta)}}{\partial{\theta}} \cdot Q_{\pi}(s, a) \\\
&= \sum_a \pi(a|s_t; \theta) \frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} \cdot Q_{\pi}(s, a) \\\
&= \mathbb{E_A}\left[ \frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} \cdot Q_{\pi}(s, a) \right]
\end{aligned}
$$<p>The last step assumes that $\frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} \cdot Q_{\pi}(s, a)$ follows a distribution of $\pi(a|s_t; \theta)$ with respect to the random variable $A$.</p><p>Let&rsquo;s take another look at the policy gradient here. First, in practice, when we calculate the expectation we can use Monte Carlo Approximation. The gradient here becomes summations as below:</p>$$
\nabla_{\theta}(J(\theta)) = \sum_{t} \nabla_{\theta}{\log\pi (a|s; \theta)} \cdot Q_{\pi}(s, a)
$$<p>This is also called Monte Carlo policy gradient. Since gradient is a direction, this formula shows that policy gradient estimation is the direction of the steepest increase in reward/return. When reward is larger, the policy gradient will be larger.</p><h3 id=temporal-difference-td-learning>Temporal Difference (TD) Learning<a hidden class=anchor aria-hidden=true href=#temporal-difference-td-learning>#</a></h3><p>Temporal Difference (TD) learning is one of the core concepts in Reinforcement Learning. Temporal difference algorithm always aims to bring the expected prediction and the new prediction together, thus matching expectations with reality and gradually increasing the accuracy of the entire chain of prediction.</p><p>The most basic version is TD(0) method. Specifically, if our agent is in a current state $s_t$, takes the action $a_t$ and receives the reward $r_t$, then we update our estimate of $V$ following</p>$$
V(s_t) \xleftarrow[]{} V(s_t) + \alpha[r_{t+1} + \gamma V(s_{t+1}) ‚Äì V(s_t)]
$$<p>Here $r_{t+1} + \gamma V(s_{t+1})$ is TD target and $r_{t+1} + \gamma V(s_{t+1}) ‚Äì V(s_t)$ is called TD error ($\delta$).</p><p>There is SARSA (state-action-reward-state-action), where we replace the value function as the action-state value function.</p>$$
Q(s_t, a_t) \xleftarrow[]{} Q(s_t, a_t) + \alpha[r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) ‚Äì Q(s_t, a_t)]
$$<p>And TD with Q-learning</p>$$
Q(s_t, a_t) \xleftarrow[]{} Q(s_t, a_t) + \alpha[r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) ‚Äì Q(s_t, a_t)]
$$<h3 id=reinforce>REINFORCE<a hidden class=anchor aria-hidden=true href=#reinforce>#</a></h3><p>Since $Q_{\pi}(s, a)$ is the expectation of the return, we can once again use Monte Carlo approximation,</p>$$
\begin{aligned}
Q_{\pi}(s_t, a_t) &= u_t \\\
&= \sum_{i=t}^{N} {\gamma^{i-t} \cdot r_{i}}
\end{aligned}
$$<p>The above MCPG actually gives us a practical algorithm to do policy gradient based RL. Let&rsquo;s summarize it as follows:</p><ol><li>Play one episode of game to get the trajectory: $s_1, a_1, r_1, s_2, a_2, r_2, ...$</li><li>Estimate all $q_t \approx u_t$ using above equation</li><li>Differentiate policy network to get $d_{\theta, t}$</li><li>Compute policy gradient $g(a_t, \theta_t) = q_t \cdot d_{\theta, t}$</li></ol><h3 id=advantage-function-and-generalized-advantage-estimation>Advantage Function and Generalized Advantage Estimation<a hidden class=anchor aria-hidden=true href=#advantage-function-and-generalized-advantage-estimation>#</a></h3><p>The above equation is the vanilla policy gradient method. More policy gradient algorithms are proposed later to reduce high variance of the vanilla version. John Schulman&rsquo;s <a href=https://arxiv.org/pdf/1506.02438.pdf>GAE paper</a> summarized all the improvement methods. In the derivation, the policy gradient is represented as</p>$$
\frac{\partial{V(s; \theta)}}{\partial{\theta}} = \mathbb{E_A}\left[ \frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} \cdot \hat{A_t}(s, a) \right]
$$<p>where $\hat{A_t}(s, a)$ is the advantage function. In implementation, we construct loss function in a way such that the policy gradient $g$ equals to the above result</p>$$
L(\theta) = \mathbb{E_t}\left[ \log\pi (a|s; \theta) \hat{A_t}(s, a) \right]
$$<p>The idea is that the Advantage function calculates how better taking that action at a state is compared to the average value of the state. It‚Äôs subtracting the mean value of the state from the state action pair. Mathematically, $A(s_t, a_t) = Q(s_t, a_t) ‚àí V (s_t)$, where $Q(s_t, a_t)$ is the action-value function, representing the expected return after taking action at at state $s$, and $V (s_t)$ is the value function, representing the average expected return at state $s_t$.</p><p>Based on the above advantage definition, we have</p>$$
\begin{aligned}
\hat{A_t^{(1)}} &= r_t + \gamma V(s_{t+1}) - V(s) \\\
\hat{A_t^{(2)}} &= r_t + \gamma r_{t+1} +\gamma^2 V(s_{t+2}) - V(s) \\\
...\\\
\hat{A_t^{(\infty)}} &= r_t + \gamma r_{t+1} +\gamma^2 r_{t+2} + ... - V(s)
\end{aligned}
$$<p>Notice that $\hat{A_t^{(1)}}$ has high bias, low variance, whilst
$\hat{A_t^{(\infty)}}$ is unbiased, high variance. A weighted average of $\hat{A_t^{(k)}}$ can be used to balance bias and variance.</p>$$\hat{A_t} = \hat{A_t^{GAE}} = \frac{\sum_k w_k \hat{A_t^{(k)}}}{\sum_k w_k}$$<p>We set $w_k = \lambda^{k-1}$, this gives clean calculation for $\hat{A_t}$. Below we have the recursion equations. (Refer to [11] to learn how to derive the second equation here.)</p>$$
\begin{aligned}
\delta_t &= r_t + \gamma V(s_{t+1}) - V(s_t)
\\\
\hat{A_t} &= \delta_t + \gamma \lambda \delta_{t+1} + ... +
(\gamma \lambda)^{T - t + 1} \delta_{T - 1}
\\\
&= \delta_t + \gamma \lambda \hat{A_{t+1}}
\end{aligned}
$$<h3 id=actor-critic-algorithm>Actor-Critic Algorithm<a hidden class=anchor aria-hidden=true href=#actor-critic-algorithm>#</a></h3><p>There we give a recap of how actor-critic method works. In Actor-Critic algorithm, we use one neural network $\pi(a|s; \theta)$ to approximate policy function $\pi(a|s)$ and use another neural network $q(s, a; w)$ to approximate value function $Q_{\pi}(s, a)$.</p><ul><li>Observe state $s_t$, and randomly sample action from policy $a_t \sim \pi(\cdot | s_t; \Theta_t)$</li><li>Let agent perform action $a_t$, and get new state $s_{t+1}$ and reward $r_t$ from environment</li><li>Randomly sample $\tilde{a}_{t+1} \sim \pi(\cdot | s_t; \Theta_t)$ without performing the action</li><li>Evaluate value network: $q_t = q(s_t, a_t; W_t)$ and $q_{t+1} = q(s_{t+1}, \tilde{a}_{t+1}; W_t)$</li><li>Compute TD error: $\delta_t = q_t - (r_t + \gamma \cdot q_{t+1})$</li><li>Differentiate value network: $d_{w,t} = \frac{\partial{q(s_t, a_t, w)}}{\partial{w}}$ (autograd will do this for us)</li><li>Update value network: $ w_{t+1} = w_t - \alpha \cdot \delta_t \cdot d_{w, t}$</li><li>Differentiate policy network: $ d_{\theta, t} = \frac{\partial{\log\pi (a|s; \theta)}}{\partial{\theta}} $ (again autograd will do this for us)</li><li>Update policy network: $\theta_{t+1} = \theta_t + \beta \cdot q_t \cdot d_{\theta, t}$.<ul><li>We can also use: $\theta_{t+1} = \theta_t + \beta \cdot \delta_t \cdot d_{\theta, t}$ to update policy network. This is called policy gradient with baseline.
Essentially, the algorithm alternates between sampling and optimization. The expectation in the above equation indicates that we need to average over a finite batch of empirical samples.</li></ul></li></ul><h3 id=proximal-policy-optimization>Proximal Policy Optimization<a hidden class=anchor aria-hidden=true href=#proximal-policy-optimization>#</a></h3><p>Vanilla policy gradient method uses on-policy update. Concretely, the algorithm samples empirical data from a policy network $\pi_{\theta}$ parameterized with $\theta$. After updating the network itself, the new policy network is $\pi_{\theta_{new}}$ and the old policy $\pi_{\theta}$ is out of use and future sampling will be from $\pi_{\theta_{new}}$. This whole process is not efficient enough. The solution to this is to reuse the old samples to achieve off-policy training. From above importance sampling section, we know that:</p>$$
\mathbb{E_{x\sim p(x)}}\left[f \right] = \mathbb{E_{x\sim q(x)}} \left[ \frac{p(x_i)}{q(x_i)}f(x_i) \right]
$$<p>Similarly, we can make a change to the objective function of our policy gradient, and the resulting policy gradient will become</p>$$
\begin{aligned}
g &= \mathbb{E_{{(s_t, a_t)} \sim \pi_{\theta}}}\left[ \frac{\partial{\log\pi (a_t|s_t; \theta)}}{\partial{\theta}} \cdot \hat{A_t}(s, a) \right] \\\
&= \mathbb{E_{{(s_t, a_t)} \sim \pi_{\theta_{old}}}}\left[ \frac{\pi_{\theta}(a_t|s_t; \theta)}{\pi_{\theta_{old}}(a_t|s_t)} \frac{\partial{\log\pi (a_t|s_t; \theta)}}{\partial{\theta}} \cdot \hat{A_t}(s, a) \right]
\end{aligned}
$$<p>Consequently, the loss becomes</p>$$
L(\theta) = \mathbb{E_{{(s_t, a_t)} \sim \pi_{\theta_{old}}}}\left[ \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A_t}(s, a) \right]
$$<p>This is so-called surrogate objective function. In the above section, we mentioned how to use chain rule to get the expectation format of gradient, here we just to reverse the process to get the above loss function.</p><p>In the importance sampling section, we saw that the variance of new distribution could be large when the proposal distribution is not so close to the original distribution. Thus, to deal with this, people add KL diveragence to the loss function to limit the old and new policy difference. Using Largrangian dual method, we can add this constraint to the objective function:</p>$$
L(\theta) = \mathbb{E_{{(s_t, a_t)} \sim \pi_{\theta_{old}}}}\left[ \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A_t}(s, a) - \beta KL[\pi_{\theta_{old}}(a_t|s_t), \pi_{\theta}(a_t|s_t)]\right]
$$<h2 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h2><p>For language generation task, generating a token is an action. Agent is the target language model we want to train.</p><p>Here we first look at the implementation from Deepspeed-chat model. The actor-critic algorithm requires to load four model in training: actor model, critic model, reference model and reward mdoel. Actor model is the poliy network and critice model is the value network. Reference model and reward model are frozen in training. Reference model is used to contrain the actor model predictions so that they won&rsquo;t divege too much. Reward model gives the current step reward.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <a href=https://arxiv.org/pdf/1506.02438.pdf>High-Dimensional Continuous Control Using Generalized Advantage Estimation</a><br>[2] <a href=https://arxiv.org/pdf/1707.06347.pdf>Proximal Policy Optimization Algorithms</a><br>[3] <a href=https://papers.nips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf>Policy Gradient Methods for Reinforcement Learning with Function Approximation</a><br>[4] <a href=https://arxiv.org/abs/1511.06581>Dueling Network Architectures for Deep Reinforcement Learning</a><br>[5] <a href=https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/schedule.html>https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/schedule.html</a><br>[6] <a href=https://github.com/wangshusen/DRL>https://github.com/wangshusen/DRL</a><br>[7] <a href=https://www.davidsilver.uk/teaching/>https://www.davidsilver.uk/teaching/</a><br>[8] <a href=https://arxiv.org/pdf/1909.08593.pdf>Fine-Tuning Language Models from Human Preferences</a><br>[9] <a href=https://zhuanlan.zhihu.com/p/677607581>https://zhuanlan.zhihu.com/p/677607581</a><br>[10] <a href=https://arxiv.org/abs/2308.01320>DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</a><br>[11] <a href=https://arxiv.org/pdf/2307.04964.pdf>Secrets of RLHF in Large Language Models Part I: PPO</a><br>[12] <a href=https://arxiv.org/pdf/2401.06080.pdf>Secrets of RLHF in Large Language Models Part II: Reward Modeling</a><br>[13] <a href=https://arxiv.org/abs/2403.17031>The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization</a><br>[14] <a href=https://arxiv.org/pdf/2404.10719.pdf>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</a><br>[15] <a href=https://arxiv.org/abs/2005.12729>Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO</a><br>[16] <a href="https://difficult-link-dd7.notion.site/eb7b2d1891f44b3a84e7396d19d39e6f?v=01bcb084210149488d730064cbabc99f">Advanced Tricks for Training Large Language Models with Proximal Policy Optimization</a></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/rl/verl/><span class=title>¬´</span><br><span>veRL</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml/diffusion/><span class=title>¬ª</span><br><span>Diffusion Probabilistic Models</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share PPO and Its Implementation on twitter" href="https://twitter.com/intent/tweet/?text=PPO%20and%20Its%20Implementation&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2frl%2fppo%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share PPO and Its Implementation on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2frl%2fppo%2f&amp;title=PPO%20and%20Its%20Implementation&amp;summary=PPO%20and%20Its%20Implementation&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2frl%2fppo%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share PPO and Its Implementation on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2frl%2fppo%2f&title=PPO%20and%20Its%20Implementation"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share PPO and Its Implementation on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2frl%2fppo%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share PPO and Its Implementation on whatsapp" href="https://api.whatsapp.com/send?text=PPO%20and%20Its%20Implementation%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2frl%2fppo%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share PPO and Its Implementation on telegram" href="https://telegram.me/share/url?text=PPO%20and%20Its%20Implementation&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2frl%2fppo%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>