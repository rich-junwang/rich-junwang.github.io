<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Flash Attention | Jun's Blog</title><meta name=keywords content><meta name=description content="In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.
GPU Compute Model and Memory Hierarchy
The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.

     
    Figure 1. GPU memory
    

Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/flash_attn/><link crossorigin=anonymous href=../../../../../assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../../../assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/flash_attn/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="Flash Attention"><meta property="og:description" content="In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.
GPU Compute Model and Memory Hierarchy
The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.

     
    Figure 1. GPU memory
    

Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/flash_attn/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-18T00:18:23+08:00"><meta property="article:modified_time" content="2023-07-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Flash Attention"><meta name=twitter:description content="In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.
GPU Compute Model and Memory Hierarchy
The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.

     
    Figure 1. GPU memory
    

Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Flash Attention","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/flash_attn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Flash Attention","name":"Flash Attention","description":"In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.\nGPU Compute Model and Memory Hierarchy The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.\nFigure 1. GPU memory Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit.\n","keywords":[],"articleBody":"In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.\nGPU Compute Model and Memory Hierarchy The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.\nFigure 1. GPU memory Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit.\nFigure 2. GPU memory hierarchy IO-aware Computation First letâ€™s take a look at the vallina attention computation which is shown below\nFigure 3. Vallina attention computation Essentially, each of the operation follows the three steps of operation below.\nRead op â€” Move tensor from HBM to SRAM Compute op - Perform compute intensive task on SRAM write op - move tensor back from SRAM to HBM The breakdown of these computation is as follows. Apparently, all these green ops in the vallina attention can be saved.\nFigure 4. Vallina attention computation break down However, itâ€™s hard to put giant attention matrix of size [N x N] in the cache. The idea to solve this challenge is to use tiling. Concretely, we slice the matrices into smaller blocks and in each of Q K computation, we do it in a small block scale. The output of the small block thus can be saved on the cache. This sounds perfectly except that softmax op is not possible with small block computation. Lucklily there are already some studies dealing with this [1-2]. Before talking about this, letâ€™s first revisit stable softmax computation.\nBlockwise Softmax Underflow in numerical computation can cause precision issue. Overflow can be more problematic because it usually leads to divergence of training job (some may argue silent error is more detrimental :)). Softmax operation involves exponential computation which without careful handling can easily lead to overflow (such as exp(2000)).\n$$ \\text{softmax}(x)_i = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}} $$\nSimilary, the cross entropy can be computed as\n$$ \\begin{aligned} H(p, q) \u0026= -\\sum_i p_i\\log(q_i) \\\\ \u0026= -1\\cdot\\log(q_y) -\\sum_{i \\neq y} 0\\cdot\\log(q_i) \\\\ \u0026= -\\log(q_y) \\\\ \u0026= -\\log(\\text{softmax}(\\hat{y})_y) \\\\ \\end{aligned} $$\nWhen $max(x)$ is very large, the numerator could become $0$, and $log$ computation could overflow. To prevent this, we can do one more step: $$ \\begin{aligned} \\log(\\text{softmax}(x)_i) \u0026= \\log(\\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}) \\\\ \u0026= x_i - \\max(x) - \\log(\\sum_j e^{x_j - \\max(x)}) \\end{aligned} $$\nBy simply extracting the max value, we limit the exponential values to be in [0, 1]. In Flashattention paper, the softmax is represented as follows:\nFigure 5. Softmax Then blockwise softmax can be computed as follows:\nFigure 6. Blockwise Softmax With saving some summary (i.e. max, sum of exponentials) statistics, the softmax op can be decomposed into blocks.\nBlockwise Computation FlashAttention processes blocks of keys rather than the entire sequence. The challenge is how to compute the denominator $Z_i$ across blocks in a numerically stable way.\nThe solution uses the log-sum-exp trick and a running maximum.\nThe Softmax Denominator For a single query vector $q_i$, the unnormalized attention scores over all keys $k_j$ are:\n$$ s_{i,j} = q_i \\cdot k_j. $$\nThe softmax weights are:\n$$ \\alpha_{i,j} = \\frac{\\exp(s_{i,j})}{\\sum_{l=1}^n \\exp(s_{i,l})}. $$\nHere, the denominator is:\n$$ Z_i = \\sum_{l=1}^n \\exp(s_{i,l}), $$\nwhich, naively, requires computing all dot products at once.\nStep 1. Processing One Block Suppose we process a block of keys $\\mathcal{B}$, a block in sequence length dim (time dim). For query $q_i$, we compute scores:\n$$ s_{i,j} = q_i \\cdot k_j, \\quad j \\in \\mathcal{B}. $$\nLet the maximum score in this block be:\n$$ m_i^{(\\text{blk})} = \\max_{j \\in \\mathcal{B}} s_{i,j}. $$\nWe can then compute a partial exponential sum (shifted for stability):\n$$ Z_i^{(\\text{blk})} = \\sum_{j \\in \\mathcal{B}} \\exp \\left(s_{i,j} - m_i^{(\\text{blk})}\\right) $$\nStep 2. Merging Across Blocks Suppose after some blocks, we have:\nA running maximum $m_i^{(\\text{old})}$. A running denominator accumulator: $$ Z_i^{(\\text{old})} = \\sum_{j \\in \\text{processed blocks}} \\exp \\left(s_{i,j} - m_i^{(\\text{old})}\\right). $$\nNow we process a new block. We update the global maximum:\n$$ m_i^{(\\text{new})} = \\max \\left(m_i^{(\\text{old})}, m_i^{(\\text{blk})}\\right). $$\nWe then rescale the old accumulator to the new maximum:\n$$ Z_i^{(\\text{new})} = Z_i^{(\\text{old})} \\cdot \\exp \\left(m_i^{(\\text{old})} - m_i^{(\\text{new})}\\right) + \\sum_{j \\in \\mathcal{B}} \\exp \\left(s_{i,j} - m_i^{(\\text{new})}\\right) $$\nThis formula ensures that all contributions are correctly normalized with respect to the same maximum, keeping the computation stable.\nStep 3. Final Normalization After processing all blocks, we end up with:\nThe global maximum $m_i$. The final denominator: $$ Z_i = \\sum_{j=1}^n \\exp(s_{i,j} - m_i). $$\nThen the attention weights for each block can be computed as:\n$$ \\alpha_{i,j} = \\frac{\\exp(s_{i,j} - m_i)}{Z_i}. $$\nSummary FlashAttention avoids ever storing the full $n \\times n$ matrix by:\nComputing scores block by block. Keeping track of a running maximum $m_i$. Maintaining a rescaled running sum of exponentials. This streaming log-sum-exp reduction makes it possible to compute attention in O(nÂ²) time but only O(n) memory, which is a major win for long-sequence transformers.\nRecomputation in Backpropagation With the fused kernel, we effectively do the computation outside Pytorch computation graph. Thus, we canâ€™t use the AutoGrad for gradient computation in backpropagation. Consequently, we have to define the backpropagation by ourselves. The way to solve this is very simple as well. We just define our own backpropagation ops for fused kernel like gradient checkpointing.\nReferences [1] SELF-ATTENTION DOES NOT NEED O(n^2) MEMORY [2] Online normalizer calculation for softmax [3] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n","wordCount":"975","inLanguage":"en-us","datePublished":"2023-06-18T00:18:23+08:00","dateModified":"2023-07-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/flash_attn/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>Flash Attention</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2023-06-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>975 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>2 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#gpu-compute-model-and-memory-hierarchy aria-label="GPU Compute Model and Memory Hierarchy">GPU Compute Model and Memory Hierarchy</a></li><li><a href=#io-aware-computation aria-label="IO-aware Computation">IO-aware Computation</a></li><li><a href=#blockwise-softmax aria-label="Blockwise Softmax">Blockwise Softmax</a></li><li><a href=#blockwise-computation aria-label="Blockwise Computation">Blockwise Computation</a><ul><li><a href=#the-softmax-denominator aria-label="The Softmax Denominator">The Softmax Denominator</a></li><li><a href=#step-1-processing-one-block aria-label="Step 1. Processing One Block">Step 1. Processing One Block</a></li><li><a href=#step-2-merging-across-blocks aria-label="Step 2. Merging Across Blocks">Step 2. Merging Across Blocks</a></li><li><a href=#step-3-final-normalization aria-label="Step 3. Final Normalization">Step 3. Final Normalization</a></li><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#recomputation-in-backpropagation aria-label="Recomputation in Backpropagation">Recomputation in Backpropagation</a></li><li><a href=#references aria-label=References>References</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.</p><h2 id=gpu-compute-model-and-memory-hierarchy>GPU Compute Model and Memory Hierarchy<a hidden class=anchor aria-hidden=true href=#gpu-compute-model-and-memory-hierarchy>#</a></h2><p>The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.</p><p align=center><img alt="gpu memory" src=images/gpu_mem.png width=80% height=auto/>
<em>Figure 1. GPU memory</em><br></p><p>Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit.</p><p align=center><img alt="gpu memory hierarchy" src=images/gpu_mem_hierarchy.png width=80% height=auto/>
<em>Figure 2. GPU memory hierarchy</em><br></p><h2 id=io-aware-computation>IO-aware Computation<a hidden class=anchor aria-hidden=true href=#io-aware-computation>#</a></h2><p>First let&rsquo;s take a look at the vallina attention computation which is shown below</p><p align=center><img alt="Vallina attention algorithm" src=images/vallina_attn.png width=100% height=auto/>
<em>Figure 3. Vallina attention computation</em><br></p><p>Essentially, each of the operation follows the three steps of operation below.</p><ul><li>Read op â€” Move tensor from HBM to SRAM</li><li>Compute op - Perform compute intensive task on SRAM</li><li>write op - move tensor back from SRAM to HBM</li></ul><p>The breakdown of these computation is as follows. Apparently, all these green ops in the vallina attention can be saved.</p><p align=center><img alt="Vallina attention algorithm" src=images/vallina_attn_break_down.png width=80% height=auto/>
<em>Figure 4. Vallina attention computation break down</em></p><p>However, it&rsquo;s hard to put giant attention matrix of size <code>[N x N]</code> in the cache. The idea to solve this challenge is to use tiling. Concretely, we slice the matrices into smaller blocks and in each of <strong>Q</strong> <strong>K</strong> computation, we do it in a small block scale. The output of the small block thus can be saved on the cache. This sounds perfectly except that softmax op is not possible with small block computation. Lucklily there are already some studies dealing with this [1-2]. Before talking about this, let&rsquo;s first revisit stable softmax computation.</p><h2 id=blockwise-softmax>Blockwise Softmax<a hidden class=anchor aria-hidden=true href=#blockwise-softmax>#</a></h2><p>Underflow in numerical computation can cause precision issue. Overflow can be more problematic because it usually leads to divergence of training job (some may argue silent error is more detrimental :)). Softmax operation involves exponential computation which without careful handling can easily lead to overflow (such as <code>exp(2000)</code>).</p><p>$$ \text{softmax}(x)_i = \frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}} $$</p><p>Similary, the cross entropy can be computed as</p><p>$$
\begin{aligned}
H(p, q) &= -\sum_i p_i\log(q_i) \\
&= -1\cdot\log(q_y) -\sum_{i \neq y} 0\cdot\log(q_i) \\
&= -\log(q_y) \\
&= -\log(\text{softmax}(\hat{y})_y) \\
\end{aligned} $$</p><p>When $max(x)$ is very large, the numerator could become $0$, and $log$ computation could overflow. To prevent this, we can do one more step:
$$
\begin{aligned}
\log(\text{softmax}(x)_i) &= \log(\frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}}) \\
&= x_i - \max(x) - \log(\sum_j e^{x_j - \max(x)})
\end{aligned} $$</p><p>By simply extracting the max value, we limit the exponential values to be in [0, 1]. In Flashattention paper, the softmax is represented as follows:</p><p align=center><img alt=softmax src=images/softmax.png width=100% height=auto/>
<em>Figure 5. Softmax</em></p><p>Then blockwise softmax can be computed as follows:</p><p align=center><img alt="blockwise softmax" src=images/blockwise_softmax.png width=100% height=auto/>
<em>Figure 6. Blockwise Softmax</em></p><p>With saving some summary (i.e. max, sum of exponentials) statistics, the softmax op can be decomposed into blocks.</p><h2 id=blockwise-computation>Blockwise Computation<a hidden class=anchor aria-hidden=true href=#blockwise-computation>#</a></h2><p>FlashAttention processes blocks of keys rather than the entire sequence. The challenge is how to compute the denominator $Z_i$ across blocks in a numerically stable way.</p><p>The solution uses the log-sum-exp trick and a running maximum.</p><h3 id=the-softmax-denominator>The Softmax Denominator<a hidden class=anchor aria-hidden=true href=#the-softmax-denominator>#</a></h3><p>For a single query vector $q_i$, the unnormalized attention scores over all keys $k_j$ are:</p><p>$$
s_{i,j} = q_i \cdot k_j.
$$</p><p>The softmax weights are:</p><p>$$
\alpha_{i,j} = \frac{\exp(s_{i,j})}{\sum_{l=1}^n \exp(s_{i,l})}.
$$</p><p>Here, the denominator is:</p><p>$$
Z_i = \sum_{l=1}^n \exp(s_{i,l}),
$$</p><p>which, naively, requires computing all dot products at once.</p><h3 id=step-1-processing-one-block>Step 1. Processing One Block<a hidden class=anchor aria-hidden=true href=#step-1-processing-one-block>#</a></h3><p>Suppose we process a block of keys $\mathcal{B}$, a block in sequence length dim (time dim). For query $q_i$, we compute scores:</p><p>$$
s_{i,j} = q_i \cdot k_j, \quad j \in \mathcal{B}.
$$</p><p>Let the maximum score in this block be:</p><p>$$
m_i^{(\text{blk})} = \max_{j \in \mathcal{B}} s_{i,j}.
$$</p><p>We can then compute a partial exponential sum (shifted for stability):</p><p>$$
Z_i^{(\text{blk})} = \sum_{j \in \mathcal{B}} \exp \left(s_{i,j} - m_i^{(\text{blk})}\right)
$$</p><h3 id=step-2-merging-across-blocks>Step 2. Merging Across Blocks<a hidden class=anchor aria-hidden=true href=#step-2-merging-across-blocks>#</a></h3><p>Suppose after some blocks, we have:</p><ul><li>A running maximum $m_i^{(\text{old})}$.</li><li>A running denominator accumulator:</li></ul><p>$$
Z_i^{(\text{old})} = \sum_{j \in \text{processed blocks}} \exp \left(s_{i,j} - m_i^{(\text{old})}\right).
$$</p><p>Now we process a new block. We update the global maximum:</p><p>$$
m_i^{(\text{new})} = \max \left(m_i^{(\text{old})}, m_i^{(\text{blk})}\right).
$$</p><p>We then rescale the old accumulator to the new maximum:</p><p>$$
Z_i^{(\text{new})} = Z_i^{(\text{old})} \cdot \exp \left(m_i^{(\text{old})} - m_i^{(\text{new})}\right) + \sum_{j \in \mathcal{B}} \exp \left(s_{i,j} - m_i^{(\text{new})}\right)
$$</p><p>This formula ensures that all contributions are correctly normalized with respect to the same maximum, keeping the computation stable.</p><h3 id=step-3-final-normalization>Step 3. Final Normalization<a hidden class=anchor aria-hidden=true href=#step-3-final-normalization>#</a></h3><p>After processing all blocks, we end up with:</p><ul><li>The global maximum $m_i$.</li><li>The final denominator:</li></ul><p>$$
Z_i = \sum_{j=1}^n \exp(s_{i,j} - m_i).
$$</p><p>Then the attention weights for each block can be computed as:</p><p>$$
\alpha_{i,j} = \frac{\exp(s_{i,j} - m_i)}{Z_i}.
$$</p><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><p>FlashAttention avoids ever storing the full $n \times n$ matrix by:</p><ol><li>Computing scores block by block.</li><li>Keeping track of a running maximum $m_i$.</li><li>Maintaining a rescaled running sum of exponentials.</li></ol><p>This streaming log-sum-exp reduction makes it possible to compute attention in O(nÂ²) time but only O(n) memory, which is a major win for long-sequence transformers.</p><h3 id=recomputation-in-backpropagation>Recomputation in Backpropagation<a hidden class=anchor aria-hidden=true href=#recomputation-in-backpropagation>#</a></h3><p>With the fused kernel, we effectively do the computation outside Pytorch computation graph. Thus, we can&rsquo;t use the AutoGrad for gradient computation in backpropagation. Consequently, we have to define the backpropagation by ourselves. The way to solve this is very simple as well. We just define our own backpropagation ops for fused kernel like gradient checkpointing.</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] <a href=https://browse.arxiv.org/pdf/2112.05682.pdf>SELF-ATTENTION DOES NOT NEED O(n^2) MEMORY</a><br>[2] <a href=https://browse.arxiv.org/pdf/1805.02867.pdf>Online normalizer calculation for softmax</a><br>[3] <a href=https://arxiv.org/abs/2205.14135>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/debug/><span class=title>Â«</span><br><span>Efficient Debugging</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/data_processing/><span class=title>Â»</span><br><span>Data Processing in Distributed Training</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Flash Attention on twitter" href="https://twitter.com/intent/tweet/?text=Flash%20Attention&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fflash_attn%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Flash Attention on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fflash_attn%2f&amp;title=Flash%20Attention&amp;summary=Flash%20Attention&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fflash_attn%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Flash Attention on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fflash_attn%2f&title=Flash%20Attention"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Flash Attention on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fflash_attn%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Flash Attention on whatsapp" href="https://api.whatsapp.com/send?text=Flash%20Attention%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fflash_attn%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Flash Attention on telegram" href="https://telegram.me/share/url?text=Flash%20Attention&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fflash_attn%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>