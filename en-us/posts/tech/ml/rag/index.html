<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Retrieval Augmented Generation | Jun's Blog</title><meta name=keywords content><meta name=description content="RAG system"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/rag/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/rag/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="Retrieval Augmented Generation"><meta property="og:description" content="RAG system"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/rag/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-18T00:18:23+08:00"><meta property="article:modified_time" content="2023-10-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Retrieval Augmented Generation"><meta name=twitter:description content="RAG system"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Retrieval Augmented Generation","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/rag/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Retrieval Augmented Generation","name":"Retrieval Augmented Generation","description":"RAG system","keywords":[""],"articleBody":"LLMs has made remarkable progress these days, however, they still exhibit notable limitations. Among these, hallucination is one of the most seen issues. In other words, the generations from LLMs are not grounded. To this end, people are turning to retrieval augmented generation to tackle the issue. In this blog, let‚Äôs roll up our sleeves and dive deep into the retrieval augmented system.\nRAG system contains: chunking, indexing, querying and generation. Chunking and indexing both are the offline processes which is the crucial data modeling phase. Querying and generation are online processes.\nRAG system for QA, image from [1] Indexing Retrieval Retrieval is the online process where the system converts user query into vector representation and retrieve relevant documents.\nRetrieval Evaluation Metric Like recommender system, retrieval system commonly use the following evaluation metrics.\nHit ratio (hit@k), Normalized Discounted Cumulative Gain (NDCG), Precision@k, Recall@k Mean Reciprocal Rank (MRR) Mean Average Precision (MAP) Hit@k Hit@k sometimes is also called Top-k accuracy. It is the percentage of search queries for each of which at least one item from the ground truth is returned within the top-k results. Simply put, it means % of queries get answer hit at top k retrieved passages. (Answer hit means user clicked on the doc). This number is meaningful when there are multiple test cases.\nNDCG Normalized Discounted Cumulative Gain (NDCG) is popular method for measuring the quality of a set of search results. It asserts the following:\nVery relevant results are more useful than somewhat relevant results which are more useful than irrelevant results (cumulative gain) Relevant results are more useful when they appear earlier in the set of results (discounting). The result of the ranking should be irrelevant to the query performed (normalization). Cumulative Gain (CG)¬†is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. Suppose you were presented with a set of search results for a query and asked to rank each result. Given a true relevance score $R_i$ (real value) for every item, there exist several ways to define a gain. One of the most popular is: $$ G_i = 2^{R_i} - 1 $$\nwhere $i$ is the rank position/index of the item. The relevance score can be defined by ourselves, for instance, we can say: 0 =\u003e Not relevant 1 =\u003e Near relevant 2 =\u003e Relevant. In practical use case, it‚Äôs defined as a binary value 0 and 1 where 0 is irrelevant and 1 is there is relevance.\nThe cumulative gain is then defined as: $$ CG@k = \\sum_{i=1}^k G_i $$\nTo achieve the¬†Discounted cumulative gain (DCG)¬†we must discount results that appear lower. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. Thus $DCG$ is: $$ DCG@k = \\sum_{i=1}^k\\frac{2^{R_i} - 1}{log_2{(i + 1)}} $$\nAs DCG either goes up with $k$ or it stays the same, the queries that return larger result sets will always have higher DCG scores than queries that return small result sets. To make comparison across queries fairer is we want to normalize the DCG score by the maximum possible DCG for different $k$. $$ NDCG@k = \\frac{DCG@k}{IDCG@k} $$ where $IDCG@k$ is the best $DCG$ we can get at position $k$.\nPrecision@k and Recall@k Precision@k measures the percentage of relevant results among top k results. At the same time, recall@k evaluates the ratio of relevant results among top k to the total number of relevant items in the whole dataset. $$ Precision@k = \\frac{\\text{number of recommended relevant items among top k}}{\\text{number of recommended items k}} $$ For example, suppose we have a corpus containing 20 documents, and we want to calculate the Precision@4 score for a given query. If the top 4 documents in the ranking contain 2 relevant documents, the Precision@4 score would be calculated as follows:\nPrecision@k = (number of relevant items in top k) / k Precision@4 = (number of relevant items in top 4) / 4 = 2 / 4 = 0.5 $$ Recall@k = \\frac{\\text{number of recommended relevant items among top k}}{\\text{number of all relevant items in the system}} $$\nCapped Recall@k The R_cap@k metric is a variant of the Recall@k metric. It measures the proportion of relevant items in the top k items of the ranking, relative to the total number of relevant items in the corpus, but caps the total number of relevant documents to k.\nThe formula to calculate Recall_cap@k is as follows: $$ Recall/cap@k = \\frac{\\text{number of recommended relevant items among top k}}{min(k, \\text{number of all relevant items in the system})} $$\nThe intuition is that if there are 100 relevant documents and we retrieved 10 docs, all of them are relevant. Then recall@k is 0.1, which seems quite low, but the system is probably doing well.\nMRR MRR measures ‚ÄúWhere is the first relevant item?‚Äù. Given a query and a list of returned items, we find the rank (position $p_i$) of the first relevant items. We take the inverse of the rank to get the so-called reciprocal rank. For mean reciprocal rank, we just take average of reciprocal rank for all queries.\nMRR MRR Pros\nThis method is simple to compute and is easy to interpret. This method puts a high focus on the first relevant element of the list. It is best suited for targeted searches such as users asking for the ‚Äúbest item for me‚Äù. Good for known-item search such as navigational queries or looking for a fact. MRR Cons\nThe MRR metric does not evaluate the rest of the list of recommended items. It focuses on a single item from the list. It gives a list with a single relevant item just a much weight as a list with many relevant items. It is fine if that is the target of the evaluation. This might not be a good evaluation metric for users that want a list of related items to browse. The goal of the users might be to compare multiple related items. MAP The P@N decision support metric calculates the fraction of n recommendations that are good. The drawback of this metric is that it does not consider the recommended list as an ordered list. Precision@k considers the whole list as a set of items, and treats all the errors in the recommended list equally. The goal is to cut the error in the first few elements rather than much later in the list. For this, we need a metric that weights the errors accordingly. The goal is to weight heavily the errors at the top of the list. Then gradually decrease the significance of the errors as we go down the lower items in a list. The Average Prediction (AP) metric tries to approximate this weighting sliding scale.\nAverage Precision (AP) is a metric about how a single sorted prediction compares with the ground truth. i.e., AP tells how correct a single ranking of documents is, with respect to a single query. Thus, MAP is meaningful when there are multiple test cases.\nAP is calculated as the average of precision@k over the list.\nMAP Codes How to calculate these metrics. We can use TREC_EVAL tool mentioned below. It has a Python interface. We can also manually calculate it.\n# compute recall_cap@k def recall_cap(qrels: Dict[str, Dict[str, int]], results: Dict[str, Dict[str, float]], k_values: List[int]) -\u003e Tuple[Dict[str, float]]: capped_recall = {} for k in k_values: capped_recall[f\"R_cap@{k}\"] = 0.0 k_max = max(k_values) logging.info(\"\\n\") for query_id, doc_scores in results.items(): top_hits = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)[0:k_max] query_relevant_docs = [doc_id for doc_id in qrels[query_id] if qrels[query_id][doc_id] \u003e 0] for k in k_values: retrieved_docs = [row[0] for row in top_hits[0:k] if qrels[query_id].get(row[0], 0) \u003e 0] denominator = min(len(query_relevant_docs), k) capped_recall[f\"R_cap@{k}\"] += (len(retrieved_docs) / denominator) for k in k_values: capped_recall[f\"R_cap@{k}\"] = round(capped_recall[f\"R_cap@{k}\"] / len(qrels), 5) logging.info(\"R_cap@{}: {:.4f}\".format(k, capped_recall[f\"R_cap@{k}\"])) return capped_recall TREC_EVAL TREC_EVAL is an evaluation tool which is used to evaluate an Information Retrieval system. TREC_EVAL requires two files: one is qrels (query relevance) and the other is retrieval output that needs to be evaluated.\ntrec_eval [-q] [-a] trec_rel_file trec_top_file Where trec_eval is the executable name for the code, -q is a parameter specifying detail for all queries, -a is a parameter specifying summary output only (-a and ‚Äìq are mutually exclusive), trec_rel_file is the qrels, trec_top_file is the results file.\nThe qrels file\nThis file contains a list of documents considered relevants for each query. This relevance judgement is made by human beings who manually select documents that should be retrieved when a particular query is executed. This file can be considered as the ‚Äúcorrect answer‚Äù and the documents retrieved by your IR system should approximate the maximum to it. It has the following format:\nquery-id 0 document-id relevance\nThe field query-id is a alphanumeric sequence to identify the query, document-id is a alphanumeric sequence to identify the judged document and the field relevance is a number to indicate the relevance degree between the document and query (0 for non relevant and 1 for relevant). The second field ‚Äú0‚Äù is not currently used, just put it in the file. The fields can be separated by a blank space or tabulation.\nThe results file\nThe results file contains a ranking of documents for each query automaticaly generated by your application. This is the file that will evaluated by trec_eval based in the ‚Äúcorrect answer‚Äù provided by the first file. This file has the following format:\nquery-id Q0 document-id rank score STANDARD\nThe field query-id is a alphanumeric sequence to identify the query. The second field, with ‚ÄúQ0‚Äù value, is currently ignored by trec_eval, just put it in the file. The field document-id is a alphanumeric sequence to identify the retrieved document. The field rank is an integer value which represents the document position in the ranking, but this field is also ignored by trec_eval. The field score can be an integer or float value to indicate the similarity degree between document and query, so the most relevants docs will have higher scores. The last field, with ‚ÄúSTANDARD‚Äù value, is used only to identify this run (this name is also showed in the output), you can use any alphanumeric sequence.\nRetrieval Fusion The straightforward idea in the era is dense retrieval is that we could combine sparse retrieval and dense retrieval together. A common approach is to use reciprocal rank fusion. The RRF score of document $d$ is: $$ RRF_d = \\sum_r^R{\\frac{1}{c + r(d)}} $$ assuming there are $R$ ranking items and $r(d)$ is the rank of document $d$. Here $c$ is a constant.\nRRF, image from [4] Generation References Retrieval-Augmented Generation for Large Language Models: A Survey Evaluation Metrics for Ranking problems: Introduction and Examples Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods Advanced RAG Techniques: an Illustrated Overview https://fabianhertwig.com/blog/information-retrieval-metrics/#ndcgk---normalized-discounted-cumulative-gain-at-k https://github.com/cvangysel/pytrec_eval ","wordCount":"1858","inLanguage":"en-us","datePublished":"2023-10-18T00:18:23+08:00","dateModified":"2023-10-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/rag/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>Retrieval Augmented Generation</h1><div class=post-description>RAG system</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2023-10-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1858 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>4 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/ml/ style=color:var(--secondary)!important>ML</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#indexing aria-label=Indexing>Indexing</a></li><li><a href=#retrieval aria-label=Retrieval>Retrieval</a><ul><li><a href=#retrieval-evaluation-metric aria-label="Retrieval Evaluation Metric">Retrieval Evaluation Metric</a><ul><li><a href=#hitk aria-label=Hit@k>Hit@k</a></li><li><a href=#ndcg aria-label=NDCG>NDCG</a></li><li><a href=#precisionk-and-recallk aria-label="Precision@k and Recall@k">Precision@k and Recall@k</a></li><li><a href=#capped-recallk aria-label="Capped Recall@k">Capped Recall@k</a></li><li><a href=#mrr aria-label=MRR>MRR</a></li><li><a href=#map aria-label=MAP>MAP</a></li></ul></li><li><a href=#codes aria-label=Codes>Codes</a></li><li><a href=#trec_eval aria-label=TREC_EVAL>TREC_EVAL</a></li><li><a href=#retrieval-fusion aria-label="Retrieval Fusion">Retrieval Fusion</a></li><li><a href=#generation aria-label=Generation>Generation</a></li><li><a href=#references aria-label=References>References</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>LLMs has made remarkable progress these days, however, they still exhibit notable limitations. Among these, hallucination is one of the most seen issues. In other words, the generations from LLMs are not grounded. To this end, people are turning to retrieval augmented generation to tackle the issue. In this blog, let‚Äôs roll up our sleeves and dive deep into the retrieval augmented system.</p><p>RAG system contains: chunking, indexing, querying and generation. Chunking and indexing both are the offline processes which is the crucial data modeling phase. Querying and generation are online processes.</p><p align=center><img alt=mrr src=images/rag.png width=80%>
RAG system for QA, image from [1]<br></p><h2 id=indexing>Indexing<a hidden class=anchor aria-hidden=true href=#indexing>#</a></h2><h2 id=retrieval>Retrieval<a hidden class=anchor aria-hidden=true href=#retrieval>#</a></h2><p>Retrieval is the online process where the system converts user query into vector representation and retrieve relevant documents.</p><h3 id=retrieval-evaluation-metric>Retrieval Evaluation Metric<a hidden class=anchor aria-hidden=true href=#retrieval-evaluation-metric>#</a></h3><p>Like recommender system, retrieval system commonly use the following evaluation metrics.</p><ul><li>Hit ratio (hit@k),</li><li>Normalized Discounted Cumulative Gain (NDCG),</li><li>Precision@k,</li><li>Recall@k</li><li>Mean Reciprocal Rank (MRR)</li><li>Mean Average Precision (MAP)</li></ul><h4 id=hitk>Hit@k<a hidden class=anchor aria-hidden=true href=#hitk>#</a></h4><p>Hit@k sometimes is also called Top-k accuracy. It is the percentage of search queries for each of which at least one item from the ground truth is returned within the top-k results. Simply put, it means % of queries get answer hit at top k retrieved passages. (Answer hit means user clicked on the doc). This number is meaningful when there are multiple test cases.</p><h4 id=ndcg>NDCG<a hidden class=anchor aria-hidden=true href=#ndcg>#</a></h4><p>Normalized Discounted Cumulative Gain (NDCG) is popular method for measuring the quality of a set of search results. It asserts the following:</p><ul><li>Very relevant results are more useful than somewhat relevant results which are more useful than irrelevant results (cumulative gain)</li><li>Relevant results are more useful when they appear earlier in the set of results (discounting).</li><li>The result of the ranking should be irrelevant to the query performed (normalization).
Cumulative Gain (CG)¬†is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. Suppose you were presented with a set of search results for a query and asked to rank each result.</li></ul><p>Given a true relevance score $R_i$ (real value) for every item, there exist several ways to define a gain. One of the most popular is:
$$
G_i = 2^{R_i} - 1
$$</p><p>where $i$ is the rank position/index of the item. The relevance score can be defined by ourselves, for instance, we can say: 0 => Not relevant 1 => Near relevant 2 => Relevant. In practical use case, it&rsquo;s defined as a binary value 0 and 1 where 0 is irrelevant and 1 is there is relevance.</p><p>The cumulative gain is then defined as:
$$
CG@k = \sum_{i=1}^k G_i
$$</p><p>To achieve the¬†Discounted cumulative gain (DCG)¬†we must discount results that appear lower. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. Thus $DCG$ is:
$$
DCG@k = \sum_{i=1}^k\frac{2^{R_i} - 1}{log_2{(i + 1)}}
$$</p><p>As DCG either goes up with $k$ or it stays the same, the queries that return larger result sets will always have higher DCG scores than queries that return small result sets. To make comparison across queries fairer is we want to normalize the DCG score by the maximum possible DCG for different $k$.
$$
NDCG@k = \frac{DCG@k}{IDCG@k}
$$
where $IDCG@k$ is the best $DCG$ we can get at position $k$.</p><h4 id=precisionk-and-recallk>Precision@k and Recall@k<a hidden class=anchor aria-hidden=true href=#precisionk-and-recallk>#</a></h4><p>Precision@k measures the percentage of relevant results among top k results. At the same time, recall@k evaluates the ratio of relevant results among top k to the total number of relevant items in the whole dataset.
$$
Precision@k = \frac{\text{number of recommended relevant items among top k}}{\text{number of recommended items k}}
$$
For example, suppose we have a corpus containing 20 documents, and we want to calculate the Precision@4 score for a given query. If the top 4 documents in the ranking contain 2 relevant documents, the Precision@4 score would be calculated as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Precision@k = (number of relevant items in top k) / k
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Precision@4 = (number of relevant items in top 4) / 4
</span></span><span style=display:flex><span>= 2 / 4
</span></span><span style=display:flex><span>= 0.5
</span></span></code></pre></div><p>$$
Recall@k = \frac{\text{number of recommended relevant items among top k}}{\text{number of all relevant items in the system}}
$$</p><h4 id=capped-recallk>Capped Recall@k<a hidden class=anchor aria-hidden=true href=#capped-recallk>#</a></h4><p>The R_cap@k metric is a variant of the Recall@k metric. It measures the proportion of relevant items in the top k items of the ranking, relative to the total number of relevant items in the corpus, but caps the total number of relevant documents to k.</p><p>The formula to calculate Recall_cap@k is as follows:
$$
Recall/cap@k = \frac{\text{number of recommended relevant items among top k}}{min(k, \text{number of all relevant items in the system})}
$$</p><p>The intuition is that if there are 100 relevant documents and we retrieved 10 docs, all of them are relevant. Then recall@k is 0.1, which seems quite low, but the system is probably doing well.</p><h4 id=mrr>MRR<a hidden class=anchor aria-hidden=true href=#mrr>#</a></h4><p>MRR measures ‚ÄúWhere is the first relevant item?‚Äù. Given a query and a list of returned items, we find the rank (position $p_i$) of the first relevant items. We take the inverse of the rank to get the so-called reciprocal rank. For mean reciprocal rank, we just take average of reciprocal rank for all queries.</p><p align=center><img alt=mrr src=images/mrr.png width=60%>
MRR<br></p><p>MRR Pros</p><ul><li>This method is simple to compute and is easy to interpret.</li><li>This method puts a high focus on the first relevant element of the list. It is best suited for targeted searches such as users asking for the ‚Äúbest item for me‚Äù.</li><li>Good for known-item search such as navigational queries or looking for a fact.</li></ul><p>MRR Cons</p><ul><li>The MRR metric does not evaluate the rest of the list of recommended items. It focuses on a single item from the list.</li><li>It gives a list with a single relevant item just a much weight as a list with many relevant items. It is fine if that is the target of the evaluation.</li><li>This might not be a good evaluation metric for users that want a list of related items to browse. The goal of the users might be to compare multiple related items.</li></ul><h4 id=map>MAP<a hidden class=anchor aria-hidden=true href=#map>#</a></h4><p>The P@N decision support metric calculates the fraction of n recommendations that are good. The drawback of this metric is that it does not consider the recommended list as an ordered list. Precision@k considers the whole list as a set of items, and treats all the errors in the recommended list equally. The goal is to cut the error in the first few elements rather than much later in the list. For this, we need a metric that weights the errors accordingly. The goal is to weight heavily the errors at the top of the list. Then gradually decrease the significance of the errors as we go down the lower items in a list. The Average Prediction (AP) metric tries to approximate this weighting sliding scale.</p><p>Average Precision (AP) is a metric about how a single sorted prediction compares with the ground truth. i.e., AP tells how correct a single ranking of documents is, with respect to a single query. Thus, MAP is meaningful when there are multiple test cases.</p><p>AP is calculated as the average of precision@k over the list.</p><p align=center><img alt=map src=images/map.png width=60%>
MAP<br></p><h3 id=codes>Codes<a hidden class=anchor aria-hidden=true href=#codes>#</a></h3><p>How to calculate these metrics. We can use <code>TREC_EVAL</code> tool mentioned below. It has a <a href=https://github.com/cvangysel/pytrec_eval>Python interface</a>. We can also manually calculate it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># compute recall_cap@k</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>recall_cap</span>(qrels: Dict[str, Dict[str, int]],
</span></span><span style=display:flex><span>           results: Dict[str, Dict[str, float]],
</span></span><span style=display:flex><span>           k_values: List[int]) <span style=color:#f92672>-&gt;</span> Tuple[Dict[str, float]]:
</span></span><span style=display:flex><span>    capped_recall <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> k_values:
</span></span><span style=display:flex><span>        capped_recall[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;R_cap@</span><span style=color:#e6db74>{</span>k<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    k_max <span style=color:#f92672>=</span> max(k_values)
</span></span><span style=display:flex><span>    logging<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> query_id, doc_scores <span style=color:#f92672>in</span> results<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        top_hits <span style=color:#f92672>=</span> sorted(doc_scores<span style=color:#f92672>.</span>items(), key<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> item: item[<span style=color:#ae81ff>1</span>], reverse<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[<span style=color:#ae81ff>0</span>:k_max]
</span></span><span style=display:flex><span>        query_relevant_docs <span style=color:#f92672>=</span> [doc_id <span style=color:#66d9ef>for</span> doc_id <span style=color:#f92672>in</span> qrels[query_id] <span style=color:#66d9ef>if</span> qrels[query_id][doc_id] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> k_values:
</span></span><span style=display:flex><span>            retrieved_docs <span style=color:#f92672>=</span> [row[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>for</span> row <span style=color:#f92672>in</span> top_hits[<span style=color:#ae81ff>0</span>:k] <span style=color:#66d9ef>if</span> qrels[query_id]<span style=color:#f92672>.</span>get(row[<span style=color:#ae81ff>0</span>], <span style=color:#ae81ff>0</span>) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            denominator <span style=color:#f92672>=</span> min(len(query_relevant_docs), k)
</span></span><span style=display:flex><span>            capped_recall[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;R_cap@</span><span style=color:#e6db74>{</span>k<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>] <span style=color:#f92672>+=</span> (len(retrieved_docs) <span style=color:#f92672>/</span> denominator)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> k_values:
</span></span><span style=display:flex><span>        capped_recall[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;R_cap@</span><span style=color:#e6db74>{</span>k<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>] <span style=color:#f92672>=</span> round(capped_recall[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;R_cap@</span><span style=color:#e6db74>{</span>k<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>] <span style=color:#f92672>/</span> len(qrels), <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>        logging<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;R_cap@</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(k, capped_recall[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;R_cap@</span><span style=color:#e6db74>{</span>k<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> capped_recall
</span></span></code></pre></div><h3 id=trec_eval>TREC_EVAL<a hidden class=anchor aria-hidden=true href=#trec_eval>#</a></h3><p><a href=https://github.com/usnistgov/trec_eval>TREC_EVAL</a> is an evaluation tool which is used to evaluate an Information Retrieval system. TREC_EVAL requires two files: one is qrels (query relevance) and the other is retrieval output that needs to be evaluated.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>trec_eval <span style=color:#f92672>[</span>-q<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>-a<span style=color:#f92672>]</span> trec_rel_file trec_top_file
</span></span></code></pre></div><p>Where trec_eval is the executable name for the code, -q is a parameter specifying detail for all queries, -a is a parameter specifying summary output only (-a and ‚Äìq are mutually exclusive), trec_rel_file is the qrels, trec_top_file is the results file.</p><p><strong>The qrels file</strong></p><p>This file contains a list of documents considered relevants for each query. This relevance judgement is made by human beings who manually select documents that should be retrieved when a particular query is executed. This file can be considered as the &ldquo;correct answer&rdquo; and the documents retrieved by your IR system should approximate the maximum to it. It has the following format:</p><p>query-id 0 document-id relevance</p><p>The field query-id is a alphanumeric sequence to identify the query, document-id is a alphanumeric sequence to identify the judged document and the field relevance is a number to indicate the relevance degree between the document and query (0 for non relevant and 1 for relevant). The second field &ldquo;0&rdquo; is not currently used, just put it in the file. The fields can be separated by a blank space or tabulation.</p><p><strong>The results file</strong></p><p>The results file contains a ranking of documents for each query automaticaly generated by your application. This is the file that will evaluated by trec_eval based in the &ldquo;correct answer&rdquo; provided by the first file. This file has the following format:</p><p>query-id Q0 document-id rank score STANDARD</p><p>The field query-id is a alphanumeric sequence to identify the query. The second field, with &ldquo;Q0&rdquo; value, is currently ignored by trec_eval, just put it in the file. The field document-id is a alphanumeric sequence to identify the retrieved document. The field rank is an integer value which represents the document position in the ranking, but this field is also ignored by trec_eval. The field score can be an integer or float value to indicate the similarity degree between document and query, so the most relevants docs will have higher scores. The last field, with &ldquo;STANDARD&rdquo; value, is used only to identify this run (this name is also showed in the output), you can use any alphanumeric sequence.</p><h3 id=retrieval-fusion>Retrieval Fusion<a hidden class=anchor aria-hidden=true href=#retrieval-fusion>#</a></h3><p>The straightforward idea in the era is dense retrieval is that we could combine sparse retrieval and dense retrieval together. A common approach is to use reciprocal rank fusion. The RRF score of document $d$ is:
$$
RRF_d = \sum_r^R{\frac{1}{c + r(d)}}
$$
assuming there are $R$ ranking items and $r(d)$ is the rank of document $d$. Here $c$ is a constant.</p><p align=center><img alt=rrf src=images/rrf.png width=80%>
RRF, image from [4]<br></p><h3 id=generation>Generation<a hidden class=anchor aria-hidden=true href=#generation>#</a></h3><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ol><li><a href=https://arxiv.org/abs/2312.10997>Retrieval-Augmented Generation for Large Language Models: A Survey</a></li><li><a href=https://queirozf.com/entries/evaluation-metrics-for-ranking-problems-introduction-and-examples>Evaluation Metrics for Ranking problems: Introduction and Examples</a></li><li><a href=https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf>Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods</a></li><li><a href=https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6>Advanced RAG Techniques: an Illustrated Overview</a></li><li><a href=https://fabianhertwig.com/blog/information-retrieval-metrics/#ndcgk---normalized-discounted-cumulative-gain-at-k>https://fabianhertwig.com/blog/information-retrieval-metrics/#ndcgk---normalized-discounted-cumulative-gain-at-k</a></li><li><a href=https://github.com/cvangysel/pytrec_eval>https://github.com/cvangysel/pytrec_eval</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/><span class=title>¬´</span><br><span>MoE Models</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml/flash_attn/><span class=title>¬ª</span><br><span>Flash Attention</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>