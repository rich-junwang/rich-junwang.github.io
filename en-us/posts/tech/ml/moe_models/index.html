<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MoE Models | Jun's Blog</title><meta name=keywords content><meta name=description content="MoE Models"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="MoE Models"><meta property="og:description" content="MoE Models"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-18T00:18:23+08:00"><meta property="article:modified_time" content="2025-02-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="MoE Models"><meta name=twitter:description content="MoE Models"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"MoE Models","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MoE Models","name":"MoE Models","description":"MoE Models","keywords":[""],"articleBody":"The biggest lesson weâ€™ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today weâ€™ll closely examine the Mixtral model to study MoE models.\nIntroduction Most of todayâ€™s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:\nFigure 1. Switch MoE Model In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.\nThe follow code snippet shows how it works for Mixtral MoE model at inference time.\n# æ³¨æ„ï¼šä¸ºäº†å®¹æ˜“ç†è§£ï¼Œæˆ‘å¯¹ä»£ç è¿›è¡Œäº†ç®€åŒ–ï¼ŒåŒæ—¶ä¸è€ƒè™‘batch sizeï¼Œå®é™…ä½¿ç”¨æ—¶è¿˜æ˜¯è¦ç”¨å®˜æ–¹ä»£ç  class MixtralSparseMoeBlock(nn.Module): def __init__(self, config): super().__init__() self.gate = nn.Linear(self.hidden_dim, 8) self.experts = nn.ModuleList([MLP(config) for _ in range(8)]) def forward(self, x): # å¯¹æ¯ä¸ªtokenè®¡ç®—8ä¸ªexpertçš„æƒé‡ï¼Œå¹¶å°†æƒé‡å½’ä¸€åŒ– router_logits = self.gate(x) routing_weights = F.softmax(router_logits, dim=1) # æ¯ä¸ªtokené€‰æ‹©top-2 expertsçš„æƒé‡ã€ç´¢å¼•ï¼Œ å¹¶å°†ç´¢å¼•è½¬ä¸ºsize=(len(tokens), 8)çš„ç‹¬çƒ­ç¼–ç  routing_weights, selected_experts = torch.top2(routing_weights, dim=-1) expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=8) # é‡æ–°å°†top-2 expertçš„æƒé‡å½’ä¸€åŒ–ï¼ˆå› ä¸ºåˆ æ‰å…¶ä½™6ä¸ªexpertï¼Œæƒé‡çš„å’Œä¸ç­‰äº1äº†ï¼‰ routing_weights /= routing_weights.sum(dim=-1) # åˆ›å»ºå½¢çŠ¶å’Œxä¸€è‡´ï¼Œåˆå§‹å€¼ä¸º0çš„çŸ©é˜µï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ªexpertçš„è¾“å‡º final_hidden_states = torch.zeros_like(x) for expert_idx in range(8): # é€‰æ‹©å½“å‰ä½¿ç”¨çš„expert expert_layer = self.experts[expert_idx] # é€‰æ‹©å½“å‰expertå¯¹åº”çš„index idx_list, top_x_list = torch.where(expert_mask[expert_idx]) # é€‰æ‹©éœ€è¦è®¡ç®—çš„çŠ¶æ€ current_state = x[top_x_list] # é€‰æ‹©å½“å‰expertå¯¹æ¯ä¸ªtokençš„æƒé‡ current_routing_weights = routing_weights.t()[top_x_list, idx_list] # å°†é€‰æ‹©çš„çŠ¶æ€è¾“å…¥ç»™ä¸“å®¶æ¨¡å‹è®¡ç®—ï¼Œå¹¶ä¹˜ä¸Šæƒé‡ current_hidden_states = expert_layer(current_state) * current_routing_weights # å°†æ¯ä¸ªexpertçš„è¾“å‡ºæŒ‰ç…§ç´¢å¼•åŠ åˆ°æœ€ç»ˆç»“æœé‡Œ final_hidden_states.index_add_(0, top_x_list, current_hidden_states) return final_hidden_states Dynamic Routing The workflow of topk router is as follows:\nCalculate the logits by the router gating network. Calculate the routing probabilities and map for top-k selection with score function. [Optional] Apply token dropping to top-k expert selection. [Optional] Apply the auxiliary load balancing loss for the given scores and routing map. A typical Megatron-LM implementation is like below:\ndef forward(self, input): # routing self._maintain_float32_expert_bias() input = self.apply_input_jitter(input) # gating logits = torch.nn.functional.linear(input.to(router_dtype), self.gate_weight.to(router_dtype)) # Apply Z-Loss logits = self.apply_z_loss(logits) # usually with loading balance loss here below. scores, routing_map, _ = topk_softmax_with_capacity(logits, top_k) # token dispatch (dispatched_input, tokens_per_expert) = self.token_dispatcher.token_permutation( hidden_states, probs, routing_map ) # experts forward expert_output, mlp_bias = self.experts(dispatched_input, tokens_per_expert) # unpermutate to restore the order output, mlp_bias = self.token_dispatcher.token_unpermutation(expert_output, mlp_bias) There are a couple of ways to design router to route tokens to each expert. Ideally, we want to design a router that could make each expert specialize one of domains/tasks. Obviously there is no straightforward way to achieve this. In Mixtral, softmax-topk based gating mechanism is used to select experts.\nFor any input $x$ of dimension $[\\text{sequence\\_len}, \\text{dim}]$, it multiplies with a gate matrix $W$ of shape $[\\text{dim}, 8]$, then we get a router representation of shape $[\\text{sequence\\_len}, 8]$. It selects top k (num of experts per token) logits which then go through softmax op to normalize to get k experts weights. In Mixtral, the k is equal to 2.\nMoE training is prone to instability issues because it has extra exponential functions. To deal with mixed precision roundoff errors, people apply z-loss to logits before sending them to router.\n$$ L_z = \\frac{1}{B} \\sum_{i=1}^{B} (log\\sum_{j=1}^{N}e^{x_j^{(i)}})^2 $$\nThis is called router z-loss [9]. In python,\nz_loss = torch.mean(torch.square(torch.logsumexp(logits, dim=-1))) * z_loss_coeff Ref [7] uses the similar kind of approach to stabilize the training. $$ L_{max_z} = 2 e^{-4} * z^2 $$ where $z$ is the max logit value.\nLoad Balancing For dynamic routing, which token is routed to which expert is unknown upfront, so there exists the load balancing issue. Common solution is to add an auxiliary load balancing loss [2].\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$\nHere $N$ is the number of experts, $T$ is the total number of tokens in batch $B$. $f_i$ is the fraction of tokens dispatched to expert $i$ and $P_i$ is the fraction of the router probability allocated for expert $i$, i.e. the summation of probability assigning token to expert i for all tokens $$ P_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x) $$\nNote that this loss is added for each MoE layer.\nLoss-free Load Balancing Introducing an auxiliary loss to encourage load balance inevitably would bring non-negligible interference gradients into language training and most of time would impair the model performance. To overcome this issue, DeepSeek paper [11] proposed a loss free load balancing strategy.\nThe idea is to introduce an expert-wise learnable bias term to modulate Top-k routing. Specifically, in the previous approach, the Top-k experts were selected based solely on their routing scores. In the new approach, the selection is based on the expertsâ€™ gating scores plus an expert bias term. When an expert is overloaded, its bias is decreased, thereby reducing the probability of it being activated; when an expert is underloaded, its bias is increased, thereby raising the probability of it being activated.\n$$ g_{i,t} = \\begin{cases} s_{i,t}, \u0026 s_{i,t} + b_i \\in \\text{Topk}({s_{j,t} + b_j \\mid 1 \\leq j \\leq N}, K), \\\\ 0, \u0026 \\text{otherwise}. \\end{cases} $$\nExpert Bias There are some nuances in MoE expert bias implementations. The router/gating function computes logits + biases â†’ softmax â†’ expert probabilities.\nIf biases canâ€™t capture small shifts, the softmax output may â€œlock inâ€ certain experts and fail to distribute properly. That leads to expert bias (some experts dominate unfairly, others starve).\nimport torch print(torch.tensor(0.5, dtype=torch.bfloat16) + 1e-3 ) # 0.001 is ignored # tensor(0.5000, dtype=torch.bfloat16) Token Dispatch Once the router outputs the assignment (e.g., token A â†’ Expert 3, token B â†’ Expert 7), the model must dispatch the tokens to the chosen experts. This involves:\nGathering the tokens assigned to each expert. Sending them to the correct expert for computation. Collecting the expert outputs and reordering them back to match the original token sequence. Token dispatch in MoE, image from [12] Training Directly training MoE could be challenging due to low efficiency. One popular approach is called sparse upcycling to use pretrained dense model to initialize the sparse model and continue to train for certain steps.\nFigure 2. Training MoE Model Expert Parallelism The logic of Expert Parallelism is shown in the figure below: each EP rank contains only a subset of experts, and the tokens on each EP rank are dispatched to the experts on other EP ranks according to the gating results. This process is carried out through all-to-all communication.\nEP in MoE, image from [10] References Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity BASE Layers: Simplifying Training of Large, Sparse Models Mixtral of Experts Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference Baichuan 2: Open Large-scale Language Models DeepSeek-V3 Technical Report ST-MoE: Designing Stable and Transferable Sparse Expert Models GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts https://zhuanlan.zhihu.com/p/13997146226 https://github.com/pjlab-sys4nlp/llama-moe https://github.com/NVIDIA/NeMo https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe https://github.com/stanford-futuredata/megablocks Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models ","wordCount":"1487","inLanguage":"en-us","datePublished":"2025-02-18T00:18:23+08:00","dateModified":"2025-02-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>MoE Models</h1><div class=post-description>MoE Models</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2025-02-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1487 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>3 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/ml/ style=color:var(--secondary)!important>ML</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#dynamic-routing aria-label="Dynamic Routing">Dynamic Routing</a></li><li><a href=#load-balancing aria-label="Load Balancing">Load Balancing</a><ul><li><a href=#loss-free-load-balancing aria-label="Loss-free Load Balancing">Loss-free Load Balancing</a></li><li><a href=#expert-bias aria-label="Expert Bias">Expert Bias</a></li></ul></li><li><a href=#token-dispatch aria-label="Token Dispatch">Token Dispatch</a></li><li><a href=#training aria-label=Training>Training</a></li></ul><li><a href=#expert-parallelism aria-label="Expert Parallelism">Expert Parallelism</a><ul><li><a href=#references aria-label=References>References</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>The biggest lesson we&rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we&rsquo;ll closely examine the Mixtral model to study MoE models.</p><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>Most of today&rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:</p><p align=center><img alt="MoE model" src=images/moe.png width=80% height=auto/>
Figure 1. Switch MoE Model<br></p><p>In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.</p><p>The follow code snippet shows how it works for Mixtral MoE model at inference time.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># æ³¨æ„ï¼šä¸ºäº†å®¹æ˜“ç†è§£ï¼Œæˆ‘å¯¹ä»£ç è¿›è¡Œäº†ç®€åŒ–ï¼ŒåŒæ—¶ä¸è€ƒè™‘batch sizeï¼Œå®é™…ä½¿ç”¨æ—¶è¿˜æ˜¯è¦ç”¨å®˜æ–¹ä»£ç </span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MixtralSparseMoeBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gate <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>hidden_dim, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>experts <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([MLP(config) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>8</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># å¯¹æ¯ä¸ªtokenè®¡ç®—8ä¸ªexpertçš„æƒé‡ï¼Œå¹¶å°†æƒé‡å½’ä¸€åŒ–</span>
</span></span><span style=display:flex><span>        router_logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>gate(x) 
</span></span><span style=display:flex><span>        routing_weights <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(router_logits, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) 
</span></span><span style=display:flex><span>        <span style=color:#75715e># æ¯ä¸ªtokené€‰æ‹©top-2 expertsçš„æƒé‡ã€ç´¢å¼•ï¼Œ å¹¶å°†ç´¢å¼•è½¬ä¸ºsize=(len(tokens), 8)çš„ç‹¬çƒ­ç¼–ç </span>
</span></span><span style=display:flex><span>        routing_weights, selected_experts <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>top2(routing_weights, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>) 
</span></span><span style=display:flex><span>        expert_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>one_hot(selected_experts, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># é‡æ–°å°†top-2 expertçš„æƒé‡å½’ä¸€åŒ–ï¼ˆå› ä¸ºåˆ æ‰å…¶ä½™6ä¸ªexpertï¼Œæƒé‡çš„å’Œä¸ç­‰äº1äº†ï¼‰</span>
</span></span><span style=display:flex><span>        routing_weights <span style=color:#f92672>/=</span> routing_weights<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)  
</span></span><span style=display:flex><span>            <span style=color:#75715e># åˆ›å»ºå½¢çŠ¶å’Œxä¸€è‡´ï¼Œåˆå§‹å€¼ä¸º0çš„çŸ©é˜µï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ªexpertçš„è¾“å‡º</span>
</span></span><span style=display:flex><span>        final_hidden_states <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros_like(x) 
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> expert_idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰ä½¿ç”¨çš„expert</span>
</span></span><span style=display:flex><span>            expert_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>experts[expert_idx] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰expertå¯¹åº”çš„index</span>
</span></span><span style=display:flex><span>            idx_list, top_x_list <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>where(expert_mask[expert_idx]) 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©éœ€è¦è®¡ç®—çš„çŠ¶æ€</span>
</span></span><span style=display:flex><span>            current_state <span style=color:#f92672>=</span> x[top_x_list] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰expertå¯¹æ¯ä¸ªtokençš„æƒé‡</span>
</span></span><span style=display:flex><span>            current_routing_weights <span style=color:#f92672>=</span> routing_weights<span style=color:#f92672>.</span>t()[top_x_list, idx_list] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># å°†é€‰æ‹©çš„çŠ¶æ€è¾“å…¥ç»™ä¸“å®¶æ¨¡å‹è®¡ç®—ï¼Œå¹¶ä¹˜ä¸Šæƒé‡</span>
</span></span><span style=display:flex><span>            current_hidden_states <span style=color:#f92672>=</span> expert_layer(current_state) <span style=color:#f92672>*</span> current_routing_weights 
</span></span><span style=display:flex><span>            <span style=color:#75715e># å°†æ¯ä¸ªexpertçš„è¾“å‡ºæŒ‰ç…§ç´¢å¼•åŠ åˆ°æœ€ç»ˆç»“æœé‡Œ</span>
</span></span><span style=display:flex><span>            final_hidden_states<span style=color:#f92672>.</span>index_add_(<span style=color:#ae81ff>0</span>, top_x_list, current_hidden_states) 
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> final_hidden_states
</span></span></code></pre></div><h3 id=dynamic-routing>Dynamic Routing<a hidden class=anchor aria-hidden=true href=#dynamic-routing>#</a></h3><p>The workflow of topk router is as follows:</p><ol><li>Calculate the logits by the router gating network.</li><li>Calculate the routing probabilities and map for top-k selection with score function.</li><li>[Optional] Apply token dropping to top-k expert selection.</li><li>[Optional] Apply the auxiliary load balancing loss for the given scores and routing map.</li></ol><p>A typical Megatron-LM implementation is like below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, input):
</span></span><span style=display:flex><span>    <span style=color:#75715e># routing </span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>_maintain_float32_expert_bias()
</span></span><span style=display:flex><span>    input <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>apply_input_jitter(input)
</span></span><span style=display:flex><span>    <span style=color:#75715e># gating</span>
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>linear(input<span style=color:#f92672>.</span>to(router_dtype), self<span style=color:#f92672>.</span>gate_weight<span style=color:#f92672>.</span>to(router_dtype))
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply Z-Loss</span>
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>apply_z_loss(logits)
</span></span><span style=display:flex><span>    <span style=color:#75715e># usually with loading balance loss here below.</span>
</span></span><span style=display:flex><span>    scores, routing_map, _ <span style=color:#f92672>=</span> topk_softmax_with_capacity(logits, top_k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># token dispatch    </span>
</span></span><span style=display:flex><span>    (dispatched_input, tokens_per_expert) <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>token_dispatcher<span style=color:#f92672>.</span>token_permutation(
</span></span><span style=display:flex><span>        hidden_states, probs, routing_map
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># experts forward</span>
</span></span><span style=display:flex><span>    expert_output, mlp_bias <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>experts(dispatched_input, tokens_per_expert)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># unpermutate to restore the order</span>
</span></span><span style=display:flex><span>    output, mlp_bias <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>token_dispatcher<span style=color:#f92672>.</span>token_unpermutation(expert_output, mlp_bias)
</span></span></code></pre></div><p>There are a couple of ways to design router to route tokens to each expert. Ideally, we want to design a router that could make each expert specialize one of domains/tasks. Obviously there is no straightforward way to achieve this. In Mixtral, softmax-topk based gating mechanism is used to select experts.</p><p>For any input $x$ of dimension $[\text{sequence\_len}, \text{dim}]$, it multiplies with a gate matrix $W$ of shape $[\text{dim}, 8]$, then we get a router representation of shape $[\text{sequence\_len}, 8]$. It selects top k (num of experts per token) logits which then go through softmax op to normalize to get k experts weights. In Mixtral, the k is equal to 2.</p><p>MoE training is prone to instability issues because it has extra exponential functions. To deal with mixed precision roundoff errors, people apply z-loss to logits before sending them to router.</p><p>$$
L_z = \frac{1}{B} \sum_{i=1}^{B} (log\sum_{j=1}^{N}e^{x_j^{(i)}})^2
$$</p><p>This is called router z-loss [9]. In python,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>z_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean(torch<span style=color:#f92672>.</span>square(torch<span style=color:#f92672>.</span>logsumexp(logits, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>))) <span style=color:#f92672>*</span> z_loss_coeff
</span></span></code></pre></div><p>Ref [7] uses the similar kind of approach to stabilize the training.
$$
L_{max_z} = 2 e^{-4} * z^2
$$
where $z$ is the max logit value.</p><h3 id=load-balancing>Load Balancing<a hidden class=anchor aria-hidden=true href=#load-balancing>#</a></h3><p>For dynamic routing, which token is routed to which expert is unknown upfront, so there exists the load balancing issue. Common solution is to add an auxiliary load balancing loss [2].</p><p>$$
\text{loss} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
$$</p><p>Here $N$ is the number of experts, $T$ is the total number of tokens in batch $B$. $f_i$ is the fraction of tokens dispatched to expert $i$ and $P_i$ is the fraction of the router probability allocated for expert $i$, i.e. the summation of probability assigning token to expert i for all tokens
$$
P_i = \frac{1}{T} \sum_{x \in \mathcal{B}} p_i(x)
$$</p><p>Note that this loss is added for each MoE layer.</p><h4 id=loss-free-load-balancing>Loss-free Load Balancing<a hidden class=anchor aria-hidden=true href=#loss-free-load-balancing>#</a></h4><p>Introducing an auxiliary loss to encourage load balance inevitably would bring non-negligible interference gradients into language training and most of time would impair the model performance. To overcome this issue, DeepSeek paper [11] proposed a loss free load balancing strategy.</p><p>The idea is to introduce an expert-wise learnable bias term to modulate Top-k routing. Specifically, in the previous approach, the Top-k experts were selected based solely on their routing scores. In the new approach, the selection is based on the expertsâ€™ gating scores plus an expert bias term. When an expert is overloaded, its bias is decreased, thereby reducing the probability of it being activated; when an expert is underloaded, its bias is increased, thereby raising the probability of it being activated.</p><p>$$
g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t} + b_i \in \text{Topk}({s_{j,t} + b_j \mid 1 \leq j \leq N}, K), \\
0, & \text{otherwise}.
\end{cases}
$$</p><h4 id=expert-bias>Expert Bias<a hidden class=anchor aria-hidden=true href=#expert-bias>#</a></h4><p>There are some nuances in MoE expert bias implementations. The router/gating function computes logits + biases â†’ softmax â†’ expert probabilities.</p><p>If biases canâ€™t capture small shifts, the softmax output may &ldquo;lock in&rdquo; certain experts and fail to distribute properly. That leads to expert bias (some experts dominate unfairly, others starve).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch 
</span></span><span style=display:flex><span>print(torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>0.5</span>, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bfloat16) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-3</span> )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 0.001 is ignored</span>
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(0.5000, dtype=torch.bfloat16)</span>
</span></span></code></pre></div><h3 id=token-dispatch>Token Dispatch<a hidden class=anchor aria-hidden=true href=#token-dispatch>#</a></h3><p>Once the router outputs the assignment (e.g., token A â†’ Expert 3, token B â†’ Expert 7), the model must dispatch the tokens to the chosen experts. This involves:</p><ul><li>Gathering the tokens assigned to each expert.</li><li>Sending them to the correct expert for computation.</li><li>Collecting the expert outputs and reordering them back to match the original token sequence.</li></ul><div align=center><img src=images/token_dispatch.png style=width:100%;height:auto> Token dispatch in MoE, image from [12]</div><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><p>Directly training MoE could be challenging due to low efficiency. One popular approach is called sparse upcycling to use pretrained dense model to initialize the sparse model and continue to train for certain steps.</p><p align=center><img alt="Sparse upcycling" src=images/upcycle.png width=80% height=auto/>
Figure 2. Training MoE Model<br></p><h2 id=expert-parallelism>Expert Parallelism<a hidden class=anchor aria-hidden=true href=#expert-parallelism>#</a></h2><p>The logic of Expert Parallelism is shown in the figure below: each EP rank contains only a subset of experts, and the tokens on each EP rank are dispatched to the experts on other EP ranks according to the gating results. This process is carried out through all-to-all communication.</p><div align=center><img src=images/scaling_moe.png style=width:100%;height:auto> EP in MoE, image from [10]</div><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ol><li><a href=https://arxiv.org/abs/1701.06538>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li><li><a href=https://arxiv.org/pdf/2101.03961.pdf>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></li><li><a href=https://arxiv.org/pdf/2103.16716.pdf>BASE Layers: Simplifying Training of Large, Sparse Models</a><br></li><li><a href=https://arxiv.org/pdf/2401.04088.pdf>Mixtral of Experts</a><br></li><li><a href=https://arxiv.org/abs/2212.05055>Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints</a><br></li><li><a href=https://arxiv.org/pdf/2110.03742.pdf>Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference</a></li><li><a href=https://arxiv.org/abs/2309.10305>Baichuan 2: Open Large-scale Language Models</a></li><li><a href=https://arxiv.org/html/2412.19437v1>DeepSeek-V3 Technical Report</a></li><li><a href=https://arxiv.org/abs/2202.08906>ST-MoE: Designing Stable and Transferable Sparse Expert Models</a></li><li><a href=https://arxiv.org/abs/2006.16668>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></li><li><a href=https://arxiv.org/abs/2408.15664>Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts</a></li><li><a href=https://zhuanlan.zhihu.com/p/13997146226>https://zhuanlan.zhihu.com/p/13997146226</a></li><li><a href=https://github.com/pjlab-sys4nlp/llama-moe>https://github.com/pjlab-sys4nlp/llama-moe</a></li><li><a href=https://github.com/NVIDIA/NeMo>https://github.com/NVIDIA/NeMo</a></li><li><a href=https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe>https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe</a></li><li><a href=https://github.com/stanford-futuredata/megablocks>https://github.com/stanford-futuredata/megablocks</a></li><li><a href=https://arxiv.org/abs/2501.11873>Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/llm_inference/pd_disaggregation/><span class=title>Â«</span><br><span>PD Disaggregation</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml/model_distillation/><span class=title>Â»</span><br><span>Distillation</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>