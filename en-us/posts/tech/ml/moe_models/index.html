<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MoE Models | Jun's Blog</title><meta name=keywords content><meta name=description content="The biggest lesson we&rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we&rsquo;ll closely examine the Mixtral model to study MoE models.
Introduction
Most of today&rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:

     
    Figure 1. Switch MoE Model
    

In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/><link crossorigin=anonymous href=../../../../../assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../../../assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="MoE Models"><meta property="og:description" content="The biggest lesson we&rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we&rsquo;ll closely examine the Mixtral model to study MoE models.
Introduction
Most of today&rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:

     
    Figure 1. Switch MoE Model
    

In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-18T00:18:23+08:00"><meta property="article:modified_time" content="2025-02-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="MoE Models"><meta name=twitter:description content="The biggest lesson we&rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we&rsquo;ll closely examine the Mixtral model to study MoE models.
Introduction
Most of today&rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:

     
    Figure 1. Switch MoE Model
    

In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"MoE Models","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MoE Models","name":"MoE Models","description":"The biggest lesson we\u0026rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we\u0026rsquo;ll closely examine the Mixtral model to study MoE models.\nIntroduction Most of today\u0026rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:\nFigure 1. Switch MoE Model In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.\n","keywords":[],"articleBody":"The biggest lesson weâ€™ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today weâ€™ll closely examine the Mixtral model to study MoE models.\nIntroduction Most of todayâ€™s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:\nFigure 1. Switch MoE Model In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.\nThe follow code snippet shows how it works for Mixtral MoE model at inference time.\n# æ³¨æ„ï¼šä¸ºäº†å®¹æ˜“ç†è§£ï¼Œæˆ‘å¯¹ä»£ç è¿›è¡Œäº†ç®€åŒ–ï¼ŒåŒæ—¶ä¸è€ƒè™‘batch sizeï¼Œå®é™…ä½¿ç”¨æ—¶è¿˜æ˜¯è¦ç”¨å®˜æ–¹ä»£ç  class MixtralSparseMoeBlock(nn.Module): def __init__(self, config): super().__init__() self.gate = nn.Linear(self.hidden_dim, 8) self.experts = nn.ModuleList([MLP(config) for _ in range(8)]) def forward(self, x): # å¯¹æ¯ä¸ªtokenè®¡ç®—8ä¸ªexpertçš„æƒé‡ï¼Œå¹¶å°†æƒé‡å½’ä¸€åŒ– router_logits = self.gate(x) routing_weights = F.softmax(router_logits, dim=1) # æ¯ä¸ªtokené€‰æ‹©top-2 expertsçš„æƒé‡ã€ç´¢å¼•ï¼Œ å¹¶å°†ç´¢å¼•è½¬ä¸ºsize=(len(tokens), 8)çš„ç‹¬çƒ­ç¼–ç  routing_weights, selected_experts = torch.top2(routing_weights, dim=-1) expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=8) # é‡æ–°å°†top-2 expertçš„æƒé‡å½’ä¸€åŒ–ï¼ˆå› ä¸ºåˆ æ‰å…¶ä½™6ä¸ªexpertï¼Œæƒé‡çš„å’Œä¸ç­‰äº1äº†ï¼‰ routing_weights /= routing_weights.sum(dim=-1) # åˆ›å»ºå½¢çŠ¶å’Œxä¸€è‡´ï¼Œåˆå§‹å€¼ä¸º0çš„çŸ©é˜µï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ªexpertçš„è¾“å‡º final_hidden_states = torch.zeros_like(x) for expert_idx in range(8): # é€‰æ‹©å½“å‰ä½¿ç”¨çš„expert expert_layer = self.experts[expert_idx] # é€‰æ‹©å½“å‰expertå¯¹åº”çš„index idx_list, top_x_list = torch.where(expert_mask[expert_idx]) # é€‰æ‹©éœ€è¦è®¡ç®—çš„çŠ¶æ€ current_state = x[top_x_list] # é€‰æ‹©å½“å‰expertå¯¹æ¯ä¸ªtokençš„æƒé‡ current_routing_weights = routing_weights.t()[top_x_list, idx_list] # å°†é€‰æ‹©çš„çŠ¶æ€è¾“å…¥ç»™ä¸“å®¶æ¨¡å‹è®¡ç®—ï¼Œå¹¶ä¹˜ä¸Šæƒé‡ current_hidden_states = expert_layer(current_state) * current_routing_weights # å°†æ¯ä¸ªexpertçš„è¾“å‡ºæŒ‰ç…§ç´¢å¼•åŠ åˆ°æœ€ç»ˆç»“æœé‡Œ final_hidden_states.index_add_(0, top_x_list, current_hidden_states) return final_hidden_states Dynamic Routing The workflow of topk router is as follows:\nCalculate the logits by the router gating network. Calculate the routing probabilities and map for top-k selection with score function. [Optional] Apply token dropping to top-k expert selection. [Optional] Apply the auxiliary load balancing loss for the given scores and routing map. A typical Megatron-LM implementation is like below:\ndef forward(self, input): # routing self._maintain_float32_expert_bias() input = self.apply_input_jitter(input) # gating logits = torch.nn.functional.linear(input.to(router_dtype), self.gate_weight.to(router_dtype)) # Apply Z-Loss logits = self.apply_z_loss(logits) # usually with loading balance loss here below. scores, routing_map, _ = topk_softmax_with_capacity(logits, top_k) # token dispatch (dispatched_input, tokens_per_expert) = self.token_dispatcher.token_permutation( hidden_states, probs, routing_map ) # experts forward expert_output, mlp_bias = self.experts(dispatched_input, tokens_per_expert) # unpermutate to restore the order output, mlp_bias = self.token_dispatcher.token_unpermutation(expert_output, mlp_bias) There are a couple of ways to design router to route tokens to each expert. Ideally, we want to design a router that could make each expert specialize one of domains/tasks. Obviously there is no straightforward way to achieve this. In Mixtral, softmax-topk based gating mechanism is used to select experts.\nFor any input $x$ of dimension $[\\text{sequence\\_len}, \\text{dim}]$, it multiplies with a gate matrix $W$ of shape $[\\text{dim}, 8]$, then we get a router representation of shape $[\\text{sequence\\_len}, 8]$. It selects top k (num of experts per token) logits which then go through softmax op to normalize to get k experts weights. In Mixtral, the k is equal to 2.\nMoE training is prone to instability issues because it has extra exponential functions. To deal with mixed precision roundoff errors, people apply z-loss to logits before sending them to router.\n$$ L_z = \\frac{1}{B} \\sum_{i=1}^{B} (log\\sum_{j=1}^{N}e^{x_j^{(i)}})^2 $$\nThis is called router z-loss [9]. In python,\nz_loss = torch.mean(torch.square(torch.logsumexp(logits, dim=-1))) * z_loss_coeff Ref [7] uses the similar kind of approach to stabilize the training. $$ L_{max_z} = 2 e^{-4} * z^2 $$ where $z$ is the max logit value.\nLoad Balancing For dynamic routing, which token is routed to which expert is unknown upfront, so there exists the load balancing issue. Common solution is to add an auxiliary load balancing loss [2].\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$\nHere $N$ is the number of experts, $T$ is the total number of tokens in batch $B$. $f_i$ is the fraction of tokens dispatched to expert $i$ and $P_i$ is the fraction of the router probability allocated for expert $i$, i.e. the summation of probability assigning token to expert i for all tokens $$ P_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x) $$\nNote that this loss is added for each MoE layer.\nLoss-free Load Balancing Introducing an auxiliary loss to encourage load balance inevitably would bring non-negligible interference gradients into language training and most of time would impair the model performance. To overcome this issue, DeepSeek paper [11] proposed a loss free load balancing strategy.\nThe idea is to introduce an expert-wise learnable bias term to modulate Top-k routing. Specifically, in the previous approach, the Top-k experts were selected based solely on their routing scores. In the new approach, the selection is based on the expertsâ€™ gating scores plus an expert bias term. When an expert is overloaded, its bias is decreased, thereby reducing the probability of it being activated; when an expert is underloaded, its bias is increased, thereby raising the probability of it being activated.\n$$ g_{i,t} = \\begin{cases} s_{i,t}, \u0026 s_{i,t} + b_i \\in \\text{Topk}({s_{j,t} + b_j \\mid 1 \\leq j \\leq N}, K), \\\\ 0, \u0026 \\text{otherwise}. \\end{cases} $$\nExpert Bias There are some nuances in MoE expert bias implementations. The router/gating function computes logits + biases â†’ softmax â†’ expert probabilities.\nIf biases canâ€™t capture small shifts, the softmax output may â€œlock inâ€ certain experts and fail to distribute properly. That leads to expert bias (some experts dominate unfairly, others starve).\nimport torch print(torch.tensor(0.5, dtype=torch.bfloat16) + 1e-3 ) # 0.001 is ignored # tensor(0.5000, dtype=torch.bfloat16) Token Dispatch Once the router outputs the assignment (e.g., token A â†’ Expert 3, token B â†’ Expert 7), the model must dispatch the tokens to the chosen experts. This involves:\nGathering the tokens assigned to each expert. Sending them to the correct expert for computation. Collecting the expert outputs and reordering them back to match the original token sequence. Token dispatch in MoE, image from [12] Training Directly training MoE could be challenging due to low efficiency. One popular approach is called sparse upcycling to use pretrained dense model to initialize the sparse model and continue to train for certain steps.\nFigure 2. Training MoE Model Expert Parallelism The logic of Expert Parallelism is shown in the figure below: each EP rank contains only a subset of experts, and the tokens on each EP rank are dispatched to the experts on other EP ranks according to the gating results. This process is carried out through all-to-all communication.\nEP in MoE, image from [10] References Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity BASE Layers: Simplifying Training of Large, Sparse Models Mixtral of Experts Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference Baichuan 2: Open Large-scale Language Models DeepSeek-V3 Technical Report ST-MoE: Designing Stable and Transferable Sparse Expert Models GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts https://zhuanlan.zhihu.com/p/13997146226 https://github.com/pjlab-sys4nlp/llama-moe https://github.com/NVIDIA/NeMo https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe https://github.com/stanford-futuredata/megablocks Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models ","wordCount":"1487","inLanguage":"en-us","datePublished":"2025-02-18T00:18:23+08:00","dateModified":"2025-02-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/moe_models/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>MoE Models</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2025-02-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1487 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>3 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#dynamic-routing aria-label="Dynamic Routing">Dynamic Routing</a></li><li><a href=#load-balancing aria-label="Load Balancing">Load Balancing</a><ul><li><a href=#loss-free-load-balancing aria-label="Loss-free Load Balancing">Loss-free Load Balancing</a></li><li><a href=#expert-bias aria-label="Expert Bias">Expert Bias</a></li></ul></li><li><a href=#token-dispatch aria-label="Token Dispatch">Token Dispatch</a></li><li><a href=#training aria-label=Training>Training</a></li></ul><li><a href=#expert-parallelism aria-label="Expert Parallelism">Expert Parallelism</a><ul><li><a href=#references aria-label=References>References</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>The biggest lesson we&rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we&rsquo;ll closely examine the Mixtral model to study MoE models.</p><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>Most of today&rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:</p><p align=center><img alt="MoE model" src=images/moe.png width=80% height=auto/>
Figure 1. Switch MoE Model<br></p><p>In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.</p><p>The follow code snippet shows how it works for Mixtral MoE model at inference time.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># æ³¨æ„ï¼šä¸ºäº†å®¹æ˜“ç†è§£ï¼Œæˆ‘å¯¹ä»£ç è¿›è¡Œäº†ç®€åŒ–ï¼ŒåŒæ—¶ä¸è€ƒè™‘batch sizeï¼Œå®é™…ä½¿ç”¨æ—¶è¿˜æ˜¯è¦ç”¨å®˜æ–¹ä»£ç </span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MixtralSparseMoeBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gate <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>hidden_dim, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>experts <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([MLP(config) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>8</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># å¯¹æ¯ä¸ªtokenè®¡ç®—8ä¸ªexpertçš„æƒé‡ï¼Œå¹¶å°†æƒé‡å½’ä¸€åŒ–</span>
</span></span><span style=display:flex><span>        router_logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>gate(x) 
</span></span><span style=display:flex><span>        routing_weights <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(router_logits, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) 
</span></span><span style=display:flex><span>        <span style=color:#75715e># æ¯ä¸ªtokené€‰æ‹©top-2 expertsçš„æƒé‡ã€ç´¢å¼•ï¼Œ å¹¶å°†ç´¢å¼•è½¬ä¸ºsize=(len(tokens), 8)çš„ç‹¬çƒ­ç¼–ç </span>
</span></span><span style=display:flex><span>        routing_weights, selected_experts <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>top2(routing_weights, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>) 
</span></span><span style=display:flex><span>        expert_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>one_hot(selected_experts, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># é‡æ–°å°†top-2 expertçš„æƒé‡å½’ä¸€åŒ–ï¼ˆå› ä¸ºåˆ æ‰å…¶ä½™6ä¸ªexpertï¼Œæƒé‡çš„å’Œä¸ç­‰äº1äº†ï¼‰</span>
</span></span><span style=display:flex><span>        routing_weights <span style=color:#f92672>/=</span> routing_weights<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)  
</span></span><span style=display:flex><span>            <span style=color:#75715e># åˆ›å»ºå½¢çŠ¶å’Œxä¸€è‡´ï¼Œåˆå§‹å€¼ä¸º0çš„çŸ©é˜µï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ªexpertçš„è¾“å‡º</span>
</span></span><span style=display:flex><span>        final_hidden_states <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros_like(x) 
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> expert_idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰ä½¿ç”¨çš„expert</span>
</span></span><span style=display:flex><span>            expert_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>experts[expert_idx] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰expertå¯¹åº”çš„index</span>
</span></span><span style=display:flex><span>            idx_list, top_x_list <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>where(expert_mask[expert_idx]) 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©éœ€è¦è®¡ç®—çš„çŠ¶æ€</span>
</span></span><span style=display:flex><span>            current_state <span style=color:#f92672>=</span> x[top_x_list] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰expertå¯¹æ¯ä¸ªtokençš„æƒé‡</span>
</span></span><span style=display:flex><span>            current_routing_weights <span style=color:#f92672>=</span> routing_weights<span style=color:#f92672>.</span>t()[top_x_list, idx_list] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># å°†é€‰æ‹©çš„çŠ¶æ€è¾“å…¥ç»™ä¸“å®¶æ¨¡å‹è®¡ç®—ï¼Œå¹¶ä¹˜ä¸Šæƒé‡</span>
</span></span><span style=display:flex><span>            current_hidden_states <span style=color:#f92672>=</span> expert_layer(current_state) <span style=color:#f92672>*</span> current_routing_weights 
</span></span><span style=display:flex><span>            <span style=color:#75715e># å°†æ¯ä¸ªexpertçš„è¾“å‡ºæŒ‰ç…§ç´¢å¼•åŠ åˆ°æœ€ç»ˆç»“æœé‡Œ</span>
</span></span><span style=display:flex><span>            final_hidden_states<span style=color:#f92672>.</span>index_add_(<span style=color:#ae81ff>0</span>, top_x_list, current_hidden_states) 
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> final_hidden_states
</span></span></code></pre></div><h3 id=dynamic-routing>Dynamic Routing<a hidden class=anchor aria-hidden=true href=#dynamic-routing>#</a></h3><p>The workflow of topk router is as follows:</p><ol><li>Calculate the logits by the router gating network.</li><li>Calculate the routing probabilities and map for top-k selection with score function.</li><li>[Optional] Apply token dropping to top-k expert selection.</li><li>[Optional] Apply the auxiliary load balancing loss for the given scores and routing map.</li></ol><p>A typical Megatron-LM implementation is like below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, input):
</span></span><span style=display:flex><span>    <span style=color:#75715e># routing </span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>_maintain_float32_expert_bias()
</span></span><span style=display:flex><span>    input <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>apply_input_jitter(input)
</span></span><span style=display:flex><span>    <span style=color:#75715e># gating</span>
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>linear(input<span style=color:#f92672>.</span>to(router_dtype), self<span style=color:#f92672>.</span>gate_weight<span style=color:#f92672>.</span>to(router_dtype))
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply Z-Loss</span>
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>apply_z_loss(logits)
</span></span><span style=display:flex><span>    <span style=color:#75715e># usually with loading balance loss here below.</span>
</span></span><span style=display:flex><span>    scores, routing_map, _ <span style=color:#f92672>=</span> topk_softmax_with_capacity(logits, top_k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># token dispatch    </span>
</span></span><span style=display:flex><span>    (dispatched_input, tokens_per_expert) <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>token_dispatcher<span style=color:#f92672>.</span>token_permutation(
</span></span><span style=display:flex><span>        hidden_states, probs, routing_map
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># experts forward</span>
</span></span><span style=display:flex><span>    expert_output, mlp_bias <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>experts(dispatched_input, tokens_per_expert)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># unpermutate to restore the order</span>
</span></span><span style=display:flex><span>    output, mlp_bias <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>token_dispatcher<span style=color:#f92672>.</span>token_unpermutation(expert_output, mlp_bias)
</span></span></code></pre></div><p>There are a couple of ways to design router to route tokens to each expert. Ideally, we want to design a router that could make each expert specialize one of domains/tasks. Obviously there is no straightforward way to achieve this. In Mixtral, softmax-topk based gating mechanism is used to select experts.</p><p>For any input $x$ of dimension $[\text{sequence\_len}, \text{dim}]$, it multiplies with a gate matrix $W$ of shape $[\text{dim}, 8]$, then we get a router representation of shape $[\text{sequence\_len}, 8]$. It selects top k (num of experts per token) logits which then go through softmax op to normalize to get k experts weights. In Mixtral, the k is equal to 2.</p><p>MoE training is prone to instability issues because it has extra exponential functions. To deal with mixed precision roundoff errors, people apply z-loss to logits before sending them to router.</p><p>$$
L_z = \frac{1}{B} \sum_{i=1}^{B} (log\sum_{j=1}^{N}e^{x_j^{(i)}})^2
$$</p><p>This is called router z-loss [9]. In python,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>z_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean(torch<span style=color:#f92672>.</span>square(torch<span style=color:#f92672>.</span>logsumexp(logits, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>))) <span style=color:#f92672>*</span> z_loss_coeff
</span></span></code></pre></div><p>Ref [7] uses the similar kind of approach to stabilize the training.
$$
L_{max_z} = 2 e^{-4} * z^2
$$
where $z$ is the max logit value.</p><h3 id=load-balancing>Load Balancing<a hidden class=anchor aria-hidden=true href=#load-balancing>#</a></h3><p>For dynamic routing, which token is routed to which expert is unknown upfront, so there exists the load balancing issue. Common solution is to add an auxiliary load balancing loss [2].</p><p>$$
\text{loss} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
$$</p><p>Here $N$ is the number of experts, $T$ is the total number of tokens in batch $B$. $f_i$ is the fraction of tokens dispatched to expert $i$ and $P_i$ is the fraction of the router probability allocated for expert $i$, i.e. the summation of probability assigning token to expert i for all tokens
$$
P_i = \frac{1}{T} \sum_{x \in \mathcal{B}} p_i(x)
$$</p><p>Note that this loss is added for each MoE layer.</p><h4 id=loss-free-load-balancing>Loss-free Load Balancing<a hidden class=anchor aria-hidden=true href=#loss-free-load-balancing>#</a></h4><p>Introducing an auxiliary loss to encourage load balance inevitably would bring non-negligible interference gradients into language training and most of time would impair the model performance. To overcome this issue, DeepSeek paper [11] proposed a loss free load balancing strategy.</p><p>The idea is to introduce an expert-wise learnable bias term to modulate Top-k routing. Specifically, in the previous approach, the Top-k experts were selected based solely on their routing scores. In the new approach, the selection is based on the expertsâ€™ gating scores plus an expert bias term. When an expert is overloaded, its bias is decreased, thereby reducing the probability of it being activated; when an expert is underloaded, its bias is increased, thereby raising the probability of it being activated.</p><p>$$
g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t} + b_i \in \text{Topk}({s_{j,t} + b_j \mid 1 \leq j \leq N}, K), \\
0, & \text{otherwise}.
\end{cases}
$$</p><h4 id=expert-bias>Expert Bias<a hidden class=anchor aria-hidden=true href=#expert-bias>#</a></h4><p>There are some nuances in MoE expert bias implementations. The router/gating function computes logits + biases â†’ softmax â†’ expert probabilities.</p><p>If biases canâ€™t capture small shifts, the softmax output may &ldquo;lock in&rdquo; certain experts and fail to distribute properly. That leads to expert bias (some experts dominate unfairly, others starve).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch 
</span></span><span style=display:flex><span>print(torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>0.5</span>, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bfloat16) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-3</span> )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 0.001 is ignored</span>
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(0.5000, dtype=torch.bfloat16)</span>
</span></span></code></pre></div><h3 id=token-dispatch>Token Dispatch<a hidden class=anchor aria-hidden=true href=#token-dispatch>#</a></h3><p>Once the router outputs the assignment (e.g., token A â†’ Expert 3, token B â†’ Expert 7), the model must dispatch the tokens to the chosen experts. This involves:</p><ul><li>Gathering the tokens assigned to each expert.</li><li>Sending them to the correct expert for computation.</li><li>Collecting the expert outputs and reordering them back to match the original token sequence.</li></ul><div align=center><img src=images/token_dispatch.png style=width:100%;height:auto> Token dispatch in MoE, image from [12]</div><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><p>Directly training MoE could be challenging due to low efficiency. One popular approach is called sparse upcycling to use pretrained dense model to initialize the sparse model and continue to train for certain steps.</p><p align=center><img alt="Sparse upcycling" src=images/upcycle.png width=80% height=auto/>
Figure 2. Training MoE Model<br></p><h2 id=expert-parallelism>Expert Parallelism<a hidden class=anchor aria-hidden=true href=#expert-parallelism>#</a></h2><p>The logic of Expert Parallelism is shown in the figure below: each EP rank contains only a subset of experts, and the tokens on each EP rank are dispatched to the experts on other EP ranks according to the gating results. This process is carried out through all-to-all communication.</p><div align=center><img src=images/scaling_moe.png style=width:100%;height:auto> EP in MoE, image from [10]</div><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ol><li><a href=https://arxiv.org/abs/1701.06538>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li><li><a href=https://arxiv.org/pdf/2101.03961.pdf>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></li><li><a href=https://arxiv.org/pdf/2103.16716.pdf>BASE Layers: Simplifying Training of Large, Sparse Models</a><br></li><li><a href=https://arxiv.org/pdf/2401.04088.pdf>Mixtral of Experts</a><br></li><li><a href=https://arxiv.org/abs/2212.05055>Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints</a><br></li><li><a href=https://arxiv.org/pdf/2110.03742.pdf>Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference</a></li><li><a href=https://arxiv.org/abs/2309.10305>Baichuan 2: Open Large-scale Language Models</a></li><li><a href=https://arxiv.org/html/2412.19437v1>DeepSeek-V3 Technical Report</a></li><li><a href=https://arxiv.org/abs/2202.08906>ST-MoE: Designing Stable and Transferable Sparse Expert Models</a></li><li><a href=https://arxiv.org/abs/2006.16668>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></li><li><a href=https://arxiv.org/abs/2408.15664>Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts</a></li><li><a href=https://zhuanlan.zhihu.com/p/13997146226>https://zhuanlan.zhihu.com/p/13997146226</a></li><li><a href=https://github.com/pjlab-sys4nlp/llama-moe>https://github.com/pjlab-sys4nlp/llama-moe</a></li><li><a href=https://github.com/NVIDIA/NeMo>https://github.com/NVIDIA/NeMo</a></li><li><a href=https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe>https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe</a></li><li><a href=https://github.com/stanford-futuredata/megablocks>https://github.com/stanford-futuredata/megablocks</a></li><li><a href=https://arxiv.org/abs/2501.11873>Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/llm_inference/pd_disaggregation/><span class=title>Â«</span><br><span>PD Disaggregation</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml/model_distillation/><span class=title>Â»</span><br><span>Distillation</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share MoE Models on twitter" href="https://twitter.com/intent/tweet/?text=MoE%20Models&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmoe_models%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share MoE Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmoe_models%2f&amp;title=MoE%20Models&amp;summary=MoE%20Models&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmoe_models%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share MoE Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmoe_models%2f&title=MoE%20Models"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share MoE Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmoe_models%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share MoE Models on whatsapp" href="https://api.whatsapp.com/send?text=MoE%20Models%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmoe_models%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share MoE Models on telegram" href="https://telegram.me/share/url?text=MoE%20Models&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml%2fmoe_models%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>