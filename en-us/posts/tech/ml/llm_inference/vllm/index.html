<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>vLLM | Jun's Blog</title><meta name=keywords content><meta name=description content="VLLM"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml/llm_inference/vllm/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml/llm_inference/vllm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="vLLM"><meta property="og:description" content="VLLM"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml/llm_inference/vllm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-05T12:01:14-07:00"><meta property="article:modified_time" content="2024-10-05T12:01:14-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="vLLM"><meta name=twitter:description content="VLLM"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"vLLM","item":"https://rich-junwang.github.io/en-us/posts/tech/ml/llm_inference/vllm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"vLLM","name":"vLLM","description":"VLLM","keywords":[""],"articleBody":"vLLM Inference Modes vLLM has two inference modes:\noffline batch mode: mostly for offline model evaluation, large-scale, high-throughput inference where latency is less critical online serving mode: Real-time applications like chatbots or APIs where latency is important. The interface to these two approaches are shown in the diagram below.\nOffline Inference Simpler offline batch inference example:\n# batch prompts prompts = [\"Hello, my name is\", \"The president of the United States is\", \"The capital of France is\", \"The future of AI is\",] # sampling parameters sampling_params = SamplingParams(temperature=0.8, top_p=0.95) # load model llm = LLM(model=\"facebook/opt-125m\") # Inference outputs = llm.generate(prompts, sampling_params) for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\") A data parallel (SPMD style) offline inference example.\nimport os from vllm import LLM, SamplingParams from vllm.utils import get_open_port GPUs_per_dp_rank = 2 DP_size = 2 def main(dp_size, dp_rank, dp_master_ip, dp_master_port, GPUs_per_dp_rank): os.environ[\"VLLM_DP_RANK\"] = str(dp_rank) os.environ[\"VLLM_DP_SIZE\"] = str(dp_size) os.environ[\"VLLM_DP_MASTER_IP\"] = dp_master_ip os.environ[\"VLLM_DP_MASTER_PORT\"] = str(dp_master_port) # set devices for each dp_rank os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join( str(i) for i in range(dp_rank * GPUs_per_dp_rank, (dp_rank + 1) * GPUs_per_dp_rank)) # Sample prompts. prompts = [ \"Hello, my name is\", \"The president of the United States is\", \"The capital of France is\", \"The future of AI is\", ] # with DP, each rank should process different prompts. # usually all the DP ranks process a full dataset, # and each rank processes a different part of the dataset. promts_per_rank = len(prompts) // dp_size start = dp_rank * promts_per_rank end = start + promts_per_rank prompts = prompts[start:end] if len(prompts) == 0: # if any rank has no prompts to process, # we need to set a placeholder prompt prompts = [\"Placeholder\"] print(f\"DP rank {dp_rank} needs to process {len(prompts)} prompts\") # Create a sampling params object. # since we are doing data parallel, every rank can have different # sampling params. here we set different max_tokens for different # ranks for demonstration. sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=16 * (dp_rank + 1)) # Create an LLM. llm = LLM(model=\"ibm-research/PowerMoE-3b\", tensor_parallel_size=GPUs_per_dp_rank, enforce_eager=True, enable_expert_parallel=True) outputs = llm.generate(prompts, sampling_params) # Print the outputs. for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\"DP rank {dp_rank}, Prompt: {prompt!r}, \" f\"Generated text: {generated_text!r}\") if __name__ == \"__main__\": from multiprocessing import Process dp_master_ip = \"127.0.0.1\" dp_master_port = get_open_port() procs = [] for i in range(DP_size): proc = Process(target=main, args=(DP_size, i, dp_master_ip, dp_master_port, GPUs_per_dp_rank)) proc.start() procs.append(proc) exit_code = 0 for proc in procs: proc.join() if proc.exitcode: exit_code = proc.exitcode exit(exit_code) API Server The API server utilizes Uvicorn to deploy FastAPI application.\n# Server python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-7b-hf # ClientÔºö(compatible with openai) curl http://localhost:8000/v1/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"meta-llama/Llama-2-7b-hf\", \"prompt\": \"San Francisco is a\", \"max_tokens\": 256, \"temperature\": 0 }' LLM Inference No matter what kind mode vLLM is in for inference, the LLM inference process has two stages:\nprefilling decoding Prefilling phase llm encodes the prompt at once. This is one forward path execution in LLM. Decoding phase, llm generates each token step by step.\nKV Cache KV Cache at inference time refers that caching key and value vectors in self-attention saves redundant computation and accelerates decoding - but takes up memory.\nPage Attention Traditional LLM inference has inefficient KV cache management. There are three issues:\nreserved slots for future tokens: although the slots are used eventually, the pre-allocation leads to most of time the memory slots are wasted . Think about the end token slot is empty in the whole decoding process. Internal fragmentation due to over-provisioning for potential maximum sequence lengths, External fragmentation from the memory allocator like the buddy allocator. Remember that in OS, each program sees isolated, contiguous logical address (virtual memory) ranging from 0 - N which is dynamically mapped to physical memory (main memory).\nBecause of the indirection between logical and physical addresses, a process‚Äôs pages can be located anywhere in memory and can be moved (relocated). Needed pages are ‚Äòpaged into‚Äô main memory as they are referenced by the executing program.\nDesigned with a similar philosophy, paged attention utilizes logical view and physical view to manage KV cache to minimize fragmentation in GPU memory.\nObviously the block table here is the same with memory management unit which manages the mapping from logical address to physical address. For page attention here logical view is just needed batch KV cache.\nContinuous Batching The main topic for inference is how to handle multiple concurrent requests efficiently. Remember that LLM inference is memory-IO bound, not compute-bound. This means it takes more time to load 1MB of data to the GPU‚Äôs compute cores than it does for those compute cores to perform LLM computations on 1MB of data. Thus LLM inference throughput is largely determined by how large a batch you can fit into high-bandwidth GPU memory.\nThe very obvious one for online serving is to batch processing individual request. The left side of the following figure shows request-level batching. However as is illustrated in the figure, GPU is underutilized as generation lengths vary.\nAs is shown in the right hand side, in continuous batching, LLMs continuously add new requests to the current batch as others finish. This allows it to maximize GPU utilization. In reality, the complexity comes from how to manage the KV cache of requests which are finished. In vLLM, KV cache is managed by pagedattention discussed above which makes things slightly easier.\nChunked Prefill By default, vLLM scheduler prioritizes prefills and doesn‚Äôt batch prefill and decode to the same batch. Chunked prefill [7] allows to chunk large prefills into smaller chunks and batch them together with decode requests.\nAsyncLLM When a request comes in, it goes through a sequence of steps:\ntokenization: tokenizing the prompt, prefilling: loading it into the model cache (prefill), decoding: generating tokens one by one (decode). In a na√Øve synchronous setup, it would process each request‚Äôs entire prefill ‚Üí decode phases before starting the next, leaving the GPU idle whenever that one request is waiting (e.g., on small decode batches) or blocked on I/O.\nBy contrast, an asynchronous engine interleaves these phases across many requests. While Request A is in its decode phase waiting for the next iteration, Request B‚Äôs prefill can run, or Request C‚Äôs decode can proceed. This ‚Äúoverlap‚Äù of lifecycles means the GPU is never sitting idle between operations, because there‚Äôs always some request at the right stage to feed it compute work\nTo utilize the AsyncLLMEngine, we can instantiate it as follows. This simple instantiation allows you to start making asynchronous calls to your models. Here‚Äôs a basic example of how to perform an inference:\nfrom vllm import AsyncLLMEngine import asyncio async_engine = AsyncLLMEngine() async def main(): response = await async_engine.infer(prompt=\"Hello, world!\") print(response) asyncio.run(main()) This code snippet demonstrates how to use the infer method to get predictions from the model asynchronously.\nReferences Efficient Memory Management for Large Language Model Serving with PagedAttention POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference Orca: A Distributed Serving System for Transformer-Based Generative Models https://www.anyscale.com/blog/continuous-batching-llm-inference vLLM slides https://github.com/vllm-project/vllm/pull/12071 SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills ","wordCount":"1252","inLanguage":"en-us","datePublished":"2024-10-05T12:01:14-07:00","dateModified":"2024-10-05T12:01:14-07:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml/llm_inference/vllm/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>vLLM</h1><div class=post-description>VLLM</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-10-05
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1252 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>3 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/inference/ style="color:var(--secondary) !important">Inference</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#vllm-inference-modes aria-label="vLLM Inference Modes">vLLM Inference Modes</a><ul><li><a href=#offline-inference aria-label="Offline Inference">Offline Inference</a></li><li><a href=#api-server aria-label="API Server">API Server</a></li></ul></li><li><a href=#llm-inference aria-label="LLM Inference">LLM Inference</a></li><li><a href=#kv-cache aria-label="KV Cache">KV Cache</a></li><li><a href=#page-attention aria-label="Page Attention">Page Attention</a></li><li><a href=#continuous-batching aria-label="Continuous Batching">Continuous Batching</a></li><li><a href=#chunked-prefill aria-label="Chunked Prefill">Chunked Prefill</a></li><li><a href=#asyncllm aria-label=AsyncLLM>AsyncLLM</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=vllm-inference-modes>vLLM Inference Modes<a hidden class=anchor aria-hidden=true href=#vllm-inference-modes>#</a></h2><p>vLLM has two inference modes:</p><ul><li>offline batch mode: mostly for offline model evaluation, large-scale, high-throughput inference where latency is less critical</li><li>online serving mode: Real-time applications like chatbots or APIs where latency is important.</li></ul><p>The interface to these two approaches are shown in the diagram below.</p><div align=center><img src=images/interface.png style=width:100%;height:auto></div><h3 id=offline-inference>Offline Inference<a hidden class=anchor aria-hidden=true href=#offline-inference>#</a></h3><p>Simpler offline batch inference example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># batch prompts</span>
</span></span><span style=display:flex><span>prompts <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;Hello, my name is&#34;</span>,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#34;The president of the United States is&#34;</span>,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#34;The capital of France is&#34;</span>,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#34;The future of AI is&#34;</span>,]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># sampling parameters</span>
</span></span><span style=display:flex><span>sampling_params <span style=color:#f92672>=</span> SamplingParams(temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, top_p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.95</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># load model</span>
</span></span><span style=display:flex><span>llm <span style=color:#f92672>=</span> LLM(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;facebook/opt-125m&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Inference</span>
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> llm<span style=color:#f92672>.</span>generate(prompts, sampling_params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> output <span style=color:#f92672>in</span> outputs:
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>prompt
</span></span><span style=display:flex><span>    generated_text <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>outputs[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Prompt: </span><span style=color:#e6db74>{</span>prompt<span style=color:#e6db74>!r}</span><span style=color:#e6db74>, Generated text: </span><span style=color:#e6db74>{</span>generated_text<span style=color:#e6db74>!r}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>A data parallel (SPMD style) offline inference example.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> vllm <span style=color:#f92672>import</span> LLM, SamplingParams
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> vllm.utils <span style=color:#f92672>import</span> get_open_port
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>GPUs_per_dp_rank <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>DP_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>(dp_size, dp_rank, dp_master_ip, dp_master_port, GPUs_per_dp_rank):
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;VLLM_DP_RANK&#34;</span>] <span style=color:#f92672>=</span> str(dp_rank)
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;VLLM_DP_SIZE&#34;</span>] <span style=color:#f92672>=</span> str(dp_size)
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;VLLM_DP_MASTER_IP&#34;</span>] <span style=color:#f92672>=</span> dp_master_ip
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;VLLM_DP_MASTER_PORT&#34;</span>] <span style=color:#f92672>=</span> str(dp_master_port)
</span></span><span style=display:flex><span>    <span style=color:#75715e># set devices for each dp_rank</span>
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;CUDA_VISIBLE_DEVICES&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>.</span>join(
</span></span><span style=display:flex><span>        str(i) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(dp_rank <span style=color:#f92672>*</span> GPUs_per_dp_rank, (dp_rank <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>                              GPUs_per_dp_rank))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Sample prompts.</span>
</span></span><span style=display:flex><span>    prompts <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Hello, my name is&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;The president of the United States is&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;The capital of France is&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;The future of AI is&#34;</span>,
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># with DP, each rank should process different prompts.</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># usually all the DP ranks process a full dataset,</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># and each rank processes a different part of the dataset.</span>
</span></span><span style=display:flex><span>    promts_per_rank <span style=color:#f92672>=</span> len(prompts) <span style=color:#f92672>//</span> dp_size
</span></span><span style=display:flex><span>    start <span style=color:#f92672>=</span> dp_rank <span style=color:#f92672>*</span> promts_per_rank
</span></span><span style=display:flex><span>    end <span style=color:#f92672>=</span> start <span style=color:#f92672>+</span> promts_per_rank
</span></span><span style=display:flex><span>    prompts <span style=color:#f92672>=</span> prompts[start:end]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(prompts) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># if any rank has no prompts to process,</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we need to set a placeholder prompt</span>
</span></span><span style=display:flex><span>        prompts <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;Placeholder&#34;</span>]
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;DP rank </span><span style=color:#e6db74>{</span>dp_rank<span style=color:#e6db74>}</span><span style=color:#e6db74> needs to process </span><span style=color:#e6db74>{</span>len(prompts)<span style=color:#e6db74>}</span><span style=color:#e6db74> prompts&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create a sampling params object.</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># since we are doing data parallel, every rank can have different</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># sampling params. here we set different max_tokens for different</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ranks for demonstration.</span>
</span></span><span style=display:flex><span>    sampling_params <span style=color:#f92672>=</span> SamplingParams(temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>,
</span></span><span style=display:flex><span>                                     top_p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.95</span>,
</span></span><span style=display:flex><span>                                     max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span> <span style=color:#f92672>*</span> (dp_rank <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create an LLM.</span>
</span></span><span style=display:flex><span>    llm <span style=color:#f92672>=</span> LLM(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ibm-research/PowerMoE-3b&#34;</span>,
</span></span><span style=display:flex><span>              tensor_parallel_size<span style=color:#f92672>=</span>GPUs_per_dp_rank,
</span></span><span style=display:flex><span>              enforce_eager<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>              enable_expert_parallel<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> llm<span style=color:#f92672>.</span>generate(prompts, sampling_params)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Print the outputs.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> output <span style=color:#f92672>in</span> outputs:
</span></span><span style=display:flex><span>        prompt <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>prompt
</span></span><span style=display:flex><span>        generated_text <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>outputs[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;DP rank </span><span style=color:#e6db74>{</span>dp_rank<span style=color:#e6db74>}</span><span style=color:#e6db74>, Prompt: </span><span style=color:#e6db74>{</span>prompt<span style=color:#e6db74>!r}</span><span style=color:#e6db74>, &#34;</span>
</span></span><span style=display:flex><span>              <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Generated text: </span><span style=color:#e6db74>{</span>generated_text<span style=color:#e6db74>!r}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> multiprocessing <span style=color:#f92672>import</span> Process
</span></span><span style=display:flex><span>    dp_master_ip <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;127.0.0.1&#34;</span>
</span></span><span style=display:flex><span>    dp_master_port <span style=color:#f92672>=</span> get_open_port()
</span></span><span style=display:flex><span>    procs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(DP_size):
</span></span><span style=display:flex><span>        proc <span style=color:#f92672>=</span> Process(target<span style=color:#f92672>=</span>main,
</span></span><span style=display:flex><span>                       args<span style=color:#f92672>=</span>(DP_size, i, dp_master_ip, dp_master_port,
</span></span><span style=display:flex><span>                             GPUs_per_dp_rank))
</span></span><span style=display:flex><span>        proc<span style=color:#f92672>.</span>start()
</span></span><span style=display:flex><span>        procs<span style=color:#f92672>.</span>append(proc)
</span></span><span style=display:flex><span>    exit_code <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> proc <span style=color:#f92672>in</span> procs:
</span></span><span style=display:flex><span>        proc<span style=color:#f92672>.</span>join()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> proc<span style=color:#f92672>.</span>exitcode:
</span></span><span style=display:flex><span>            exit_code <span style=color:#f92672>=</span> proc<span style=color:#f92672>.</span>exitcode
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    exit(exit_code)
</span></span></code></pre></div><h3 id=api-server>API Server<a hidden class=anchor aria-hidden=true href=#api-server>#</a></h3><p>The API server utilizes Uvicorn to deploy FastAPI application.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># Server</span>
</span></span><span style=display:flex><span>python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-7b-hf
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ClientÔºö(compatible with openai)</span>
</span></span><span style=display:flex><span>curl http://localhost:8000/v1/completions <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -d <span style=color:#e6db74>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;model&#34;: &#34;meta-llama/Llama-2-7b-hf&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;prompt&#34;: &#34;San Francisco is a&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;max_tokens&#34;: 256,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;temperature&#34;: 0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    }&#39;</span>
</span></span></code></pre></div><h2 id=llm-inference>LLM Inference<a hidden class=anchor aria-hidden=true href=#llm-inference>#</a></h2><p>No matter what kind mode vLLM is in for inference, the LLM inference process has two stages:</p><ul><li>prefilling</li><li>decoding</li></ul><p>Prefilling phase llm encodes the prompt at once. This is one forward path execution in LLM.
Decoding phase, llm generates each token step by step.</p><h2 id=kv-cache>KV Cache<a hidden class=anchor aria-hidden=true href=#kv-cache>#</a></h2><p>KV Cache at inference time refers that caching key and value vectors in self-attention saves redundant computation and accelerates decoding - but takes up memory.</p><h2 id=page-attention>Page Attention<a hidden class=anchor aria-hidden=true href=#page-attention>#</a></h2><p>Traditional LLM inference has inefficient KV cache management. There are three issues:</p><ul><li>reserved slots for future tokens: although the slots are used eventually, the pre-allocation leads to most of time the memory slots are wasted . Think about the end token slot is empty in the whole decoding process.</li><li>Internal fragmentation due to over-provisioning for potential maximum sequence lengths,</li><li>External fragmentation from the memory allocator like the buddy allocator.</li></ul><p>Remember that in OS, each program sees isolated, contiguous logical address (virtual memory) ranging from 0 - N which is dynamically mapped to physical memory (main memory).</p><div align=center><img src=images/mem.png style=width:80%;height:auto></div><p>Because of the indirection between logical and physical addresses, a process‚Äôs pages can be located anywhere in memory and can be moved (relocated). Needed pages are ‚Äòpaged into‚Äô main memory as they are referenced by the executing program.</p><p>Designed with a similar philosophy, paged attention utilizes logical view and physical view to manage KV cache to minimize fragmentation in GPU memory.</p><div align=center><img src=images/pagedattn.png style=width:80%;height:auto></div><p>Obviously the block table here is the same with memory management unit which manages the mapping from logical address to physical address. For page attention here logical view is just needed batch KV cache.</p><h2 id=continuous-batching>Continuous Batching<a hidden class=anchor aria-hidden=true href=#continuous-batching>#</a></h2><p>The main topic for inference is how to handle multiple concurrent requests efficiently. Remember that LLM inference is memory-IO bound, not compute-bound. This means it takes more time to load 1MB of data to the GPU‚Äôs compute cores than it does for those compute cores to perform LLM computations on 1MB of data. Thus LLM inference throughput is largely determined by how large a batch you can fit into high-bandwidth GPU memory.</p><p>The very obvious one for online serving is to batch processing individual request. The left side of the following figure shows request-level batching. However as is illustrated in the figure, GPU is underutilized as generation lengths vary.</p><p>As is shown in the right hand side, in continuous batching, LLMs continuously add new requests to the current batch as others finish. This allows it to maximize GPU utilization. In reality, the complexity comes from how to manage the KV cache of requests which are finished. In vLLM, KV cache is managed by pagedattention discussed above which makes things slightly easier.</p><div align=center><img src=images/batching.png style=width:80%;height:auto></div><h2 id=chunked-prefill>Chunked Prefill<a hidden class=anchor aria-hidden=true href=#chunked-prefill>#</a></h2><p>By default, vLLM scheduler prioritizes prefills and doesn‚Äôt batch prefill and decode to the same batch. Chunked prefill [7] allows to chunk large prefills into smaller chunks and batch them together with decode requests.</p><h2 id=asyncllm>AsyncLLM<a hidden class=anchor aria-hidden=true href=#asyncllm>#</a></h2><p>When a request comes in, it goes through a sequence of steps:</p><ol><li>tokenization: tokenizing the prompt,</li><li>prefilling: loading it into the model cache (prefill),</li><li>decoding: generating tokens one by one (decode).</li></ol><p>In a na√Øve synchronous setup, it would process each request‚Äôs entire prefill ‚Üí decode phases before starting the next, leaving the GPU idle whenever that one request is waiting (e.g., on small decode batches) or blocked on I/O.</p><p>By contrast, an asynchronous engine interleaves these phases across many requests. While Request A is in its decode phase waiting for the next iteration, Request B‚Äôs prefill can run, or Request C‚Äôs decode can proceed. This ‚Äúoverlap‚Äù of lifecycles means the GPU is never sitting idle between operations, because there‚Äôs always some request at the right stage to feed it compute work</p><p>To utilize the AsyncLLMEngine, we can instantiate it as follows. This simple instantiation allows you to start making asynchronous calls to your models. Here‚Äôs a basic example of how to perform an inference:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> vllm <span style=color:#f92672>import</span> AsyncLLMEngine
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> asyncio
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>async_engine <span style=color:#f92672>=</span> AsyncLLMEngine()
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    response <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> async_engine<span style=color:#f92672>.</span>infer(prompt<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Hello, world!&#34;</span>)
</span></span><span style=display:flex><span>    print(response)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>asyncio<span style=color:#f92672>.</span>run(main())
</span></span></code></pre></div><p>This code snippet demonstrates how to use the infer method to get predictions from the model asynchronously.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://arxiv.org/abs/2309.06180>Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li><li>POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference</li><li>Orca: A Distributed Serving System for Transformer-Based Generative Models</li><li><a href=https://www.anyscale.com/blog/continuous-batching-llm-inference>https://www.anyscale.com/blog/continuous-batching-llm-inference</a></li><li><a href=https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/>vLLM slides</a></li><li><a href=https://github.com/vllm-project/vllm/pull/12071>https://github.com/vllm-project/vllm/pull/12071</a></li><li>SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills</li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/llm_inference/entropy_decoding/><span class=title>¬´</span><br><span>Entropy Collapsing in RL Training</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/programming_language/python/asyncio/><span class=title>¬ª</span><br><span>AsyncIO</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>