<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Triton, Cuda and GPU | Jun's Blog</title><meta name=keywords content><meta name=description content="Cuda programming is all about multi-threading. Multiple threads create a thread block. Thread block is executed by the stream multiprocessor (SM, the SM-0, SM-1 etc shown below). Threads within the same block has a shared memory which is the shared memory below. Note that L1 cache and shared memory are different. The main difference between shared memory and the L1 is that the contents of shared memory are managed by user code explicitly, whereas the L1 cache is automatically managed. Shared memory is also a better way to exchange data between threads in a block with predictable timing.
Whenever shared memory needs to fetch data from global memory, it first checks whether the data is already in the L2 cache. If it is, then there&rsquo;s no need to access global memory. The higher the probability of a cache hit, the higher the so-called memory hit rate. One of the goals in CUDA programming is to maximize this hit rate as much as possible."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/triton/><link crossorigin=anonymous href=../../../../../assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../../../assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/triton/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="Triton, Cuda and GPU"><meta property="og:description" content="Cuda programming is all about multi-threading. Multiple threads create a thread block. Thread block is executed by the stream multiprocessor (SM, the SM-0, SM-1 etc shown below). Threads within the same block has a shared memory which is the shared memory below. Note that L1 cache and shared memory are different. The main difference between shared memory and the L1 is that the contents of shared memory are managed by user code explicitly, whereas the L1 cache is automatically managed. Shared memory is also a better way to exchange data between threads in a block with predictable timing.
Whenever shared memory needs to fetch data from global memory, it first checks whether the data is already in the L2 cache. If it is, then there&rsquo;s no need to access global memory. The higher the probability of a cache hit, the higher the so-called memory hit rate. One of the goals in CUDA programming is to maximize this hit rate as much as possible."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml_infra/triton/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-05T00:18:23+08:00"><meta property="article:modified_time" content="2024-05-05T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Triton, Cuda and GPU"><meta name=twitter:description content="Cuda programming is all about multi-threading. Multiple threads create a thread block. Thread block is executed by the stream multiprocessor (SM, the SM-0, SM-1 etc shown below). Threads within the same block has a shared memory which is the shared memory below. Note that L1 cache and shared memory are different. The main difference between shared memory and the L1 is that the contents of shared memory are managed by user code explicitly, whereas the L1 cache is automatically managed. Shared memory is also a better way to exchange data between threads in a block with predictable timing.
Whenever shared memory needs to fetch data from global memory, it first checks whether the data is already in the L2 cache. If it is, then there&rsquo;s no need to access global memory. The higher the probability of a cache hit, the higher the so-called memory hit rate. One of the goals in CUDA programming is to maximize this hit rate as much as possible."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Triton, Cuda and GPU","item":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/triton/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Triton, Cuda and GPU","name":"Triton, Cuda and GPU","description":"Cuda programming is all about multi-threading. Multiple threads create a thread block. Thread block is executed by the stream multiprocessor (SM, the SM-0, SM-1 etc shown below). Threads within the same block has a shared memory which is the shared memory below. Note that L1 cache and shared memory are different. The main difference between shared memory and the L1 is that the contents of shared memory are managed by user code explicitly, whereas the L1 cache is automatically managed. Shared memory is also a better way to exchange data between threads in a block with predictable timing.\nWhenever shared memory needs to fetch data from global memory, it first checks whether the data is already in the L2 cache. If it is, then there\u0026rsquo;s no need to access global memory. The higher the probability of a cache hit, the higher the so-called memory hit rate. One of the goals in CUDA programming is to maximize this hit rate as much as possible.\n","keywords":[],"articleBody":"Cuda programming is all about multi-threading. Multiple threads create a thread block. Thread block is executed by the stream multiprocessor (SM, the SM-0, SM-1 etc shown below). Threads within the same block has a shared memory which is the shared memory below. Note that L1 cache and shared memory are different. The main difference between shared memory and the L1 is that the contents of shared memory are managed by user code explicitly, whereas the L1 cache is automatically managed. Shared memory is also a better way to exchange data between threads in a block with predictable timing.\nWhenever shared memory needs to fetch data from global memory, it first checks whether the data is already in the L2 cache. If it is, then there‚Äôs no need to access global memory. The higher the probability of a cache hit, the higher the so-called memory hit rate. One of the goals in CUDA programming is to maximize this hit rate as much as possible.\nAn SM contains multiple subcores, and each subcore has a warp scheduler and dispatcher capable of handling 32 threads.\nFigure 1. image from Nvidia Registers‚ÄîThese are private to each thread, which means that registers assigned to a thread are not visible to other threads. The compiler makes decisions about register utilization. L1/Shared memory (SMEM)‚ÄîEvery SM has a fast, on-chip scratchpad memory that can be used as L1 cache and shared memory. All threads in a CUDA block can share shared memory, and all CUDA blocks running on a given SM can share the physical memory resource provided by the SM. L1 cache is used by system, and Read-only memory ‚ÄîEach SM has an instruction cache, constant memory, texture memory and RO cache, which is read-only to kernel code. L2 cache‚ÄîThe L2 cache is shared across all SMs, so every thread in every CUDA block can access this memory. The NVIDIA A100 GPU has increased the L2 cache size to 40 MB as compared to 6 MB in V100 GPUs. Global memory‚ÄîThis is the framebuffer size of the GPU and DRAM sitting in the GPU Cuda Programming When we talk about cuda GPU parallel computing, we are actually referring to a heterogeneous computing architecture based on both CPU and GPU. In this architecture, the GPU and CPU are connected via a PCIe bus to work together collaboratively. The CPU side is referred to as the host, while the GPU side is referred to as the device.\nThe CUDA programming requires cooperation between the CPU and GPU. In CUDA, the term host refers to the CPU and its memory, while device refers to the GPU and its memory. A CUDA program includes both host code and device code, which run on the CPU and GPU respectively. Additionally, the host and device can communicate with each other, allowing data to be transferred between them.\nThe typical execution flow of a CUDA program is as follows:\nAllocate host memory and initialize the data; Allocate device memory and copy data from the host to the device; Launch a CUDA kernel to perform computations on the device; Copy the results from the device back to the host; Free the memory allocated on both the device and the host. CUDA organizes threads in three levels:\nThread ‚Äì The smallest execution unit. Each has an ID given by threadIdx. Block ‚Äì A group of threads. Each block has an ID given by blockIdx. Grid ‚Äì A collection of blocks. The grid‚Äôs size (number of blocks) is described by gridDim. Correspondingly, we can\nUse threadIdx for thread position inside a block. Use blockIdx for block position inside a grid. Use blockDim for number of threads per block Use gridDim for number of blocks exist, so we can compute global indices or partition work. A kernel is a function that runs in parallel across multiple threads on the device (GPU). Kernel functions are declared using the global qualifier, and when calling a kernel, the syntax \u003c\u003c\u003e\u003e is used to specify the number of threads to execute. The nvcc compiler recognizes this modifier and splits the code into two parts, sending them to the CPU and GPU compilers respectively for compilation.\nkernel_func\u003c\u003c\u003cgrid_size, block_size, shared_memory_size, stream\u003e\u003e\u003e(params_list) // grid_size: threads blocks to launch by the kernel // block_size: number of threads in each block // shared_memory_size: required shared_memory_size, optional // stream: the steam to execute the kernel, optional The ¬´ syntax following the kernel function call is a special CUDA syntax. These grid_size and block_size represent the number of thread blocks and the number of threads per block used during the kernel function execution. Since each thread block and each thread operate in parallel, this allocation determines the degree of parallelism in the program.\nAll the threads launched by a kernel are collectively called a grid. Threads within the same grid share the same global memory space.A grid can be divided into multiple thread blocks (blocks), and each block contains many threads.\nBelow both the grid and the block are 2-dimensional. Both grid and block are defined as variables of type dim3. The dim3 type can be thought of as a struct containing three unsigned integer members: x, y, and z, which are initialized to 1 by default. Therefore, grid and block can be flexibly defined as 1-dimensional, 2-dimensional, or 3-dimensional structures. When calling the kernel, the execution configuration \u003c\u003c\u003e\u003e must be used to specify the number and structure of threads that the kernel will use.\nA thread requires two built-in coordinate variables (blockIdx and threadIdx) to be uniquely identified. Both are variables of type dim3.\nFigure 2. kernel 2-dim structure In the above diagram, the grid and block can be defined as follows\ndim3 grid(3, 2); dim3 block(5, 3); kernel_func\u003c\u003c\u003c grid, block \u003e\u003e\u003e(params...); In the follow diagram, the grid and block is deined as\nkernel_func\u003c\u003c\u003c4,8\u003e\u003e\u003e(params...); // we can also define one block or 32 blocks kernel_func\u003c\u003c\u003c1, 32\u003e\u003e\u003e(params...); kernel_func\u003c\u003c\u003c32, 1\u003e\u003e\u003e(params...); // people define single thread kernel to debug kernel_func\u003c\u003c\u003c1, 1\u003e\u003e\u003e(params...); In CUDA, every thread executes a kernel function, and each thread is assigned a unique thread ID. This thread ID can be accessed within the kernel using the built-in variable threadIdx.\n#include __global__ void kernel(int a, int b, int *c){ *c = a + b; } int main(){ int c = 20; int *c_cuda; cudaMalloc((void**)\u0026c_cuda,sizeof(int)); kernel\u003c\u003c\u003c1,1\u003e\u003e\u003e(1,1,c_cuda); cudaMemcpy(\u0026c,c_cuda,sizeof(int),cudaMemcpyDeviceToHost); printf(\"c=%d\\n\",c); cudaFree(c_cuda); return 0; } The CPU can pass c_cuda as a parameter and perform type conversions, but it absolutely cannot read from or write to c_cuda, because this variable was allocated using cudaMalloc and therefore resides in GPU memory, not system memory. Similarly, the GPU cannot access the variable c. The bridge between the two is the cudaMemcpy function, which transfers values back and forth over the data bus. This essentially forms a logical structure where the CPU is responsible for sending and receiving data, while the GPU handles the computation.\nIn summary, for cuda we have two kinds of model, one is programming model and the other is hardware model. In programming model, we have\nKernel -\u003e multiple threads -\u003e blocks -\u003e grids\nHardware model\nMultiple register -\u003e (owned by a) thread -\u003e (32 of threads) warp -\u003e (SM has multiple warps) SM\nGPU-related Performance Roofline GPU system performance is constrained by three primary components:\nMemory Bandwidth Mainly the speed of transferring data from gpu HBM to on-chip SM. For memory-bound operations, the strategy is to fuse them into a single kernel to eliminate intermediate memory traffic.\nCompute Time spent on your GPU computing actual floating point operations. For a single, complex operation with high potential arithmetic intensity (like matrix multiplication), the strategy is to use tiling to maximize data reuse within the SM‚Äôs fast memory.\nCPU Scheduling Overhead System performance may also be constrained by host-side overhead‚Äîtime spent by the CPU (host) on tasks such as preparing and dispatching work (kernels) to the GPU. This could happen when host launches multiple GPU kernels which are either too small or too numerous. In such cases, the GPU completes each kernel execution rapidly but then remains idle while waiting for the CPU to issue subsequent commands. As a result, overall runtime becomes dominated by the CPU‚Äôs limited ability to keep the GPU consistently fed with work.\nOne solution to scheduling overhead is async execution. For instance, SGLang proposed to optimize CPU overhead using async ops.\nSGLang solution to CPU overhead Fundamentally, this is designed to offer flexibility in designing kernels/tasks at each step. We can balance the tradeoff using jit.trace, jax.jit or torch.compile(). We can even go to lower level by manipulate the cuda graph. CUDA Graphs is designed to allow work to be defined as graphs rather than single operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduce CPU overheads.\nA kernel‚Äôs performance is limited by either its memory bandwidth or its compute throughput. These two limits define the performance regions.\nArithmetic Intensity (AI) is the formal metric that determines the region. It is the ratio of computation to memory traffic.\nArithmetic Intensity = Total Computation FLOPs / Total Bytes Accessed\nFor the Roofline model, Total Bytes Accessed specifically counts the data transferred between Global Memory (HBM) and the on-chip SM. This is because the model evaluates a kernel‚Äôs performance against the primary bottleneck: the slow off-chip memory bus. On-chip traffic, such as from Shared Memory to registers, is not included in this calculation.\nTo visualize the tradeoff between memory and compute, people are using a roofline plot, which plots the peak achievable FLOPs/s (throughput) of an algorithm on our hardware (the y-axis) against the arithmetic intensity of that algorithm (the x-axis).\nimage from [4] Cuda Graph In CUDA 10, NVIDIA introduced CUDA Graphs, a feature that allows developers to capture a sequence of GPU operations‚Äîincluding kernel launches, memory copies, and other device activities‚Äîinto a graph structure that can be instantiated and launched with minimal CPU overhead. This approach is particularly beneficial in workloads where the same sequence of operations is executed repeatedly, as it avoids the overhead of repeatedly issuing individual operations from the CPU.\nA CUDA graph represents a recorded sequence of GPU operations and their dependencies. When a graph is instantiated (i.e., converted into an executable graph), it encodes the exact set of operations and the arguments used during recording‚Äîincluding memory addresses and kernel parameters. Therefore, all pointers and arguments must remain valid and consistent across launches of the graph executable. If kernel arguments or memory locations change between executions, the graph must be updated or re-recorded.\nBy reducing CPU launch overhead and enabling more efficient scheduling on the GPU, CUDA Graphs can significantly improve performance in scenarios with fixed execution patterns, such as deep learning training loops, simulation time steps, or repeated signal processing stages.\nCommon Libs Cuda: Library to use GPUs. CuTLASS: CUDA GEMM lib. CuBLAS: cuda basic linear algebra lib. CuDNN: Library to do Neural Net stuff on GPUs (probably uses cuda to talk to the GPUs) image from [1] References https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/ https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model https://zhuanlan.zhihu.com/p/34587739 https://jax-ml.github.io/scaling-book/roofline/ https://damek.github.io/random/basic-facts-about-gpus/ Making Deep Learning Go Brrrr From First Principles Getting Started with CUDA Graphs https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/ ","wordCount":"1983","inLanguage":"en-us","datePublished":"2024-05-05T00:18:23+08:00","dateModified":"2024-05-05T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/triton/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>Triton, Cuda and GPU</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-05-05
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1983 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>4 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#cuda-programming aria-label="Cuda Programming">Cuda Programming</a></li><li><a href=#gpu-related-performance-roofline aria-label="GPU-related Performance Roofline">GPU-related Performance Roofline</a><ul><li><a href=#cuda-graph aria-label="Cuda Graph">Cuda Graph</a></li><li><a href=#common-libs aria-label="Common Libs">Common Libs</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Cuda programming is all about multi-threading. Multiple threads create a thread block. Thread block is executed by the stream multiprocessor (SM, the SM-0, SM-1 etc shown below). Threads within the same block has a shared memory which is the shared memory below. Note that L1 cache and shared memory are different. The main difference between shared memory and the L1 is that the contents of shared memory are managed by user code explicitly, whereas the L1 cache is automatically managed. Shared memory is also a better way to exchange data between threads in a block with predictable timing.</p><p>Whenever shared memory needs to fetch data from global memory, it first checks whether the data is already in the L2 cache. If it is, then there&rsquo;s no need to access global memory. The higher the probability of a cache hit, the higher the so-called memory hit rate. One of the goals in CUDA programming is to maximize this hit rate as much as possible.</p><p>An SM contains multiple subcores, and each subcore has a warp scheduler and dispatcher capable of handling 32 threads.</p><div align=center><img src=images/gpu_mem_hierarchy.png style=width:100%;height:auto> Figure 1. image from Nvidia</div><ul><li>Registers‚ÄîThese are private to each thread, which means that registers assigned to a thread are not visible to other threads. The compiler makes decisions about register utilization.</li><li>L1/Shared memory (SMEM)‚ÄîEvery SM has a fast, on-chip scratchpad memory that can be used as L1 cache and shared memory. All threads in a CUDA block can share shared memory, and all CUDA blocks running on a given SM can share the physical memory resource provided by the SM. L1 cache is used by system, and</li><li>Read-only memory ‚ÄîEach SM has an instruction cache, constant memory, texture memory and RO cache, which is read-only to kernel code.</li><li>L2 cache‚ÄîThe L2 cache is shared across all SMs, so every thread in every CUDA block can access this memory. The NVIDIA A100 GPU has increased the L2 cache size to 40 MB as compared to 6 MB in V100 GPUs.</li><li>Global memory‚ÄîThis is the framebuffer size of the GPU and DRAM sitting in the GPU</li></ul><h2 id=cuda-programming>Cuda Programming<a hidden class=anchor aria-hidden=true href=#cuda-programming>#</a></h2><p>When we talk about cuda GPU parallel computing, we are actually referring to a heterogeneous computing architecture based on both CPU and GPU. In this architecture, the GPU and CPU are connected via a PCIe bus to work together collaboratively. The CPU side is referred to as the <strong>host</strong>, while the GPU side is referred to as the <strong>device</strong>.</p><p>The CUDA programming requires cooperation between the CPU and GPU. In CUDA, the term host refers to the CPU and its memory, while device refers to the GPU and its memory. A CUDA program includes both host code and device code, which run on the CPU and GPU respectively. Additionally, the host and device can communicate with each other, allowing data to be transferred between them.</p><p>The typical execution flow of a CUDA program is as follows:</p><ul><li>Allocate host memory and initialize the data;</li><li>Allocate device memory and copy data from the host to the device;</li><li>Launch a CUDA kernel to perform computations on the device;</li><li>Copy the results from the device back to the host;</li><li>Free the memory allocated on both the device and the host.</li></ul><p>CUDA organizes threads in three levels:</p><ul><li>Thread ‚Äì The smallest execution unit. Each has an ID given by threadIdx.</li><li>Block ‚Äì A group of threads. Each block has an ID given by blockIdx.</li><li>Grid ‚Äì A collection of blocks. The grid‚Äôs size (number of blocks) is described by gridDim.</li></ul><p>Correspondingly, we can</p><ul><li>Use threadIdx for thread position inside a block.</li><li>Use blockIdx for block position inside a grid.</li><li>Use blockDim for number of threads per block</li><li>Use gridDim for number of blocks exist, so we can compute global indices or partition work.</li></ul><p>A kernel is a function that runs in parallel across multiple threads on the device (GPU). Kernel functions are declared using the <strong>global</strong> qualifier, and when calling a kernel, the syntax &lt;&lt;&lt;grid, block>>> is used to specify the number of threads to execute. The nvcc compiler recognizes this modifier and splits the code into two parts, sending them to the CPU and GPU compilers respectively for compilation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span>kernel_func<span style=color:#f92672>&lt;&lt;&lt;</span>grid_size, block_size, shared_memory_size, stream<span style=color:#f92672>&gt;&gt;&gt;</span>(params_list)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// grid_size: threads blocks to launch by the kernel
</span></span></span><span style=display:flex><span><span style=color:#75715e>// block_size: number of threads in each block
</span></span></span><span style=display:flex><span><span style=color:#75715e>// shared_memory_size: required shared_memory_size, optional
</span></span></span><span style=display:flex><span><span style=color:#75715e>// stream: the steam to execute the kernel, optional
</span></span></span></code></pre></div><p>The &#171;&lt;a, b&#187;> syntax following the kernel function call is a special CUDA syntax. These grid_size and block_size represent the number of thread blocks and the number of threads per block used during the kernel function execution. Since each thread block and each thread operate in parallel, this allocation determines the degree of parallelism in the program.</p><p>All the threads launched by a kernel are collectively called a <strong>grid</strong>. Threads within the same grid share the same global memory space.A grid can be divided into multiple thread blocks (blocks), and each block contains many threads.</p><p>Below both the grid and the block are 2-dimensional. Both grid and block are defined as variables of type <code>dim3</code>. The <code>dim3</code> type can be thought of as a struct containing three unsigned integer members: <code>x</code>, <code>y</code>, and <code>z</code>, which are initialized to 1 by default. Therefore, grid and block can be flexibly defined as 1-dimensional, 2-dimensional, or 3-dimensional structures. When calling the kernel, the execution configuration <code>&lt;&lt;&lt;grid, block>>></code> must be used to specify the number and structure of threads that the kernel will use.</p><p>A thread requires two built-in coordinate variables (<code>blockIdx</code> and <code>threadIdx</code>) to be uniquely identified. Both are variables of type <code>dim3</code>.</p><div align=center><img src=images/kernel.png style=width:80%;height:auto> Figure 2. kernel 2-dim structure</div><p>In the above diagram, the grid and block can be defined as follows</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span>dim3 <span style=color:#a6e22e>grid</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>);
</span></span><span style=display:flex><span>dim3 <span style=color:#a6e22e>block</span>(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>);
</span></span><span style=display:flex><span>kernel_func<span style=color:#f92672>&lt;&lt;&lt;</span> grid, block <span style=color:#f92672>&gt;&gt;&gt;</span>(params...);
</span></span></code></pre></div><p>In the follow diagram, the grid and block is deined as</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span>kernel_func<span style=color:#f92672>&lt;&lt;&lt;</span><span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>8</span><span style=color:#f92672>&gt;&gt;&gt;</span>(params...);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// we can also define one block or 32 blocks
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>kernel_func<span style=color:#f92672>&lt;&lt;&lt;</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>32</span><span style=color:#f92672>&gt;&gt;&gt;</span>(params...);
</span></span><span style=display:flex><span>kernel_func<span style=color:#f92672>&lt;&lt;&lt;</span><span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>1</span><span style=color:#f92672>&gt;&gt;&gt;</span>(params...);
</span></span><span style=display:flex><span><span style=color:#75715e>// people define single thread kernel to debug
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>kernel_func<span style=color:#f92672>&lt;&lt;&lt;</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span><span style=color:#f92672>&gt;&gt;&gt;</span>(params...);
</span></span></code></pre></div><p>In CUDA, every thread executes a kernel function, and each thread is assigned a unique thread ID. This thread ID can be accessed within the kernel using the built-in variable threadIdx.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#75715e>#include</span><span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>__global__ <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>kernel</span>(<span style=color:#66d9ef>int</span> a, <span style=color:#66d9ef>int</span> b, <span style=color:#66d9ef>int</span> <span style=color:#f92672>*</span>c){
</span></span><span style=display:flex><span>	<span style=color:#f92672>*</span>c <span style=color:#f92672>=</span> a <span style=color:#f92672>+</span> b;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>(){
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> c <span style=color:#f92672>=</span> <span style=color:#ae81ff>20</span>;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> <span style=color:#f92672>*</span>c_cuda;
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>cudaMalloc</span>((<span style=color:#66d9ef>void</span><span style=color:#f92672>**</span>)<span style=color:#f92672>&amp;</span>c_cuda,<span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>int</span>));
</span></span><span style=display:flex><span>	kernel<span style=color:#f92672>&lt;&lt;&lt;</span><span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span><span style=color:#f92672>&gt;&gt;&gt;</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>,c_cuda);
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>cudaMemcpy</span>(<span style=color:#f92672>&amp;</span>c,c_cuda,<span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>int</span>),cudaMemcpyDeviceToHost);
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>printf</span>(<span style=color:#e6db74>&#34;c=%d</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>,c);
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>cudaFree</span>(c_cuda);
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The CPU can pass <code>c_cuda</code> as a parameter and perform type conversions, but it absolutely cannot read from or write to <code>c_cuda</code>, because this variable was allocated using <code>cudaMalloc</code> and therefore resides in GPU memory, not system memory. Similarly, the GPU cannot access the variable <code>c</code>. The bridge between the two is the <code>cudaMemcpy</code> function, which transfers values back and forth over the data bus. This essentially forms a logical structure where the CPU is responsible for sending and receiving data, while the GPU handles the computation.</p><p>In summary, for cuda we have two kinds of model, one is programming model and the other is hardware model. In programming model,
we have</p><blockquote><p>Kernel -> multiple threads -> blocks -> grids</p></blockquote><p>Hardware model</p><blockquote><p>Multiple register -> (owned by a) thread -> (32 of threads) warp -> (SM has multiple warps) SM</p></blockquote><h2 id=gpu-related-performance-roofline>GPU-related Performance Roofline<a hidden class=anchor aria-hidden=true href=#gpu-related-performance-roofline>#</a></h2><p>GPU system performance is constrained by three primary components:</p><ul><li>Memory Bandwidth</li></ul><p>Mainly the speed of transferring data from gpu HBM to on-chip SM. For memory-bound operations, the strategy is to fuse them into a single kernel to eliminate intermediate memory traffic.</p><ul><li>Compute</li></ul><p>Time spent on your GPU computing actual floating point operations. For a single, complex operation with high potential arithmetic intensity (like matrix multiplication), the strategy is to use tiling to maximize data reuse within the SM‚Äôs fast memory.</p><ul><li>CPU Scheduling Overhead</li></ul><p>System performance may also be constrained by host-side overhead‚Äîtime spent by the CPU (host) on tasks such as preparing and dispatching work (kernels) to the GPU. This could happen when host launches multiple GPU kernels which are either too small or too numerous. In such cases, the GPU completes each kernel execution rapidly but then remains idle while waiting for the CPU to issue subsequent commands. As a result, overall runtime becomes dominated by the CPU‚Äôs limited ability to keep the GPU consistently fed with work.</p><p>One solution to scheduling overhead is async execution. For instance, SGLang proposed to optimize CPU overhead using async ops.</p><div align=center><img src=images/sglang.png style=width:90%;height:auto> SGLang solution to CPU overhead</div><p>Fundamentally, this is designed to offer flexibility in designing kernels/tasks at each step. We can balance the tradeoff using <code>jit.trace</code>, <code>jax.jit</code> or <code>torch.compile()</code>. We can even go to lower level by manipulate the cuda graph. CUDA Graphs is designed to allow work to be defined as graphs rather than single operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduce CPU overheads.</p><p>A kernel‚Äôs performance is limited by either its <strong>memory bandwidth</strong> or its <strong>compute throughput</strong>. These two limits define the performance regions.</p><p>Arithmetic Intensity (AI) is the formal metric that determines the region. It is the ratio of computation to memory traffic.</p><blockquote><p><code>Arithmetic Intensity = Total Computation FLOPs / Total Bytes Accessed</code></p></blockquote><p>For the Roofline model, Total Bytes Accessed specifically counts the data transferred between Global Memory (HBM) and the on-chip SM. This is because the model evaluates a kernel‚Äôs performance against the primary bottleneck: the slow off-chip memory bus. On-chip traffic, such as from Shared Memory to registers, is not included in this calculation.</p><p>To visualize the tradeoff between memory and compute, people are using a roofline plot, which plots the peak achievable FLOPs/s (throughput) of an algorithm on our hardware (the y-axis) against the arithmetic intensity of that algorithm (the x-axis).</p><div align=center><img src=images/roofline.png style=width:70%;height:auto> image from [4]</div><h3 id=cuda-graph>Cuda Graph<a hidden class=anchor aria-hidden=true href=#cuda-graph>#</a></h3><p>In CUDA 10, NVIDIA introduced CUDA Graphs, a feature that allows developers to capture a sequence of GPU operations‚Äîincluding kernel launches, memory copies, and other device activities‚Äîinto a graph structure that can be instantiated and launched with minimal CPU overhead. This approach is particularly beneficial in workloads where the same sequence of operations is executed repeatedly, as it avoids the overhead of repeatedly issuing individual operations from the CPU.</p><p>A CUDA graph represents a recorded sequence of GPU operations and their dependencies. When a graph is instantiated (i.e., converted into an executable graph), it encodes the exact set of operations and the arguments used during recording‚Äîincluding memory addresses and kernel parameters. Therefore, all pointers and arguments must remain valid and consistent across launches of the graph executable. If kernel arguments or memory locations change between executions, the graph must be updated or re-recorded.</p><p>By reducing CPU launch overhead and enabling more efficient scheduling on the GPU, CUDA Graphs can significantly improve performance in scenarios with fixed execution patterns, such as deep learning training loops, simulation time steps, or repeated signal processing stages.</p><h3 id=common-libs>Common Libs<a hidden class=anchor aria-hidden=true href=#common-libs>#</a></h3><ul><li>Cuda: Library to use GPUs.</li><li>CuTLASS: CUDA GEMM lib.</li><li>CuBLAS: cuda basic linear algebra lib.</li><li>CuDNN: Library to do Neural Net stuff on GPUs (probably uses cuda to talk to the GPUs)</li></ul><div align=center><img src=images/gemm_cuda.png style=width:100%;height:auto> image from [1]</div><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/>https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/</a></li><li><a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model>https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model</a></li><li><a href=https://zhuanlan.zhihu.com/p/34587739>https://zhuanlan.zhihu.com/p/34587739</a></li><li><a href=https://jax-ml.github.io/scaling-book/roofline/>https://jax-ml.github.io/scaling-book/roofline/</a></li><li><a href=https://damek.github.io/random/basic-facts-about-gpus/>https://damek.github.io/random/basic-facts-about-gpus/</a></li><li><a href=https://horace.io/brrr_intro.html>Making Deep Learning Go Brrrr From First Principles</a></li><li><a href=https://developer.nvidia.com/blog/cuda-graphs/>Getting Started with CUDA Graphs</a></li><li><a href=https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/>https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vqvae/><span class=title>¬´</span><br><span>VQ-VAE</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/programming_language/python/pipe/><span class=title>¬ª</span><br><span>Pipe in Multiprocessing</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Triton, Cuda and GPU on twitter" href="https://twitter.com/intent/tweet/?text=Triton%2c%20Cuda%20and%20GPU&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2ftriton%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Triton, Cuda and GPU on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2ftriton%2f&amp;title=Triton%2c%20Cuda%20and%20GPU&amp;summary=Triton%2c%20Cuda%20and%20GPU&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2ftriton%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Triton, Cuda and GPU on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2ftriton%2f&title=Triton%2c%20Cuda%20and%20GPU"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Triton, Cuda and GPU on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2ftriton%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Triton, Cuda and GPU on whatsapp" href="https://api.whatsapp.com/send?text=Triton%2c%20Cuda%20and%20GPU%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2ftriton%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Triton, Cuda and GPU on telegram" href="https://telegram.me/share/url?text=Triton%2c%20Cuda%20and%20GPU&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2ftriton%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>