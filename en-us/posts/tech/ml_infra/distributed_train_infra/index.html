<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Distributed Training Infra | Jun's Blog</title><meta name=keywords content><meta name=description content="Distributed training infrastructure"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/distributed_train_infra/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/distributed_train_infra/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="Distributed Training Infra"><meta property="og:description" content="Distributed training infrastructure"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml_infra/distributed_train_infra/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-05T00:18:23+08:00"><meta property="article:modified_time" content="2022-05-05T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Distributed Training Infra"><meta name=twitter:description content="Distributed training infrastructure"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Distributed Training Infra","item":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/distributed_train_infra/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Distributed Training Infra","name":"Distributed Training Infra","description":"Distributed training infrastructure","keywords":[""],"articleBody":"Distributed infrastructure is a big and interesting topic. I don‚Äôt work on infrastructure side, but I run into the concepts a lot, so I create this blog to help me understand more about infrastructure.\nMost of today‚Äôs distributed framework involves three parts, collective communication, data loading and preprocessing and distributed scheduler. We‚Äôll look into these three parts respectively.\nDistributed System Overview In the diagram below, I‚Äôm showing the modern distributed network communication implementation stack, from the bottom hardware to top level application.\nTraining system architecture Collective Communication We can start with point to point communication. Normally point to point communication refers to two processes communication and it‚Äôs one to one communication. Accordingly, collective communication refers to 1 to many or many to many communication. In distributed system, there are large amount of communications among the nodes.\nThere are some common communication ops, such as Broadcast, Reduce, Allreduce, Scatter, Gather, Allgather etc.\nBroadcast and Scatter Broadcast is to distribute data from one node to other nodes. Scatter is to distribute a portion of data to different nodes.\nMPI broadcast and scatter Gather and Allgather Gather is an inverse operation of scatter.\nMPI gather All gather: supposing we have 4 gpus, on each gpu we have tensor of size [B*S, 1024], after allgather, the tensor size on all gpus are [B*S, 4096]\nReduce and Allreduce Reduce is a collections of ops. Specifically, the operator will process an array from each process and get reduced number of elements.\nMPI reduce MPI reduce Allreduce means that the reduce operation will be conducted throughout all nodes. An all_reduce takes in a local array on each machine and returns the sum of all the arrays on every machine. Here we show flat all reduce operation below. However, the most common algorithm for doing this is a variant of the ‚Äúring allreduce‚Äù,\nMPI Allreduce The ReduceScatter operation performs the same operation as Reduce, except that the result is scattered in equal-sized blocks between ranks, each rank getting a chunk of data based on its rank index. In the figure below, each rank provides an array in of N (also called N element buffer, 4 here) values,\nMPI ReduceScatter Note: Executing ReduceScatter, followed by AllGather, is equivalent to the AllReduce operation.\nAll to All The following figure shows the difference between MPI all2all and all_gather.\nMPI all2all and all_gather For all2all, assuming we have 4 gpus, we on each one we have tensor size [B*S/4, 4096], after all2all, all the other gpus send data to gpu0. Its tensor size becomes [B*S, 4096]\nAll to all communication, is like during class break, there are several cluster of students. Each cluster has students from different classes. Then bell rings, and students go to their respective classrooms. All-gather: students share everything they know so that each one goes back to class with the full set of knowledge.\nAll-to-All in MPI: every process sends unique data to every other process, so each process ends up with a piece from everyone else.\nDuring break, students from different classes form mixed clusters. They exchange their notes / gossip / news with everyone in their cluster. When the bell rings, each student goes back to their own classroom. Now, every student has something from every other student, but each classroom contains only its own students again (no mixing remains). All-Gather in MPI all-gather: every process contributes one piece, and in the end, everyone has the full collection.\nDuring break, each student has one piece of news. Instead of splitting up, they all share their news so everyone hears everything. When the bell rings, each student goes back to their classroom. Now, every student knows all the news (the complete set). Ring-AllReduce Implementation An all_reduce takes in a local array on each machine and returns the sum of all the arrays on every machine. The most common algorithm for doing this is a variant of the ‚Äúring allreduce‚Äù, which we‚Äôll show how it works below. In practice, it‚Äôs usually optimized for better performance. It has two steps:\nReduce-scatter All-gather Mixed Precision Training Normally, during training we use single precision (32-bit floats). However, for LLM pretraining, this requires high-bandwidth computing platform. To address this challenge, people proposed mixed precision training. As the name suggested, mixed precision training is to leverage mixed different data type during training process, e.g. fp32 and fp16 or fp32 and bf16. We train model mostly in half precision and leave some critical ops in fp32. ss\nMixed precision training (image from fastai) Since it has same range as FP32, BF16 mixed precision training skips the scaling steps. All other Mixed Precision steps remain the same as FP16 Mixed Precision. We leave the batchnorm layers in single precision (they don‚Äôt have many weights so it‚Äôs not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss). The training loop is as follows:\ncompute the output with the FP16 model, then the loss back-propagate the gradients in half-precision. copy the gradients in FP32 precision do the update on the master model (in FP32 precision) copy the master model in the FP16 model. References https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html Andrew Gibiansky. Bringing HPC techniques to deep learning. http://research.baidu.com/bringing-hpc-techniques-deep-learning, 2017. [Online; accessed 6-December2017]. ","wordCount":"920","inLanguage":"en-us","datePublished":"2022-05-05T00:18:23+08:00","dateModified":"2022-05-05T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/distributed_train_infra/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>Distributed Training Infra</h1><div class=post-description>Distributed training infrastructure</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2022-05-05
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>920 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>2 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/blog/ style=color:var(--secondary)!important>Blog</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#distributed-system-overview aria-label="Distributed System Overview">Distributed System Overview</a></li><li><a href=#collective-communication aria-label="Collective Communication">Collective Communication</a><ul><li><a href=#broadcast-and-scatter aria-label="Broadcast and Scatter">Broadcast and Scatter</a></li><li><a href=#gather-and-allgather aria-label="Gather and Allgather">Gather and Allgather</a></li><li><a href=#reduce-and-allreduce aria-label="Reduce and Allreduce">Reduce and Allreduce</a></li><li><a href=#all-to-all aria-label="All to All">All to All</a></li><li><a href=#ring-allreduce-implementation aria-label="Ring-AllReduce Implementation">Ring-AllReduce Implementation</a></li><li><a href=#mixed-precision-training aria-label="Mixed Precision Training">Mixed Precision Training</a></li><li><a href=#references aria-label=References>References</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Distributed infrastructure is a big and interesting topic. I don&rsquo;t work on infrastructure side, but I run into the concepts a lot, so I create this blog to help me understand more about infrastructure.</p><p>Most of today&rsquo;s distributed framework involves three parts, collective communication, data loading and preprocessing and distributed scheduler. We&rsquo;ll look into these three parts respectively.</p><h2 id=distributed-system-overview>Distributed System Overview<a hidden class=anchor aria-hidden=true href=#distributed-system-overview>#</a></h2><p>In the diagram below, I&rsquo;m showing the modern distributed network communication implementation stack, from the bottom hardware to top level application.</p><p align=center><img alt="flat sharp minimum" src=images/system.png width=100% height=auto/>
<em>Training system architecture</em><br></p><h2 id=collective-communication>Collective Communication<a hidden class=anchor aria-hidden=true href=#collective-communication>#</a></h2><p>We can start with point to point communication. Normally point to point communication refers to two processes communication and it&rsquo;s one to one communication. Accordingly, collective communication refers to 1 to many or many to many communication. In distributed system, there are large amount of communications among the nodes.</p><p>There are some common communication ops, such as Broadcast, Reduce, Allreduce, Scatter, Gather, Allgather etc.</p><h3 id=broadcast-and-scatter>Broadcast and Scatter<a hidden class=anchor aria-hidden=true href=#broadcast-and-scatter>#</a></h3><p>Broadcast is to distribute data from one node to other nodes. Scatter is to distribute a portion of data to different nodes.</p><p align=center><img alt="flat sharp minimum" src=images/broadcast_and_scatter.png width=40% height=auto/>
<em>MPI broadcast and scatter</em><br></p><h3 id=gather-and-allgather>Gather and Allgather<a hidden class=anchor aria-hidden=true href=#gather-and-allgather>#</a></h3><p>Gather is an inverse operation of scatter.</p><p align=center><img alt="flat sharp minimum" src=images/gather.png width=40% height=auto/>
<em>MPI gather</em><br></p><p>All gather: supposing we have 4 gpus, on each gpu we have tensor of size [B*S, 1024], after allgather, the tensor size on all gpus are [B*S, 4096]</p><div align=center><img src=images/allgather.png style=width:30%;height:auto></div><h3 id=reduce-and-allreduce>Reduce and Allreduce<a hidden class=anchor aria-hidden=true href=#reduce-and-allreduce>#</a></h3><p>Reduce is a collections of ops. Specifically, the operator will process an array from each process and get reduced number of elements.</p><p align=center><img alt="flat sharp minimum" src=images/reduce1.png width=60% height=auto/>
<em>MPI reduce</em><br></p><p align=center><img alt="flat sharp minimum" src=images/reduce2.png width=60% height=auto/>
<em>MPI reduce</em><br></p><p>Allreduce means that the reduce operation will be conducted throughout all nodes. An all_reduce takes in a local array on each machine and returns the sum of all the arrays on every machine. Here we show flat all reduce operation below. However, the most common algorithm for doing this is a variant of the ‚Äúring allreduce‚Äù,</p><p align=center><img alt="flat sharp minimum" src=images/allreduce.png width=60% height=auto/>
<em>MPI Allreduce</em><br></p><p>The ReduceScatter operation performs the same operation as Reduce, except that the result is scattered in equal-sized blocks between ranks, each rank getting a chunk of data based on its rank index. In the figure below, each rank provides an array in of N (also called N element buffer, 4 here) values,</p><p align=center><img alt="flat sharp minimum" src=images/reduce_scatter.png width=60% height=auto/>
<em>MPI ReduceScatter</em><br></p><p>Note: Executing ReduceScatter, followed by AllGather, is equivalent to the AllReduce operation.</p><h3 id=all-to-all>All to All<a hidden class=anchor aria-hidden=true href=#all-to-all>#</a></h3><p>The following figure shows the difference between MPI all2all and all_gather.</p><p align=center><img alt="all2all and allgather" src=images/all2all_and_allgather.png width=60% height=auto/>
<em>MPI all2all and all_gather</em><br></p><p>For all2all, assuming we have 4 gpus, we on each one we have tensor size [B*S/4, 4096], after all2all, all the other gpus send
data to gpu0. Its tensor size becomes [B*S, 4096]</p><p>All to all communication, is like during class break, there are several cluster of students. Each cluster has students from different classes. Then bell rings, and students go to their respective classrooms.
All-gather: students share everything they know so that each one goes back to class with the full set of knowledge.</p><p>All-to-All in MPI: every process sends unique data to every other process, so each process ends up with a piece from everyone else.</p><ul><li>During break, students from different classes form mixed clusters.</li><li>They exchange their notes / gossip / news with everyone in their cluster.</li><li>When the bell rings, each student goes back to their own classroom.</li><li>Now, every student has something from every other student, but each classroom contains only its own students again (no mixing remains).</li></ul><p>All-Gather in MPI all-gather: every process contributes one piece, and in the end, everyone has the full collection.</p><ul><li>During break, each student has one piece of news.</li><li>Instead of splitting up, they all share their news so everyone hears everything.</li><li>When the bell rings, each student goes back to their classroom.</li><li>Now, every student knows all the news (the complete set).</li></ul><h3 id=ring-allreduce-implementation>Ring-AllReduce Implementation<a hidden class=anchor aria-hidden=true href=#ring-allreduce-implementation>#</a></h3><p>An all_reduce takes in a local array on each machine and returns the sum of all the arrays on every machine. The most common algorithm for doing this is a variant of the ‚Äúring allreduce‚Äù, which we‚Äôll show how it works below. In practice, it&rsquo;s usually optimized for better performance. It has two steps:</p><ul><li>Reduce-scatter</li><li>All-gather</li></ul><h3 id=mixed-precision-training>Mixed Precision Training<a hidden class=anchor aria-hidden=true href=#mixed-precision-training>#</a></h3><p>Normally, during training we use single precision (32-bit floats). However, for LLM pretraining, this requires high-bandwidth computing platform. To address this challenge, people proposed mixed precision training. As the name suggested, mixed precision training is to leverage mixed different data type during training process, e.g. fp32 and fp16 or fp32 and bf16. We train model mostly in
half precision and leave some critical ops in fp32. ss</p><p align=center><img alt="mixed precision training" src=images/mixed_precision.png width=60% height=auto/><br><em>Mixed precision training (image from fastai)</em><br></p><p>Since it has same range as FP32, BF16 mixed precision training skips the scaling steps. All other Mixed Precision steps remain the same as FP16 Mixed Precision.
We leave the batchnorm layers in single precision (they don‚Äôt have many weights so it‚Äôs not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss).
The training loop is as follows:</p><ul><li>compute the output with the FP16 model, then the loss</li><li>back-propagate the gradients in half-precision.</li><li>copy the gradients in FP32 precision</li><li>do the update on the master model (in FP32 precision)</li><li>copy the master model in the FP16 model.</li></ul><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ol><li><a href=https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html>https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html</a></li><li>Andrew Gibiansky. Bringing HPC techniques to deep learning. <a href=http://research.baidu.com/bringing-hpc-techniques-deep-learning>http://research.baidu.com/bringing-hpc-techniques-deep-learning</a>, 2017. [Online; accessed 6-December2017].</li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch/><span class=title>¬´</span><br><span>Pytorch Multiple-GPU Training</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/read/history/><span class=title>¬ª</span><br><span>History and Reputation</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>