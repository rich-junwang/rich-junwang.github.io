<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Autograd | Jun's Blog</title><meta name=keywords content><meta name=description content="In PyTorch&rsquo;s computation graph, there are only two types of elements: data (tensors) and operations (ops).
Operations include: addition, subtraction, multiplication, division, square root, exponentiation, trigonometric functions, and other differentiable operations.
Data has leaf nodes which are created by user and non-leaf node. The difference is after back propagation, gradient of non-leaf nodes will be released to save memory. If we want to retain non-leaf node gradient, we have to use retain_grad.
Tensor
Tensor in Pytorch has the following attributes:

data: stored data
require_grad: whether need to compute gradient. Self-defined leaf nodes usually default require_grad as False, and non-leaf nodes default as True. Neural network weights default as True.
grad: grad holds the value of gradient. Each time when performing a backward computation, you need to reset (zero out) the gradients from the previous step; otherwise, the gradient values will keep accumulating.
grad_fn: This is the backward function used to calculate the gradient. Leaf nodes usually have None for their grad_fn, and only the result nodes have a valid grad_fn, which indicates the type of gradient function.
is_leaf

Gradient Computation
There are two ways to compute grad in Pytorch."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="Autograd"><meta property="og:description" content="In PyTorch&rsquo;s computation graph, there are only two types of elements: data (tensors) and operations (ops).
Operations include: addition, subtraction, multiplication, division, square root, exponentiation, trigonometric functions, and other differentiable operations.
Data has leaf nodes which are created by user and non-leaf node. The difference is after back propagation, gradient of non-leaf nodes will be released to save memory. If we want to retain non-leaf node gradient, we have to use retain_grad.
Tensor
Tensor in Pytorch has the following attributes:

data: stored data
require_grad: whether need to compute gradient. Self-defined leaf nodes usually default require_grad as False, and non-leaf nodes default as True. Neural network weights default as True.
grad: grad holds the value of gradient. Each time when performing a backward computation, you need to reset (zero out) the gradients from the previous step; otherwise, the gradient values will keep accumulating.
grad_fn: This is the backward function used to calculate the gradient. Leaf nodes usually have None for their grad_fn, and only the result nodes have a valid grad_fn, which indicates the type of gradient function.
is_leaf

Gradient Computation
There are two ways to compute grad in Pytorch."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-11T00:18:23+08:00"><meta property="article:modified_time" content="2024-03-11T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Autograd"><meta name=twitter:description content="In PyTorch&rsquo;s computation graph, there are only two types of elements: data (tensors) and operations (ops).
Operations include: addition, subtraction, multiplication, division, square root, exponentiation, trigonometric functions, and other differentiable operations.
Data has leaf nodes which are created by user and non-leaf node. The difference is after back propagation, gradient of non-leaf nodes will be released to save memory. If we want to retain non-leaf node gradient, we have to use retain_grad.
Tensor
Tensor in Pytorch has the following attributes:

data: stored data
require_grad: whether need to compute gradient. Self-defined leaf nodes usually default require_grad as False, and non-leaf nodes default as True. Neural network weights default as True.
grad: grad holds the value of gradient. Each time when performing a backward computation, you need to reset (zero out) the gradients from the previous step; otherwise, the gradient values will keep accumulating.
grad_fn: This is the backward function used to calculate the gradient. Leaf nodes usually have None for their grad_fn, and only the result nodes have a valid grad_fn, which indicates the type of gradient function.
is_leaf

Gradient Computation
There are two ways to compute grad in Pytorch."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Autograd","item":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Autograd","name":"Autograd","description":"In PyTorch\u0026rsquo;s computation graph, there are only two types of elements: data (tensors) and operations (ops).\nOperations include: addition, subtraction, multiplication, division, square root, exponentiation, trigonometric functions, and other differentiable operations.\nData has leaf nodes which are created by user and non-leaf node. The difference is after back propagation, gradient of non-leaf nodes will be released to save memory. If we want to retain non-leaf node gradient, we have to use retain_grad.\nTensor Tensor in Pytorch has the following attributes:\ndata: stored data require_grad: whether need to compute gradient. Self-defined leaf nodes usually default require_grad as False, and non-leaf nodes default as True. Neural network weights default as True. grad: grad holds the value of gradient. Each time when performing a backward computation, you need to reset (zero out) the gradients from the previous step; otherwise, the gradient values will keep accumulating. grad_fn: This is the backward function used to calculate the gradient. Leaf nodes usually have None for their grad_fn, and only the result nodes have a valid grad_fn, which indicates the type of gradient function. is_leaf Gradient Computation There are two ways to compute grad in Pytorch.\n","keywords":[],"articleBody":"In PyTorchâ€™s computation graph, there are only two types of elements: data (tensors) and operations (ops).\nOperations include: addition, subtraction, multiplication, division, square root, exponentiation, trigonometric functions, and other differentiable operations.\nData has leaf nodes which are created by user and non-leaf node. The difference is after back propagation, gradient of non-leaf nodes will be released to save memory. If we want to retain non-leaf node gradient, we have to use retain_grad.\nTensor Tensor in Pytorch has the following attributes:\ndata: stored data require_grad: whether need to compute gradient. Self-defined leaf nodes usually default require_grad as False, and non-leaf nodes default as True. Neural network weights default as True. grad: grad holds the value of gradient. Each time when performing a backward computation, you need to reset (zero out) the gradients from the previous step; otherwise, the gradient values will keep accumulating. grad_fn: This is the backward function used to calculate the gradient. Leaf nodes usually have None for their grad_fn, and only the result nodes have a valid grad_fn, which indicates the type of gradient function. is_leaf Gradient Computation There are two ways to compute grad in Pytorch.\nBackward(): used to compute grad for leaf node. torch.autograd.grad() : Automatic grad computation Backward Letâ€™s first take a look at backward() function. The definition of the backward() function of the torch.autograd is as follows\ntorch.autograd.backward( tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None ) Here is the meaning of the parameters here:\ntensor: The tensor used for gradient computation. In other words, these two ways are equivalent: torch.autograd.backward(z) == z.backward(). grad_tensors: Used when computing gradients for matrices. It is also a tensor, and its shape generally needs to match the shape of the preceding tensor. retain_graph: Normally, after calling backward once, PyTorch will automatically destroy the computation graph. So if you want to call backward on a variable multiple times, you need to set this parameter to True. create_graph: When set to True, it allows the computation of higher-order gradients. grad_variables: According to the official documentation, â€œgrad_variables is deprecated. Use grad_tensors instead.â€ In other words, this parameter will likely be removed in future versions, so just use grad_tensors.\nNote that here t.backward() is equivalent to torch.autograd.backward(t).\nScaler Backward By default, autograd can only compute gradient for a scaler using backward function. For example:\nx = torch.tensor(2.0, requires_grad=True) y = torch.tensor(3.0, requires_grad=True) z = x**2+y z.backward() print(z, x.grad, y.grad) # tensor(7., grad_fn=) tensor(4.) tensor(1.) Tensor Backward x = torch.ones(2,requires_grad=True) z = x + 2 z.backward() # raise RuntimeError: grad can be implicitly created only for scalar outputs x = torch.ones(2,requires_grad=True) z = x + 2 z.backward(torch.ones_like(z)) We can sum z here to compute the grad. Or we can use the grad_tensor to multiply with z to compute the tensor.\nAutograd The internal nodes gradient are compute with autograd. Its interface is defined as below:\n# pytorch interface torch.autograd.grad( outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False ) We can also compute the gradient for leaf node using autograd. For example,\nimport torch x = torch.tensor(2.0, requires_grad=True) y = torch.tensor(3.0, requires_grad=True) z = x**2+y z.backward() print(z, x.grad, y.grad) x = torch.tensor(2.0, requires_grad=True) z = x**2 print(torch.autograd.grad(outputs=z, inputs=x), x.grad) Grad_fn and next_functions How the backward computation graph works with grad_fn and next_functions?\nEssentially grad_fn is an objection which\na callable to compute current step gradient with respect to input (such as loss) a pointer to previous compute node grad_fn through next_functions. Think of this as a linked list. # Example from ref 1. torch.manual_seed(6) x = torch.randn(4, 4, requires_grad=True) y = torch.randn(4, 4, requires_grad=True) z = x * y l = z.sum() l.backward() print(x.grad) print(y.grad) # Notice that we have ops (like multiply, sum) and tensors (x, y, z, l) # Forward # x # \\ # multi -\u003e z -\u003e sum -\u003e l # / # y # backward # dx # \\ # back_multi \u003c- dz \u003c- back_sum \u003c- dl # / # dy # equivalent torch.manual_seed(6) x = torch.randn(4, 4, requires_grad=True) y = torch.randn(4, 4, requires_grad=True) z = x * y l = z.sum() dl = torch.tensor(1.) back_sum = l.grad_fn dz = back_sum(dl) back_mul = back_sum.next_functions[0][0] dx, dy = back_mul(dz) back_x = back_mul.next_functions[0][0] back_x(dx) back_y = back_mul.next_functions[1][0] back_y(dy) print(x.grad) print(y.grad) Another example [2]\n# Notice that we have ops (like multiply, sum) and tensors (A, B, C etc) # A # \\ # multi -\u003e C -\u003e exp -\u003e D -\u003e sum -\u003e F # / / # B E A = torch.tensor(2., requires_grad=True) B = torch.tensor(.5, requires_grad=True) E = torch.tensor(1., requires_grad=True) C = A * B D = C.exp() F = D + E # tensor(3.7183, grad_fn=) æ‰“å°è®¡ç®—ç»“æœï¼Œå¯ä»¥çœ‹åˆ°Fçš„grad_fnæŒ‡å‘AddBackwardï¼Œå³äº§ç”ŸFçš„è¿ç®— print(F) # [True, True, False, False, True, False] æ‰“å°æ˜¯å¦ä¸ºå¶èŠ‚ç‚¹ï¼Œç”±ç”¨æˆ·åˆ›å»ºï¼Œä¸”requires_gradè®¾ä¸ºTrueçš„èŠ‚ç‚¹ä¸ºå¶èŠ‚ç‚¹ print([x.is_leaf for x in [A, B, C, D, E, F]]) # [, , , None] # æ¯ä¸ªå˜é‡çš„grad_fnæŒ‡å‘äº§ç”Ÿå…¶ç®—å­çš„backward functionï¼Œå¶èŠ‚ç‚¹çš„grad_fnä¸ºç©º print([x.grad_fn for x in [F, D, C, A]]) # print ((, 0), (, 0)) # ç”±äºF = D + Eï¼Œ å› æ­¤F.grad_fn.next_functionsä¹Ÿå­˜åœ¨ä¸¤é¡¹ï¼Œåˆ†åˆ«å¯¹åº”äºD, Eä¸¤ä¸ªå˜é‡ï¼Œ # æ¯ä¸ªå…ƒç»„ä¸­çš„ç¬¬ä¸€é¡¹å¯¹åº”äºç›¸åº”å˜é‡çš„grad_fnï¼Œç¬¬äºŒé¡¹æŒ‡ç¤ºç›¸åº”å˜é‡æ˜¯äº§ç”Ÿå…¶opçš„ç¬¬å‡ ä¸ªè¾“å‡ºã€‚ # Eä½œä¸ºå¶èŠ‚ç‚¹ï¼Œå…¶ä¸Šæ²¡æœ‰grad_fnï¼Œä½†æœ‰æ¢¯åº¦ç´¯ç§¯å‡½æ•°ï¼Œå³AccumulateGradï¼ˆç”±äºåä¼ æ—¶å¤šå‡ºå¯èƒ½äº§ç”Ÿæ¢¯åº¦ï¼Œéœ€è¦è¿›è¡Œç´¯åŠ ï¼‰ print(F.grad_fn.next_functions) # è¿›è¡Œæ¢¯åº¦åä¼  F.backward(retain_graph=True) # tensor(1.3591) tensor(5.4366) tensor(1.) ç®—å¾—æ¯ä¸ªå˜é‡æ¢¯åº¦ï¼Œä¸æ±‚å¯¼å¾—åˆ°çš„ç›¸ç¬¦ print(A.grad, B.grad, E.grad) print(C.grad, D.grad) next_functions returns a tuple, each element of which is also a tuple with two elements. The first is the previous grad_fn function we need to call, e.g. back_mul in the example. The second is the argument index of the previous ops in the previous output.\nRegister Hook register_hook function registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook can be registered for both tensor and ops.\nimport torch def print_grad(grad): print(grad) return grad / 2 w = torch.nn.Parameter(torch.randn(2, 2)) w.register_hook(print_grad) loss = (w - 1) ** 2 print('before backward') loss.mean().backward() print('after backward') print(w.grad) def parameter_hook(grad): print('parameter hook') def operator_hook(*grads): print('operator hook' ) w = torch.nn.Parameter(torch.randn(2, 2)) w.register_hook(parameter_hook) print('first') y = w + 1 op1 = y.grad_fn print(op1) op1.register_hook(operator_hook) y.sum().backward() print('second') z = w + 1 op2 = z.grad_fn print(op2) z.sum().backward() model.eval() and torch.no_grad() One last word at eval() and no_grad(). These two are actually unrelated. During inference, both need to be used: model.eval() sets modules like BatchNorm and Dropout to evaluation mode, ensuring the correctness of the inference results, but it does not help save memory. torch.no_grad() declares that no gradients should be calculated, which does save a lot of memory and GPU memory.\nScale Gradient In megablocks lib, there is a piece of code which catches my attention\nclass ScaleGradient(torch.autograd.Function): @staticmethod @torch.amp.autocast_mode.custom_fwd(device_type='cuda') def forward(ctx: Any, x: torch.Tensor, scale: float): ctx.scale = scale return x @staticmethod @torch.amp.autocast_mode.custom_bwd(device_type='cuda') def backward(ctx: torch.Tensor, grad: torch.Tensor): return grad * ctx.scale, None Why we want to scale gradient in such a way instead of scale the loss?\nLetâ€™s take a look at this class. In the forward pass, it simply returns x unchanged. In the backward pass, it multiplies the gradient by scale before propagating it backward. So the value of x remains the same during forward, but its gradient flow is scaled. Essentially it acts as identity operator with a custom gradient.\nWhy not just scale the loss? Scaling the loss (loss * scale) will scale all gradients in the computation graph by that factor, globally.\nBut sometimes we might want to:\nScale gradients for a particular tensor or path, While leaving the rest of the model unchanged. Thatâ€™s what ScaleGradient enables. We can selectively apply it to certain variables or branches of the computation graph.\nExample:\ny1 = scale_gradient(f(x), 0.5) # Gradients to f(x) scaled down by 0.5 y2 = g(x) # Normal gradient flow loss = y1 + y2 Here, only the gradient flowing through f(x) is scaled. Scaling the whole loss would shrink both y1 and y2 paths, which isnâ€™t what we want.\nReferences https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00 https://zhuanlan.zhihu.com/p/321449610 ","wordCount":"1599","inLanguage":"en-us","datePublished":"2024-03-11T00:18:23+08:00","dateModified":"2024-03-11T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>Autograd</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-03-11
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1599 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>4 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tensor aria-label=Tensor>Tensor</a></li><li><a href=#gradient-computation aria-label="Gradient Computation">Gradient Computation</a><ul><li><a href=#backward aria-label=Backward>Backward</a><ul><li><a href=#scaler-backward aria-label="Scaler Backward">Scaler Backward</a></li><li><a href=#tensor-backward aria-label="Tensor Backward">Tensor Backward</a></li></ul></li><li><a href=#autograd aria-label=Autograd>Autograd</a></li></ul></li><li><a href=#grad_fn-and-next_functions aria-label="Grad_fn and next_functions">Grad_fn and next_functions</a></li><li><a href=#register-hook aria-label="Register Hook">Register Hook</a></li><li><a href=#modeleval-and-torchno_grad aria-label="model.eval() and torch.no_grad()">model.eval() and torch.no_grad()</a></li><li><a href=#scale-gradient aria-label="Scale Gradient">Scale Gradient</a><ul><li><a href=#why-not-just-scale-the-loss aria-label="Why not just scale the loss?">Why not just scale the loss?</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>In PyTorch&rsquo;s computation graph, there are only two types of elements: data (tensors) and operations (ops).</p><p>Operations include: addition, subtraction, multiplication, division, square root, exponentiation, trigonometric functions, and other differentiable operations.</p><p>Data has leaf nodes which are created by user and non-leaf node. The difference is after back propagation, gradient of non-leaf nodes will be released to save memory. If we want to retain non-leaf node gradient, we have to use <code>retain_grad</code>.</p><h2 id=tensor>Tensor<a hidden class=anchor aria-hidden=true href=#tensor>#</a></h2><p>Tensor in Pytorch has the following attributes:</p><ol><li>data: stored data</li><li>require_grad: whether need to compute gradient. Self-defined leaf nodes usually default require_grad as False, and non-leaf nodes default as True. Neural network weights default as True.</li><li>grad: grad holds the value of gradient. Each time when performing a backward computation, you need to reset (zero out) the gradients from the previous step; otherwise, the gradient values will keep accumulating.</li><li>grad_fn: This is the backward function used to calculate the gradient. Leaf nodes usually have <code>None</code> for their grad_fn, and only the result nodes have a valid grad_fn, which indicates the type of gradient function.</li><li>is_leaf</li></ol><h2 id=gradient-computation>Gradient Computation<a hidden class=anchor aria-hidden=true href=#gradient-computation>#</a></h2><p>There are two ways to compute grad in Pytorch.</p><ul><li>Backward(): used to compute grad for leaf node.</li><li>torch.autograd.grad() : Automatic grad computation</li></ul><h3 id=backward>Backward<a hidden class=anchor aria-hidden=true href=#backward>#</a></h3><p>Let&rsquo;s first take a look at <code>backward()</code> function. The definition of the <code>backward()</code> function of the <code>torch.autograd</code> is as follows</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>backward(
</span></span><span style=display:flex><span>    tensors, 
</span></span><span style=display:flex><span>    grad_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>    retain_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>    create_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, 
</span></span><span style=display:flex><span>    grad_variables<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Here is the meaning of the parameters here:</p><blockquote><p>tensor: The tensor used for gradient computation. In other words, these two ways are equivalent: <code>torch.autograd.backward(z) == z.backward()</code>.
grad_tensors: Used when computing gradients for matrices. It is also a tensor, and its shape generally needs to match the shape of the preceding tensor.
retain_graph: Normally, after calling backward once, PyTorch will automatically destroy the computation graph. So if you want to call backward on a variable multiple times, you need to set this parameter to True.
create_graph: When set to True, it allows the computation of higher-order gradients.
grad_variables: According to the official documentation, &ldquo;grad_variables is deprecated. Use grad_tensors instead.&rdquo; In other words, this parameter will likely be removed in future versions, so just use grad_tensors.</p></blockquote><p>Note that here <code>t.backward()</code> is equivalent to <code>torch.autograd.backward(t)</code>.</p><h4 id=scaler-backward>Scaler Backward<a hidden class=anchor aria-hidden=true href=#scaler-backward>#</a></h4><p>By default, autograd can only compute gradient for a scaler using <code>backward</code> function. For example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>3.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#f92672>+</span>y
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(z, x<span style=color:#f92672>.</span>grad, y<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(7., grad_fn=&lt;AddBackward0&gt;) tensor(4.) tensor(1.)</span>
</span></span></code></pre></div><h4 id=tensor-backward>Tensor Backward<a hidden class=anchor aria-hidden=true href=#tensor-backward>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>2</span>,requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># raise RuntimeError: grad can be implicitly created only for scalar outputs</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>2</span>,requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>backward(torch<span style=color:#f92672>.</span>ones_like(z))
</span></span></code></pre></div><p>We can sum <code>z</code> here to compute the grad. Or we can use the <code>grad_tensor</code> to multiply with <code>z</code> to compute the tensor.</p><h3 id=autograd>Autograd<a hidden class=anchor aria-hidden=true href=#autograd>#</a></h3><p>The internal nodes gradient are compute with autograd. Its interface is defined as below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># pytorch interface</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>grad(
</span></span><span style=display:flex><span>    outputs, 
</span></span><span style=display:flex><span>    inputs, 
</span></span><span style=display:flex><span>    grad_outputs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>    retain_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>    create_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, 
</span></span><span style=display:flex><span>    only_inputs<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, 
</span></span><span style=display:flex><span>    allow_unused<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>We can also compute the gradient for leaf node using autograd. For example,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>3.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#f92672>+</span>y
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(z, x<span style=color:#f92672>.</span>grad, y<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>print(torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>grad(outputs<span style=color:#f92672>=</span>z, inputs<span style=color:#f92672>=</span>x), x<span style=color:#f92672>.</span>grad)
</span></span></code></pre></div><h2 id=grad_fn-and-next_functions>Grad_fn and next_functions<a hidden class=anchor aria-hidden=true href=#grad_fn-and-next_functions>#</a></h2><p>How the backward computation graph works with grad_fn and next_functions?</p><p>Essentially <code>grad_fn</code> is an objection which</p><ul><li>a callable to compute current step gradient with respect to input (such as loss)</li><li>a pointer to previous compute node <code>grad_fn</code> through <code>next_functions</code>. Think of this as a linked list.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Example from ref 1. </span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> y
</span></span><span style=display:flex><span>l <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>l<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(x<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>print(y<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Notice that we have ops (like multiply, sum) and tensors (x, y, z, l)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Forward</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     \</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#         multi  -&gt; z  -&gt; sum  -&gt; l</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     /</span>
</span></span><span style=display:flex><span><span style=color:#75715e># y</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># backward</span>
</span></span><span style=display:flex><span><span style=color:#75715e># dx</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     \</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#         back_multi  &lt;- dz  &lt;- back_sum  &lt;- dl</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     /</span>
</span></span><span style=display:flex><span><span style=color:#75715e># dy</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># equivalent </span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> y
</span></span><span style=display:flex><span>l <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>dl <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>1.</span>)
</span></span><span style=display:flex><span>back_sum <span style=color:#f92672>=</span> l<span style=color:#f92672>.</span>grad_fn
</span></span><span style=display:flex><span>dz <span style=color:#f92672>=</span> back_sum(dl)
</span></span><span style=display:flex><span>back_mul <span style=color:#f92672>=</span> back_sum<span style=color:#f92672>.</span>next_functions[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>dx, dy <span style=color:#f92672>=</span> back_mul(dz)
</span></span><span style=display:flex><span>back_x <span style=color:#f92672>=</span> back_mul<span style=color:#f92672>.</span>next_functions[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>back_x(dx)
</span></span><span style=display:flex><span>back_y <span style=color:#f92672>=</span> back_mul<span style=color:#f92672>.</span>next_functions[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>back_y(dy)
</span></span><span style=display:flex><span>print(x<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>print(y<span style=color:#f92672>.</span>grad)
</span></span></code></pre></div><p>Another example [2]</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Notice that we have ops (like multiply, sum) and tensors (A, B, C etc)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># A</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     \</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#       multi -&gt; C  -&gt; exp  -&gt; D  -&gt; sum  -&gt; F</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     /                           /  </span>
</span></span><span style=display:flex><span><span style=color:#75715e># B                            E </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>B <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>.5</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>E <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>1.</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>C <span style=color:#f92672>=</span> A <span style=color:#f92672>*</span> B
</span></span><span style=display:flex><span>D <span style=color:#f92672>=</span> C<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>F <span style=color:#f92672>=</span> D <span style=color:#f92672>+</span> E
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(3.7183, grad_fn=&lt;AddBackward0&gt;) æ‰“å°è®¡ç®—ç»“æœï¼Œå¯ä»¥çœ‹åˆ°Fçš„grad_fnæŒ‡å‘AddBackwardï¼Œå³äº§ç”ŸFçš„è¿ç®—</span>
</span></span><span style=display:flex><span>print(F)       
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [True, True, False, False, True, False] æ‰“å°æ˜¯å¦ä¸ºå¶èŠ‚ç‚¹ï¼Œç”±ç”¨æˆ·åˆ›å»ºï¼Œä¸”requires_gradè®¾ä¸ºTrueçš„èŠ‚ç‚¹ä¸ºå¶èŠ‚ç‚¹</span>
</span></span><span style=display:flex><span>print([x<span style=color:#f92672>.</span>is_leaf <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> [A, B, C, D, E, F]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [&lt;AddBackward0 object at 0x7f972de8c7b8&gt;, &lt;ExpBackward object at 0x7f972de8c278&gt;, &lt;MulBackward0 object at 0x7f972de8c2b0&gt;, None]  </span>
</span></span><span style=display:flex><span><span style=color:#75715e># æ¯ä¸ªå˜é‡çš„grad_fnæŒ‡å‘äº§ç”Ÿå…¶ç®—å­çš„backward functionï¼Œå¶èŠ‚ç‚¹çš„grad_fnä¸ºç©º</span>
</span></span><span style=display:flex><span>print([x<span style=color:#f92672>.</span>grad_fn <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> [F, D, C, A]])    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># print ((&lt;ExpBackward object at 0x7f972de8c390&gt;, 0), (&lt;AccumulateGrad object at 0x7f972de8c5f8&gt;, 0)) </span>
</span></span><span style=display:flex><span><span style=color:#75715e># ç”±äºF = D + Eï¼Œ å› æ­¤F.grad_fn.next_functionsä¹Ÿå­˜åœ¨ä¸¤é¡¹ï¼Œåˆ†åˆ«å¯¹åº”äºD, Eä¸¤ä¸ªå˜é‡ï¼Œ</span>
</span></span><span style=display:flex><span><span style=color:#75715e># æ¯ä¸ªå…ƒç»„ä¸­çš„ç¬¬ä¸€é¡¹å¯¹åº”äºç›¸åº”å˜é‡çš„grad_fnï¼Œç¬¬äºŒé¡¹æŒ‡ç¤ºç›¸åº”å˜é‡æ˜¯äº§ç”Ÿå…¶opçš„ç¬¬å‡ ä¸ªè¾“å‡ºã€‚</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Eä½œä¸ºå¶èŠ‚ç‚¹ï¼Œå…¶ä¸Šæ²¡æœ‰grad_fnï¼Œä½†æœ‰æ¢¯åº¦ç´¯ç§¯å‡½æ•°ï¼Œå³AccumulateGradï¼ˆç”±äºåä¼ æ—¶å¤šå‡ºå¯èƒ½äº§ç”Ÿæ¢¯åº¦ï¼Œéœ€è¦è¿›è¡Œç´¯åŠ ï¼‰</span>
</span></span><span style=display:flex><span>print(F<span style=color:#f92672>.</span>grad_fn<span style=color:#f92672>.</span>next_functions) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># è¿›è¡Œæ¢¯åº¦åä¼ </span>
</span></span><span style=display:flex><span>F<span style=color:#f92672>.</span>backward(retain_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)   
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(1.3591) tensor(5.4366) tensor(1.) ç®—å¾—æ¯ä¸ªå˜é‡æ¢¯åº¦ï¼Œä¸æ±‚å¯¼å¾—åˆ°çš„ç›¸ç¬¦</span>
</span></span><span style=display:flex><span>print(A<span style=color:#f92672>.</span>grad, B<span style=color:#f92672>.</span>grad, E<span style=color:#f92672>.</span>grad)   
</span></span><span style=display:flex><span>print(C<span style=color:#f92672>.</span>grad, D<span style=color:#f92672>.</span>grad)  
</span></span></code></pre></div><p>next_functions returns a tuple, each element of which is also a tuple with two elements. The first is the previous <code>grad_fn</code> function we need to call, e.g. back_mul in the example. The second is the argument index of the previous ops in the previous output.</p><h2 id=register-hook>Register Hook<a hidden class=anchor aria-hidden=true href=#register-hook>#</a></h2><p><code>register_hook</code> function registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook can be registered for both tensor and ops.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>print_grad</span>(grad):
</span></span><span style=display:flex><span>    print(grad)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> grad <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>w<span style=color:#f92672>.</span>register_hook(print_grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> (w <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;before backward&#39;</span>)
</span></span><span style=display:flex><span>loss<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;after backward&#39;</span>)
</span></span><span style=display:flex><span>print(w<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parameter_hook</span>(grad):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;parameter hook&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>operator_hook</span>(<span style=color:#f92672>*</span>grads):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;operator hook&#39;</span> )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>w<span style=color:#f92672>.</span>register_hook(parameter_hook)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;first&#39;</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> w <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>op1 <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>grad_fn
</span></span><span style=display:flex><span>print(op1)
</span></span><span style=display:flex><span>op1<span style=color:#f92672>.</span>register_hook(operator_hook)
</span></span><span style=display:flex><span>y<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;second&#39;</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> w <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>op2 <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>grad_fn
</span></span><span style=display:flex><span>print(op2)
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><h2 id=modeleval-and-torchno_grad>model.eval() and torch.no_grad()<a hidden class=anchor aria-hidden=true href=#modeleval-and-torchno_grad>#</a></h2><p>One last word at <code>eval()</code> and <code>no_grad()</code>. These two are actually unrelated. During inference, both need to be used: model.eval() sets modules like BatchNorm and Dropout to evaluation mode, ensuring the correctness of the inference results, but it does not help save memory. torch.no_grad() declares that no gradients should be calculated, which does save a lot of memory and GPU memory.</p><h2 id=scale-gradient>Scale Gradient<a hidden class=anchor aria-hidden=true href=#scale-gradient>#</a></h2><p>In megablocks lib, there is a piece of code which catches my attention</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ScaleGradient</span>(torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>Function):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@torch.amp.autocast_mode.custom_fwd</span>(device_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(ctx: Any, x: torch<span style=color:#f92672>.</span>Tensor, scale: float):
</span></span><span style=display:flex><span>        ctx<span style=color:#f92672>.</span>scale <span style=color:#f92672>=</span> scale
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@torch.amp.autocast_mode.custom_bwd</span>(device_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backward</span>(ctx: torch<span style=color:#f92672>.</span>Tensor, grad: torch<span style=color:#f92672>.</span>Tensor):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> grad <span style=color:#f92672>*</span> ctx<span style=color:#f92672>.</span>scale, <span style=color:#66d9ef>None</span>
</span></span></code></pre></div><p>Why we want to scale gradient in such a way instead of scale the loss?</p><p>Let&rsquo;s take a look at this class. In the forward pass, it simply returns <code>x</code> unchanged. In the backward pass, it multiplies the gradient by <code>scale</code> before propagating it backward. So the value of <code>x</code> remains the same during forward, but its gradient flow is scaled. Essentially it acts as <strong>identity operator with a custom gradient</strong>.</p><h3 id=why-not-just-scale-the-loss>Why not just scale the loss?<a hidden class=anchor aria-hidden=true href=#why-not-just-scale-the-loss>#</a></h3><p>Scaling the loss (<code>loss * scale</code>) will scale <strong>all gradients in the computation graph</strong> by that factor, globally.</p><p>But sometimes we might want to:</p><ul><li><strong>Scale gradients for a particular tensor or path</strong>,</li><li>While leaving the rest of the model unchanged.</li></ul><p>Thatâ€™s what <code>ScaleGradient</code> enables. We can selectively apply it to certain variables or branches of the computation graph.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y1 <span style=color:#f92672>=</span> scale_gradient(f(x), <span style=color:#ae81ff>0.5</span>)   <span style=color:#75715e># Gradients to f(x) scaled down by 0.5</span>
</span></span><span style=display:flex><span>y2 <span style=color:#f92672>=</span> g(x)                        <span style=color:#75715e># Normal gradient flow</span>
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> y1 <span style=color:#f92672>+</span> y2
</span></span></code></pre></div><p>Here, only the gradient flowing through <code>f(x)</code> is scaled. Scaling the whole loss would shrink <strong>both y1 and y2 paths</strong>, which isnâ€™t what we want.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00>https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00</a></li><li><a href=https://zhuanlan.zhihu.com/p/321449610>https://zhuanlan.zhihu.com/p/321449610</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/mcts/><span class=title>Â«</span><br><span>Monte Carlo Tree Search</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml/speech/whisper/><span class=title>Â»</span><br><span>Whisper Model</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Autograd on twitter" href="https://twitter.com/intent/tweet/?text=Autograd&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fpytorch%2fpytorch_autograd%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Autograd on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fpytorch%2fpytorch_autograd%2f&amp;title=Autograd&amp;summary=Autograd&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fpytorch%2fpytorch_autograd%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Autograd on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fpytorch%2fpytorch_autograd%2f&title=Autograd"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Autograd on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fpytorch%2fpytorch_autograd%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Autograd on whatsapp" href="https://api.whatsapp.com/send?text=Autograd%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fpytorch%2fpytorch_autograd%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Autograd on telegram" href="https://telegram.me/share/url?text=Autograd&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fpytorch%2fpytorch_autograd%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>