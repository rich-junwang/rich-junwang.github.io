<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Autograd | Jun's Blog</title>
<meta name=keywords content><meta name=description content="PyTorch.."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="Autograd"><meta property="og:description" content="PyTorch.."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-11T00:18:23+08:00"><meta property="article:modified_time" content="2024-03-11T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Autograd"><meta name=twitter:description content="PyTorch.."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"📚Articles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"👨🏻‍💻 Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Autograd","item":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Autograd","name":"Autograd","description":"PyTorch..","keywords":[""],"articleBody":"In PyTorch’s computation graph, there are only two types of elements: data (tensors) and operations (ops).\nOperations include: addition, subtraction, multiplication, division, square root, exponentiation, trigonometric functions, and other differentiable operations.\nData has leaf nodes which are created by user and non-leaf node. The difference is after back propagation, gradient of non-leaf nodes will be released to save memory. If we want to retain non-leaf node gradient, we have to use retain_grad.\nTensor Tensor in Pytorch has the following attributes:\ndata: stored data require_grad: whether need to compute gradient. Self-defined leaf nodes usually default require_grad as False, and non-leaf nodes default as True. Neural network weights default as True. grad: grad holds the value of gradient. Each time when performing a backward computation, you need to reset (zero out) the gradients from the previous step; otherwise, the gradient values will keep accumulating. grad_fn: This is the backward function used to calculate the gradient. Leaf nodes usually have None for their grad_fn, and only the result nodes have a valid grad_fn, which indicates the type of gradient function. is_leaf Gradient Computation There are two ways to compute grad in Pytorch.\nBackward(): used to compute grad for leaf node. torch.autograd.grad() : Automatic grad computation Backward Let’s first take a look at backward() function. The definition of the backward() function of the torch.autograd is as follows\ntorch.autograd.backward( tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None ) Here is the meaning of the parameters here:\ntensor: The tensor used for gradient computation. In other words, these two ways are equivalent: torch.autograd.backward(z) == z.backward(). grad_tensors: Used when computing gradients for matrices. It is also a tensor, and its shape generally needs to match the shape of the preceding tensor. retain_graph: Normally, after calling backward once, PyTorch will automatically destroy the computation graph. So if you want to call backward on a variable multiple times, you need to set this parameter to True. create_graph: When set to True, it allows the computation of higher-order gradients. grad_variables: According to the official documentation, “grad_variables is deprecated. Use grad_tensors instead.” In other words, this parameter will likely be removed in future versions, so just use grad_tensors.\nNote that here t.backward() is equivalent to torch.autograd.backward(t).\nScaler Backward By default, autograd can only compute gradient for a scaler using backward function. For example:\nx = torch.tensor(2.0, requires_grad=True) y = torch.tensor(3.0, requires_grad=True) z = x**2+y z.backward() print(z, x.grad, y.grad) # tensor(7., grad_fn=) tensor(4.) tensor(1.) Tensor Backward x = torch.ones(2,requires_grad=True) z = x + 2 z.backward() # raise RuntimeError: grad can be implicitly created only for scalar outputs x = torch.ones(2,requires_grad=True) z = x + 2 z.backward(torch.ones_like(z)) We can sum z here to compute the grad. Or we can use the grad_tensor to multiply with z to compute the tensor.\nAutograd The internal nodes gradient are compute with autograd. Its interface is defined as below:\n# pytorch interface torch.autograd.grad( outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False ) We can also compute the gradient for leaf node using autograd. For example,\nimport torch x = torch.tensor(2.0, requires_grad=True) y = torch.tensor(3.0, requires_grad=True) z = x**2+y z.backward() print(z, x.grad, y.grad) x = torch.tensor(2.0, requires_grad=True) z = x**2 print(torch.autograd.grad(outputs=z, inputs=x), x.grad) Grad_fn and next_functions How the backward computation graph works with grad_fn and next_functions?\nEssentially grad_fn is an objection which\na callable to compute current step gradient with respect to input (such as loss) a pointer to previous compute node grad_fn through next_functions. Think of this as a linked list. # Example from ref 1. torch.manual_seed(6) x = torch.randn(4, 4, requires_grad=True) y = torch.randn(4, 4, requires_grad=True) z = x * y l = z.sum() l.backward() print(x.grad) print(y.grad) # Notice that we have ops (like multiply, sum) and tensors (x, y, z, l) # Forward # x # \\ # multi -\u003e z -\u003e sum -\u003e l # / # y # backward # dx # \\ # back_multi \u003c- dz \u003c- back_sum \u003c- dl # / # dy # equivalent torch.manual_seed(6) x = torch.randn(4, 4, requires_grad=True) y = torch.randn(4, 4, requires_grad=True) z = x * y l = z.sum() dl = torch.tensor(1.) back_sum = l.grad_fn dz = back_sum(dl) back_mul = back_sum.next_functions[0][0] dx, dy = back_mul(dz) back_x = back_mul.next_functions[0][0] back_x(dx) back_y = back_mul.next_functions[1][0] back_y(dy) print(x.grad) print(y.grad) Another example [2]\n# Notice that we have ops (like multiply, sum) and tensors (A, B, C etc) # A # \\ # multi -\u003e C -\u003e exp -\u003e D -\u003e sum -\u003e F # / / # B E A = torch.tensor(2., requires_grad=True) B = torch.tensor(.5, requires_grad=True) E = torch.tensor(1., requires_grad=True) C = A * B D = C.exp() F = D + E # tensor(3.7183, grad_fn=) 打印计算结果，可以看到F的grad_fn指向AddBackward，即产生F的运算 print(F) # [True, True, False, False, True, False] 打印是否为叶节点，由用户创建，且requires_grad设为True的节点为叶节点 print([x.is_leaf for x in [A, B, C, D, E, F]]) # [, , , None] # 每个变量的grad_fn指向产生其算子的backward function，叶节点的grad_fn为空 print([x.grad_fn for x in [F, D, C, A]]) # print ((, 0), (, 0)) # 由于F = D + E， 因此F.grad_fn.next_functions也存在两项，分别对应于D, E两个变量， # 每个元组中的第一项对应于相应变量的grad_fn，第二项指示相应变量是产生其op的第几个输出。 # E作为叶节点，其上没有grad_fn，但有梯度累积函数，即AccumulateGrad（由于反传时多出可能产生梯度，需要进行累加） print(F.grad_fn.next_functions) # 进行梯度反传 F.backward(retain_graph=True) # tensor(1.3591) tensor(5.4366) tensor(1.) 算得每个变量梯度，与求导得到的相符 print(A.grad, B.grad, E.grad) print(C.grad, D.grad) next_functions returns a tuple, each element of which is also a tuple with two elements. The first is the previous grad_fn function we need to call, e.g. back_mul in the example. The second is the argument index of the previous ops in the previous output.\nRegister Hook register_hook function registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook can be registered for both tensor and ops.\nimport torch def print_grad(grad): print(grad) return grad / 2 w = torch.nn.Parameter(torch.randn(2, 2)) w.register_hook(print_grad) loss = (w - 1) ** 2 print('before backward') loss.mean().backward() print('after backward') print(w.grad) def parameter_hook(grad): print('parameter hook') def operator_hook(*grads): print('operator hook' ) w = torch.nn.Parameter(torch.randn(2, 2)) w.register_hook(parameter_hook) print('first') y = w + 1 op1 = y.grad_fn print(op1) op1.register_hook(operator_hook) y.sum().backward() print('second') z = w + 1 op2 = z.grad_fn print(op2) z.sum().backward() model.eval() and torch.no_grad() One last word at eval() and no_grad(). These two are actually unrelated. During inference, both need to be used: model.eval() sets modules like BatchNorm and Dropout to evaluation mode, ensuring the correctness of the inference results, but it does not help save memory. torch.no_grad() declares that no gradients should be calculated, which does save a lot of memory and GPU memory.\nReferences https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00 https://zhuanlan.zhihu.com/p/321449610 ","wordCount":"1362","inLanguage":"en-us","datePublished":"2024-03-11T00:18:23+08:00","dateModified":"2024-03-11T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/pytorch_autograd/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="🏠 Home"><span>🏠 Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="🙋🏻‍♂️ About"><span>🙋🏻‍♂️ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="📚 Posts"><span>📚 Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="🧩 Tags"><span>🧩 Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="⏱️ Archives"><span>⏱️ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="🔍 Search (Alt + /)" accesskey=/><span>🔍 Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>🏠 Home</a>&nbsp;»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>📚Articles</a>&nbsp;»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>👨🏻‍💻 Tech</a></div><h1 class=post-title>Autograd</h1><div class=post-description>PyTorch..</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-03-11
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>1362 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>3 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/ml/ style=color:var(--secondary)!important>ML</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tensor aria-label=Tensor>Tensor</a></li><li><a href=#gradient-computation aria-label="Gradient Computation">Gradient Computation</a><ul><li><a href=#backward aria-label=Backward>Backward</a><ul><li><a href=#scaler-backward aria-label="Scaler Backward">Scaler Backward</a></li><li><a href=#tensor-backward aria-label="Tensor Backward">Tensor Backward</a></li></ul></li><li><a href=#autograd aria-label=Autograd>Autograd</a></li></ul></li><li><a href=#grad_fn-and-next_functions aria-label="Grad_fn and next_functions">Grad_fn and next_functions</a></li><li><a href=#register-hook aria-label="Register Hook">Register Hook</a></li><li><a href=#modeleval-and-torchno_grad aria-label="model.eval() and torch.no_grad()">model.eval() and torch.no_grad()</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>In PyTorch&rsquo;s computation graph, there are only two types of elements: data (tensors) and operations (ops).</p><p>Operations include: addition, subtraction, multiplication, division, square root, exponentiation, trigonometric functions, and other differentiable operations.</p><p>Data has leaf nodes which are created by user and non-leaf node. The difference is after back propagation, gradient of non-leaf nodes will be released to save memory. If we want to retain non-leaf node gradient, we have to use <code>retain_grad</code>.</p><h2 id=tensor>Tensor<a hidden class=anchor aria-hidden=true href=#tensor>#</a></h2><p>Tensor in Pytorch has the following attributes:</p><ol><li>data: stored data</li><li>require_grad: whether need to compute gradient. Self-defined leaf nodes usually default require_grad as False, and non-leaf nodes default as True. Neural network weights default as True.</li><li>grad: grad holds the value of gradient. Each time when performing a backward computation, you need to reset (zero out) the gradients from the previous step; otherwise, the gradient values will keep accumulating.</li><li>grad_fn: This is the backward function used to calculate the gradient. Leaf nodes usually have <code>None</code> for their grad_fn, and only the result nodes have a valid grad_fn, which indicates the type of gradient function.</li><li>is_leaf</li></ol><h2 id=gradient-computation>Gradient Computation<a hidden class=anchor aria-hidden=true href=#gradient-computation>#</a></h2><p>There are two ways to compute grad in Pytorch.</p><ul><li>Backward(): used to compute grad for leaf node.</li><li>torch.autograd.grad() : Automatic grad computation</li></ul><h3 id=backward>Backward<a hidden class=anchor aria-hidden=true href=#backward>#</a></h3><p>Let&rsquo;s first take a look at <code>backward()</code> function. The definition of the <code>backward()</code> function of the <code>torch.autograd</code> is as follows</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>backward(
</span></span><span style=display:flex><span>    tensors, 
</span></span><span style=display:flex><span>    grad_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>    retain_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>    create_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, 
</span></span><span style=display:flex><span>    grad_variables<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Here is the meaning of the parameters here:</p><blockquote><p>tensor: The tensor used for gradient computation. In other words, these two ways are equivalent: <code>torch.autograd.backward(z) == z.backward()</code>.
grad_tensors: Used when computing gradients for matrices. It is also a tensor, and its shape generally needs to match the shape of the preceding tensor.
retain_graph: Normally, after calling backward once, PyTorch will automatically destroy the computation graph. So if you want to call backward on a variable multiple times, you need to set this parameter to True.
create_graph: When set to True, it allows the computation of higher-order gradients.
grad_variables: According to the official documentation, &ldquo;grad_variables is deprecated. Use grad_tensors instead.&rdquo; In other words, this parameter will likely be removed in future versions, so just use grad_tensors.</p></blockquote><p>Note that here <code>t.backward()</code> is equivalent to <code>torch.autograd.backward(t)</code>.</p><h4 id=scaler-backward>Scaler Backward<a hidden class=anchor aria-hidden=true href=#scaler-backward>#</a></h4><p>By default, autograd can only compute gradient for a scaler using <code>backward</code> function. For example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>3.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#f92672>+</span>y
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(z, x<span style=color:#f92672>.</span>grad, y<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(7., grad_fn=&lt;AddBackward0&gt;) tensor(4.) tensor(1.)</span>
</span></span></code></pre></div><h4 id=tensor-backward>Tensor Backward<a hidden class=anchor aria-hidden=true href=#tensor-backward>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>2</span>,requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># raise RuntimeError: grad can be implicitly created only for scalar outputs</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>2</span>,requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>backward(torch<span style=color:#f92672>.</span>ones_like(z))
</span></span></code></pre></div><p>We can sum <code>z</code> here to compute the grad. Or we can use the <code>grad_tensor</code> to multiply with <code>z</code> to compute the tensor.</p><h3 id=autograd>Autograd<a hidden class=anchor aria-hidden=true href=#autograd>#</a></h3><p>The internal nodes gradient are compute with autograd. Its interface is defined as below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># pytorch interface</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>grad(
</span></span><span style=display:flex><span>    outputs, 
</span></span><span style=display:flex><span>    inputs, 
</span></span><span style=display:flex><span>    grad_outputs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>    retain_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, 
</span></span><span style=display:flex><span>    create_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, 
</span></span><span style=display:flex><span>    only_inputs<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, 
</span></span><span style=display:flex><span>    allow_unused<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>We can also compute the gradient for leaf node using autograd. For example,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>3.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#f92672>+</span>y
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(z, x<span style=color:#f92672>.</span>grad, y<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>print(torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>grad(outputs<span style=color:#f92672>=</span>z, inputs<span style=color:#f92672>=</span>x), x<span style=color:#f92672>.</span>grad)
</span></span></code></pre></div><h2 id=grad_fn-and-next_functions>Grad_fn and next_functions<a hidden class=anchor aria-hidden=true href=#grad_fn-and-next_functions>#</a></h2><p>How the backward computation graph works with grad_fn and next_functions?</p><p>Essentially <code>grad_fn</code> is an objection which</p><ul><li>a callable to compute current step gradient with respect to input (such as loss)</li><li>a pointer to previous compute node <code>grad_fn</code> through <code>next_functions</code>. Think of this as a linked list.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Example from ref 1. </span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> y
</span></span><span style=display:flex><span>l <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>l<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(x<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>print(y<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Notice that we have ops (like multiply, sum) and tensors (x, y, z, l)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Forward</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     \</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#         multi  -&gt; z  -&gt; sum  -&gt; l</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     /</span>
</span></span><span style=display:flex><span><span style=color:#75715e># y</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># backward</span>
</span></span><span style=display:flex><span><span style=color:#75715e># dx</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     \</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#         back_multi  &lt;- dz  &lt;- back_sum  &lt;- dl</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     /</span>
</span></span><span style=display:flex><span><span style=color:#75715e># dy</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># equivalent </span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> y
</span></span><span style=display:flex><span>l <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>dl <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>1.</span>)
</span></span><span style=display:flex><span>back_sum <span style=color:#f92672>=</span> l<span style=color:#f92672>.</span>grad_fn
</span></span><span style=display:flex><span>dz <span style=color:#f92672>=</span> back_sum(dl)
</span></span><span style=display:flex><span>back_mul <span style=color:#f92672>=</span> back_sum<span style=color:#f92672>.</span>next_functions[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>dx, dy <span style=color:#f92672>=</span> back_mul(dz)
</span></span><span style=display:flex><span>back_x <span style=color:#f92672>=</span> back_mul<span style=color:#f92672>.</span>next_functions[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>back_x(dx)
</span></span><span style=display:flex><span>back_y <span style=color:#f92672>=</span> back_mul<span style=color:#f92672>.</span>next_functions[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>back_y(dy)
</span></span><span style=display:flex><span>print(x<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>print(y<span style=color:#f92672>.</span>grad)
</span></span></code></pre></div><p>Another example [2]</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Notice that we have ops (like multiply, sum) and tensors (A, B, C etc)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># A</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     \</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#       multi -&gt; C  -&gt; exp  -&gt; D  -&gt; sum  -&gt; F</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     /                           /  </span>
</span></span><span style=display:flex><span><span style=color:#75715e># B                            E </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>2.</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>B <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>.5</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>E <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(<span style=color:#ae81ff>1.</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>C <span style=color:#f92672>=</span> A <span style=color:#f92672>*</span> B
</span></span><span style=display:flex><span>D <span style=color:#f92672>=</span> C<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>F <span style=color:#f92672>=</span> D <span style=color:#f92672>+</span> E
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(3.7183, grad_fn=&lt;AddBackward0&gt;) 打印计算结果，可以看到F的grad_fn指向AddBackward，即产生F的运算</span>
</span></span><span style=display:flex><span>print(F)       
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [True, True, False, False, True, False] 打印是否为叶节点，由用户创建，且requires_grad设为True的节点为叶节点</span>
</span></span><span style=display:flex><span>print([x<span style=color:#f92672>.</span>is_leaf <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> [A, B, C, D, E, F]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [&lt;AddBackward0 object at 0x7f972de8c7b8&gt;, &lt;ExpBackward object at 0x7f972de8c278&gt;, &lt;MulBackward0 object at 0x7f972de8c2b0&gt;, None]  </span>
</span></span><span style=display:flex><span><span style=color:#75715e># 每个变量的grad_fn指向产生其算子的backward function，叶节点的grad_fn为空</span>
</span></span><span style=display:flex><span>print([x<span style=color:#f92672>.</span>grad_fn <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> [F, D, C, A]])    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># print ((&lt;ExpBackward object at 0x7f972de8c390&gt;, 0), (&lt;AccumulateGrad object at 0x7f972de8c5f8&gt;, 0)) </span>
</span></span><span style=display:flex><span><span style=color:#75715e># 由于F = D + E， 因此F.grad_fn.next_functions也存在两项，分别对应于D, E两个变量，</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 每个元组中的第一项对应于相应变量的grad_fn，第二项指示相应变量是产生其op的第几个输出。</span>
</span></span><span style=display:flex><span><span style=color:#75715e># E作为叶节点，其上没有grad_fn，但有梯度累积函数，即AccumulateGrad（由于反传时多出可能产生梯度，需要进行累加）</span>
</span></span><span style=display:flex><span>print(F<span style=color:#f92672>.</span>grad_fn<span style=color:#f92672>.</span>next_functions) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 进行梯度反传</span>
</span></span><span style=display:flex><span>F<span style=color:#f92672>.</span>backward(retain_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)   
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(1.3591) tensor(5.4366) tensor(1.) 算得每个变量梯度，与求导得到的相符</span>
</span></span><span style=display:flex><span>print(A<span style=color:#f92672>.</span>grad, B<span style=color:#f92672>.</span>grad, E<span style=color:#f92672>.</span>grad)   
</span></span><span style=display:flex><span>print(C<span style=color:#f92672>.</span>grad, D<span style=color:#f92672>.</span>grad)  
</span></span></code></pre></div><p>next_functions returns a tuple, each element of which is also a tuple with two elements. The first is the previous <code>grad_fn</code> function we need to call, e.g. back_mul in the example. The second is the argument index of the previous ops in the previous output.</p><h2 id=register-hook>Register Hook<a hidden class=anchor aria-hidden=true href=#register-hook>#</a></h2><p><code>register_hook</code> function registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook can be registered for both tensor and ops.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>print_grad</span>(grad):
</span></span><span style=display:flex><span>    print(grad)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> grad <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>w<span style=color:#f92672>.</span>register_hook(print_grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> (w <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;before backward&#39;</span>)
</span></span><span style=display:flex><span>loss<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;after backward&#39;</span>)
</span></span><span style=display:flex><span>print(w<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parameter_hook</span>(grad):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;parameter hook&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>operator_hook</span>(<span style=color:#f92672>*</span>grads):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;operator hook&#39;</span> )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>w<span style=color:#f92672>.</span>register_hook(parameter_hook)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;first&#39;</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> w <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>op1 <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>grad_fn
</span></span><span style=display:flex><span>print(op1)
</span></span><span style=display:flex><span>op1<span style=color:#f92672>.</span>register_hook(operator_hook)
</span></span><span style=display:flex><span>y<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;second&#39;</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> w <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>op2 <span style=color:#f92672>=</span> z<span style=color:#f92672>.</span>grad_fn
</span></span><span style=display:flex><span>print(op2)
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><h2 id=modeleval-and-torchno_grad>model.eval() and torch.no_grad()<a hidden class=anchor aria-hidden=true href=#modeleval-and-torchno_grad>#</a></h2><p>One last word at <code>eval()</code> and <code>no_grad()</code>. These two are actually unrelated. During inference, both need to be used: model.eval() sets modules like BatchNorm and Dropout to evaluation mode, ensuring the correctness of the inference results, but it does not help save memory. torch.no_grad() declares that no gradients should be calculated, which does save a lot of memory and GPU memory.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00>https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00</a></li><li><a href=https://zhuanlan.zhihu.com/p/321449610>https://zhuanlan.zhihu.com/p/321449610</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/llm/mcts/><span class=title>«</span><br><span>Monte Carlo Tree Search</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/llm/speech/whisper/><span class=title>»</span><br><span>Whisper Model</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'👉Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'👇Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

————————————————\r
版权声明：本文为「Jun's Blog」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\r
原文链接：`+location.href,s=window.getSelection().toString()+`\r

————————————————\r
版权声明：本文为「Jun's Blog」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\r
原文链接：`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
————————————————\r
版权声明：本文为「Jun's Blog」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\r
原文链接：`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>