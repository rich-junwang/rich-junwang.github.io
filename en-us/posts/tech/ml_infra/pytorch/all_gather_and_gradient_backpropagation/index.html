<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>All Gather and Gradients | Jun's Blog</title>
<meta name=keywords content><meta name=description content="PyTorch.."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/all_gather_and_gradient_backpropagation/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/all_gather_and_gradient_backpropagation/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="All Gather and Gradients"><meta property="og:description" content="PyTorch.."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/all_gather_and_gradient_backpropagation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-11T00:18:23+08:00"><meta property="article:modified_time" content="2023-12-11T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="All Gather and Gradients"><meta name=twitter:description content="PyTorch.."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"All Gather and Gradients","item":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/all_gather_and_gradient_backpropagation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"All Gather and Gradients","name":"All Gather and Gradients","description":"PyTorch..","keywords":[""],"articleBody":"In PyTorch, autograd keeps a record of data (tensors) \u0026 all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, we can automatically compute the gradients using the chain rule.\nIn a forward pass, autograd does two things simultaneously: run the requested operation to compute a resulting tensor, and maintain the operationâ€™s gradient function in the DAG. The backward pass kicks off when .backward() is called on the DAG root. autograd then: computes the gradients from each grad_fn, accumulates them in the respective tensorâ€™s .grad attribute, and using the chain rule, propagates all the way to the leaf tensors. From this, we can know that when we call functions like torch.distributed.all_gather, the resulting tensors do not propagate back gradients. This can be verified with the following code snippet.\nimport os import torch from torch import nn batch_size = 16 rank = int(os.environ.get('OMPI_COMM_WORLD_RANK', '0')) world_size = int(os.environ.get('OMPI_COMM_WORLD_SIZE', '1')) bs_each = batch_size // world_size device_id = int(os.environ.get('OMPI_COMM_WORLD_LOCAL_RANK', '0')) torch.cuda.set_device(device_id) torch.distributed.init_process_group( backend='nccl', init_method='tcp://localhost:12345', rank=rank, world_size=world_size, ) model = nn.Linear(1, 1, bias=False) model.weight.data[:] = 1. model = model.cuda() x = torch.ones((bs_each, 1), requires_grad=True).cuda() y = model(x) ys = [torch.zeros_like(y) for i in range(world_size)] torch.distributed.all_gather(ys, y) print(y.grad_fn) # for x in ys: print(x.grad_fn) # None print(x.requires_grad) # False Here we talk about how to use all_gather function in the pytorch so that we could still leverage auto_grad to help us for backpropagation.\nSolution One We can wrap the all_gather function and pass the context information to the gathered tensor.\nimport torch import torch.distributed as dist class GatherLayer(torch.autograd.Function): \"\"\"Gather tensors from all process, supporting backward propagation.\"\"\" @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = [torch.zeros_like(input) for _ in range(dist.get_world_size())] dist.all_gather(output, input) return tuple(output) @staticmethod def backward(ctx, *grads): (input,) = ctx.saved_tensors grad_out = torch.zeros_like(input) grad_out[:] = grads[dist.get_rank()] return grad_out Solution Two As shown below, we put the auto_grad captured tensor back to the gather tensor. In this way, this specific element on current rank will have gradient.\nall_x = [torch.zeros_like(x) for _ in range(world_size)] torch.distributed.all_gather(all_x, x) all_x[rank] = x References https://github.com/Spijkervet/SimCLR https://github.com/princeton-nlp/SimCSE ","wordCount":"384","inLanguage":"en-us","datePublished":"2023-12-11T00:18:23+08:00","dateModified":"2023-12-11T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/pytorch/all_gather_and_gradient_backpropagation/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>All Gather and Gradients</h1><div class=post-description>PyTorch..</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2023-12-11
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>384 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>1 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/blog/ style=color:var(--secondary)!important>Blog</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#solution-one aria-label="Solution One">Solution One</a></li><li><a href=#solution-two aria-label="Solution Two">Solution Two</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>In PyTorch, autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, we can automatically compute the gradients using the chain rule.</p><p></p>In a forward pass, autograd does two things simultaneously:<ul><li>run the requested operation to compute a resulting tensor, and</li><li>maintain the operationâ€™s gradient function in the DAG.</li></ul><p></p>The backward pass kicks off when .backward() is called on the DAG root. autograd then:<ul><li>computes the gradients from each <a href=https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00>grad_fn</a>,</li><li>accumulates them in the respective tensorâ€™s .grad attribute, and</li><li>using the chain rule, propagates all the way to the leaf tensors.</li></ul><p>From this, we can know that when we call functions like <code>torch.distributed.all_gather</code>, the resulting tensors do not propagate back gradients. This can be verified with the following code snippet.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>rank <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;OMPI_COMM_WORLD_RANK&#39;</span>, <span style=color:#e6db74>&#39;0&#39;</span>))
</span></span><span style=display:flex><span>world_size <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;OMPI_COMM_WORLD_SIZE&#39;</span>, <span style=color:#e6db74>&#39;1&#39;</span>))
</span></span><span style=display:flex><span>bs_each <span style=color:#f92672>=</span> batch_size <span style=color:#f92672>//</span> world_size
</span></span><span style=display:flex><span>device_id <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;OMPI_COMM_WORLD_LOCAL_RANK&#39;</span>, <span style=color:#e6db74>&#39;0&#39;</span>))
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>set_device(device_id)
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>distributed<span style=color:#f92672>.</span>init_process_group(
</span></span><span style=display:flex><span>    backend<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nccl&#39;</span>,
</span></span><span style=display:flex><span>    init_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tcp://localhost:12345&#39;</span>,
</span></span><span style=display:flex><span>    rank<span style=color:#f92672>=</span>rank,
</span></span><span style=display:flex><span>    world_size<span style=color:#f92672>=</span>world_size,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data[:] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones((bs_each, <span style=color:#ae81ff>1</span>), requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>ys <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>zeros_like(y) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(world_size)]
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>distributed<span style=color:#f92672>.</span>all_gather(ys, y)
</span></span><span style=display:flex><span>print(y<span style=color:#f92672>.</span>grad_fn)
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;MmBackward object at 0x7ff10dfea500&gt;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> ys:
</span></span><span style=display:flex><span>     print(x<span style=color:#f92672>.</span>grad_fn)   <span style=color:#75715e># None</span>
</span></span><span style=display:flex><span>     print(x<span style=color:#f92672>.</span>requires_grad)  <span style=color:#75715e># False</span>
</span></span></code></pre></div><p>Here we talk about how to use all_gather function in the pytorch so that we could still leverage auto_grad to help us for backpropagation.</p><h2 id=solution-one>Solution One<a hidden class=anchor aria-hidden=true href=#solution-one>#</a></h2><p>We can wrap the all_gather function and pass the context information to the gathered tensor.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.distributed <span style=color:#66d9ef>as</span> dist
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GatherLayer</span>(torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>Function):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Gather tensors from all process, supporting backward propagation.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(ctx, input):
</span></span><span style=display:flex><span>        ctx<span style=color:#f92672>.</span>save_for_backward(input)
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>zeros_like(input) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(dist<span style=color:#f92672>.</span>get_world_size())]
</span></span><span style=display:flex><span>        dist<span style=color:#f92672>.</span>all_gather(output, input)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> tuple(output)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backward</span>(ctx, <span style=color:#f92672>*</span>grads):
</span></span><span style=display:flex><span>        (input,) <span style=color:#f92672>=</span> ctx<span style=color:#f92672>.</span>saved_tensors
</span></span><span style=display:flex><span>        grad_out <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros_like(input)
</span></span><span style=display:flex><span>        grad_out[:] <span style=color:#f92672>=</span> grads[dist<span style=color:#f92672>.</span>get_rank()]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> grad_out
</span></span></code></pre></div><h2 id=solution-two>Solution Two<a hidden class=anchor aria-hidden=true href=#solution-two>#</a></h2><p>As shown below, we put the auto_grad captured tensor back to the gather tensor. In this way, this specific element on current rank will have gradient.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>all_x <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>zeros_like(x) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(world_size)]
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>distributed<span style=color:#f92672>.</span>all_gather(all_x, x)
</span></span><span style=display:flex><span>all_x[rank] <span style=color:#f92672>=</span> x
</span></span></code></pre></div><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://github.com/Spijkervet/SimCLR>https://github.com/Spijkervet/SimCLR</a></li><li><a href=https://github.com/princeton-nlp/SimCSE>https://github.com/princeton-nlp/SimCSE</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/coder_training/><span class=title>Â«</span><br><span>Coder Training</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml/model_eval/><span class=title>Â»</span><br><span>Model Evaluation</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2026
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>