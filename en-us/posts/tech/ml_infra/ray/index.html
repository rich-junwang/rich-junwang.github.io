<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ray | Jun's Blog</title><meta name=keywords content><meta name=description content="Promise and Future
Before diving deep into Ray, I&rsquo;ll first give a brief introduction to the async ops in programming in C++.
An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread&rsquo;s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task&rsquo;s result. The result of the delegated task is only required at some point in the future when it is needed.
An asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.

    
    Promise and future: worker gets the promise instance and main driver gets the future
    
"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/ray/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/ray/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="Ray"><meta property="og:description" content="Promise and Future
Before diving deep into Ray, I&rsquo;ll first give a brief introduction to the async ops in programming in C++.
An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread&rsquo;s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task&rsquo;s result. The result of the delegated task is only required at some point in the future when it is needed.
An asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.

    
    Promise and future: worker gets the promise instance and main driver gets the future
    
"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/ml_infra/ray/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-18T00:18:23+08:00"><meta property="article:modified_time" content="2024-06-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ray"><meta name=twitter:description content="Promise and Future
Before diving deep into Ray, I&rsquo;ll first give a brief introduction to the async ops in programming in C++.
An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread&rsquo;s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task&rsquo;s result. The result of the delegated task is only required at some point in the future when it is needed.
An asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.

    
    Promise and future: worker gets the promise instance and main driver gets the future
    
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Ray","item":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/ray/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ray","name":"Ray","description":"Promise and Future Before diving deep into Ray, I\u0026rsquo;ll first give a brief introduction to the async ops in programming in C++. An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread\u0026rsquo;s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task\u0026rsquo;s result. The result of the delegated task is only required at some point in the future when it is needed.\nAn asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.\nPromise and future: worker gets the promise instance and main driver gets the future ","keywords":[],"articleBody":"Promise and Future Before diving deep into Ray, Iâ€™ll first give a brief introduction to the async ops in programming in C++. An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current threadâ€™s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated taskâ€™s result. The result of the delegated task is only required at some point in the future when it is needed.\nAn asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.\nPromise and future: worker gets the promise instance and main driver gets the future When an asynchronous call is created, an instance of std::future is returned to the creator of the asynchronous call (receiver). Meanwhile, the executor of the asynchronous call (sender) holds an instance of std::promise. The executor uses std::promise to fulfill its promise (a commitment to deliver the result at a future point in time after the operation is completed), while the creator uses std::future to obtain this future value (the result corresponding to the promise fulfilled in the future). The std::promise instance held by the executor and the std::future instance held by the creator are both connected to a shared object. This shared object establishes a communication channel for information synchronization between the creator and the executor of the asynchronous call, enabling both parties to exchange information about the execution status of the asynchronous operation through this channel.\nThe executor accesses this channel via its std::promise instance to write values into the channel. The creator uses its std::future instance to retrieve values from the channel. Once the executor completes the execution of the asynchronous operation, it writes the result of the operation into the channel via the std::promise instance. The creator then retrieves the result of the asynchronous operation through its std::future instance.\n#include #include #include #include #include #include int main() { // åˆ›å»ºä¸€ä¸ªpromiseå¯¹è±¡å®ä¾‹ std::promise\u003cint\u003e _promise; // ä»promiseå¯¹è±¡å®ä¾‹ä¸­è·å–å¯¹åº”çš„futureå¯¹è±¡å®ä¾‹ std::future\u003cint\u003e _future = _promise.get_future(); // æ„å»ºæµ‹è¯•å¯¹è±¡ std::vector\u003cint\u003e test_data = {1, 2, 3, 4, 5, 6}; // åˆ›å»ºä¸€ä¸ªä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡å¯¹å®¹å™¨å†…çš„æ‰€æœ‰å…ƒç´ æ±‚å’Œï¼Œæ±‚å’Œå®Œæˆä¹‹åé€šè¿‡promiseæ¥è¿”å›ç»“æœ auto sum_of_data = [](const std::vector\u003cint\u003e\u0026 data, std::promise\u003cint\u003e prom) { int sum = std::accumulate(data.begin(), data.end(), 0); std::this_thread::sleep_for(std::chrono::milliseconds(500)); // ä¼‘çœ 500ms prom.set_value(sum); // å®Œæˆæ‰¿è¯ºï¼Œå°†ç»“æœå†™å…¥é€šé“ä¸­ }; // å°†è¿™ä¸ªä»»åŠ¡äº¤ç»™ä¸€ä¸ªçº¿ç¨‹ï¼Œè¿›è¡Œå¼‚æ­¥æ‰§è¡Œ std::thread work_thread(sum_of_data, test_data, std::move(_promise)); // é€šè¿‡futureçš„æ–¹æ³•æ¥è·å–å¼‚æ­¥è°ƒç”¨çš„ç»“æœ std::cout \u003c\u003c \"async result is \" \u003c\u003c _future.get() \u003c\u003c std::endl; // ç­‰å¾…work_threadé€€å‡ºé”€æ¯å®Œæˆ work_thread.join(); return 0; } // output // async result is 21 Introduction Ray was designed with training RL system in mind. There are three kinds of tasks in RL system training\nSimulation where agents interact with environments and environments omit responses Evaluation where agents generate rollout trajectory Training where policy is updated for improvement To handle these heterogenous tasks, Ray abstracts two kinds of computation:\nStateless task: functions decorated with @ray.remote in python Stateful actor: classes decorated with @ray.remote in python Below is one example of task computation in Ray.\n# Ray task example import ray ray.init() @ray.remote def func(x): return x ** 2 # driver process if __name__ == \"__main__\": # create 2 workers, execute func remotely, return 2 futures which each points to a remote op futures = [func.remote(i) for i in range(2)] # blocking results = ray.get(futures)) print(f\"The final result is: {results}\") # [0, 1] Ray Actor To declare an actor, we can annotate a class with @ray.remote, just like declaring a task from a function. Ray actor is different from python class in the following ways:\nAdd accessor methods for any data members that you need to read or write, because using direct access, such as my_game.state, doesnâ€™t work for actors. Construct actor instances with my_instance = MyClass.remote(â€¦). Call methods with my_instance.some_method.remote(â€¦). Use ray.get() and ray.wait() to retrieve results, just like you do for task results. Another way to get an Ray actor is through ray.remote(myClass). Similarly, we can instantiate the ray actor using myClass().remote().\nimport ray @ray.remote class Counter: def __init__(self): self.value = 0 def increment(self): self.value += 1 return self.value # define the accessor as we can't use direct access def get_counter(self): return self.value # Create an actor from this class. counter = Counter.remote() # Call the actor. obj_ref = counter.increment.remote() print(ray.get(obj_ref)) # Create ten Counter actors. counters = [Counter.remote() for _ in range(10)] # Increment each Counter once and get the results. These tasks all happen in # parallel. results = ray.get([c.increment.remote() for c in counters]) print(results) # Increment the first Counter five times. These tasks are executed serially # and share state. results = ray.get([counters[0].increment.remote() for _ in range(5)]) print(results) Ray Architecture Ray is a general-purpose framework for parallel programming on a cluster.\nRay Architecture Ray Tips Delay ray.get() With Ray, the invocation of every remote operation (e.g., task, actor method) is asynchronous. This means that the operation returns immediately a promise/future, which is essentially an identifier (ID) of the operationâ€™s result. This is key to achieve parallelism, as it allows the driver program to launch multiple operations in parallel. To get the actual results, the programmer needs to call ray.get() on the IDs of the results. This call blocks until the results are available. As a side effect, this operation also blocks the driver program from invoking other operations, which can hurt parallelism.\nimport time def do_some_work(x): time.sleep(1) # Replace this with work you need to do. return x start = time.time() results = [do_some_work(x) for x in range(4)] print(\"duration =\", time.time() - start, \"\\nresults = \", results) # Above output, the program takes around 4 ses: # duration = 4.0149290561676025 # results = [0, 1, 2, 3] start = time.time() results = [do_some_work.remote(x) for x in range(4)] print(\"duration =\", time.time() - start, \"\\nresults = \", results) # Above output, # duration = 0.0003619194030761719 # results = [ObjectID(0100000000bdf683fc3e45db42685232b19d2a61), ObjectID(01000000da69c40e1c2f43b391443ce23de46cda), ObjectID(010000007fe0954ac2b3c0ab991538043e8f37e0), ObjectID(01000000cf47d5ecd1e26b42624454c795abe89b)] start = time.time() results = [ray.get(do_some_work.remote(x)) for x in range(4)] print(\"duration =\", time.time() - start, \"\\nresults = \", results) # Above output, # duration = 4.018050909042358 # results = [0, 1, 2, 3] results = ray.get([do_some_work.remote(x) for x in range(4)]) # output # duration = 1.0064549446105957 # results = [0, 1, 2, 3] Note that ray.get() is blocking, so calling it after each remote operation means that we wait for that operation to complete, which essentially means that we execute one operation at a time, hence no parallelism! To enable parallelism, we need to call ray.get() after invoking all tasks. We can easily do so in our example by replacing line â€œresults = [do_some_work.remote(x) for x in range(4)]â€ with:\nAvoid passing same object repeatedly to remote tasks When we pass a large object as an argument to a remote function, Ray calls ray.put() under the hood to store that object in the local object store. This can significantly improve the performance of a remote task invocation when the remote task is executed locally, as all local tasks share the object store. However, there are cases when automatically calling ray.put() on a task invocation leads to performance issues. One example is passing the same large object as an argument repeatedly, as illustrated by the program below:\nimport time import numpy as np import ray ray.init(num_cpus = 4) @ray.remote def no_work(a): return start = time.time() a = np.zeros((10000, 2000)) result_ids = [no_work.remote(a) for x in range(10)] results = ray.get(result_ids) print(\"duration =\", time.time() - start) # output # duration = 1.0699057579040527 The right way is to put the shared object in the object store as shown below.\nimport time import numpy as np import ray ray.init(num_cpus = 4) @ray.remote def no_work(a): return start = time.time() a_id = ray.put(np.zeros((10000, 2000))) result_ids = [no_work.remote(a_id) for x in range(10)] results = ray.get(result_ids) print(\"duration =\", time.time() - start) # output # duration = 0.12425804138183594 Pipeline data processing If we use ray.get() on the results of multiple tasks we will have to wait until the last one of these tasks finishes. This can be an issue if tasks take widely different amounts of time. To illustrate this issue, consider the following example where we run four do_some_work() tasks in parallel, with each task taking a time uniformly distributed between 0 and 4 sec. Next, assume the results of these tasks are processed by process_results(), which takes 1 sec per result. The expected running time is then (1) the time it takes to execute the slowest of the do_some_work() tasks, plus (2) 4 sec which is the time it takes to execute process_results().\nimport time import random import ray ray.init(num_cpus = 4) @ray.remote def do_some_work(x): time.sleep(random.uniform(0, 4)) # Replace this with work you need to do. return x def process_results(results): sum = 0 for x in results: time.sleep(1) # Replace this with some processing code. sum += x return sum start = time.time() data_list = ray.get([do_some_work.remote(x) for x in range(4)]) sum = process_results(data_list) print(\"duration =\", time.time() - start, \"\\nresult = \", sum) # output # duration = 7.82636022567749 # result = 6 In order to process data as soon as itâ€™s available, we can use the pipeline to remove the waiting time. Ray allows us to do exactly this by calling ray.wait() on a list of object IDs. Without specifying any other parameters, this function returns as soon as an object in its argument list is ready. This call has two returns: (1) the ID of the ready object, and (2) the list containing the IDs of the objects not ready yet. The modified program is below.\nimport time import random import ray ray.init(num_cpus = 4) @ray.remote def do_some_work(x): time.sleep(random.uniform(0, 4)) # Replace this with work you need to do. return x def process_incremental(sum, result): time.sleep(1) # Replace this with some processing code. return sum + result start = time.time() result_ids = [do_some_work.remote(x) for x in range(4)] sum = 0 while len(result_ids): done_id, result_ids = ray.wait(result_ids) sum = process_incremental(sum, ray.get(done_id[0])) print(\"duration =\", time.time() - start, \"\\nresult = \", sum) #output # duration = 4.852453231811523 # result = 6 Pipeline execution, image from 1 A full example on how to use Ray for distributed neural network training\nimport torch import torch.nn as nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor from ray import train from ray.train.torch import TorchTrainer from ray.train import ScalingConfig def get_dataset(): return datasets.FashionMNIST( root=\"/tmp/data\", train=True, download=True, transform=ToTensor(), ) class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, inputs): inputs = self.flatten(inputs) logits = self.linear_relu_stack(inputs) return logits # without distributed training, pure pytorch def train_func(): num_epochs = 3 batch_size = 64 dataset = get_dataset() dataloader = DataLoader(dataset, batch_size=batch_size) model = NeuralNetwork() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(num_epochs): for inputs, labels in dataloader: optimizer.zero_grad() pred = model(inputs) loss = criterion(pred, labels) loss.backward() optimizer.step() print(f\"epoch: {epoch}, loss: {loss.item()}\") # train_func() # distributed training def train_func_distributed(): num_epochs = 3 batch_size = 64 dataset = get_dataset() dataloader = DataLoader(dataset, batch_size=batch_size) dataloader = train.torch.prepare_data_loader(dataloader) model = NeuralNetwork() model = train.torch.prepare_model(model) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(num_epochs): for inputs, labels in dataloader: optimizer.zero_grad() pred = model(inputs) loss = criterion(pred, labels) loss.backward() optimizer.step() print(f\"epoch: {epoch}, loss: {loss.item()}\") # For GPU Training, set `use_gpu` to True. use_gpu = False trainer = TorchTrainer( train_func_distributed, scaling_config=ScalingConfig(num_workers=4, use_gpu=use_gpu) ) results = trainer.fit() Ray Debugging # check ray status, if seeing connection error ray status # stop existing ray server ray stop # start new ray server ray start --head # multi-node training # on head node ray start --head --dashboard-host=0.0.0.0 ray start --head --dashboard-host=0.0.0.0 --num-gpus 8 # on worker node ray start --address= # submit a job ray job submit --address=\"http://127.0.0.1:8265\" \\ --runtime-env=my_runtime_env.yaml \\ python3 my_script.py \\ # to test the setup ray job submit --address http://localhost:8265 -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\" # sometime the error is not that obvious, we can go check ray logs here, e.g. tail -f /tmp/ray/session_latest/logs/dashboard* less /tmp/ray/session_latest/logs/dashboard* # list all ray jobs ray job list There is another way to stop ray\nimport ray ray.shutdown() There are two ways to initialize ray with environment variables.\nset env variables before ray initialization. It will pick up the env variables at startup. read -r -d '' my_config_json \u003c","wordCount":"2398","inLanguage":"en-us","datePublished":"2024-06-18T00:18:23+08:00","dateModified":"2024-06-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/ml_infra/ray/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>Ray</h1><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2024-06-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>2398 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>5 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#promise-and-future aria-label="Promise and Future">Promise and Future</a></li><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#ray-actor aria-label="Ray Actor">Ray Actor</a><ul><li><a href=#ray-architecture aria-label="Ray Architecture">Ray Architecture</a></li></ul></li><li><a href=#ray-tips aria-label="Ray Tips">Ray Tips</a><ul><li><a href=#delay-rayget aria-label="Delay ray.get()">Delay ray.get()</a></li><li><a href=#avoid-passing-same-object-repeatedly-to-remote-tasks aria-label="Avoid passing same object repeatedly to remote tasks">Avoid passing same object repeatedly to remote tasks</a></li><li><a href=#pipeline-data-processing aria-label="Pipeline data processing">Pipeline data processing</a></li></ul></li><li><a href=#ray-debugging aria-label="Ray Debugging">Ray Debugging</a><ul><li><a href=#logging aria-label=Logging>Logging</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=promise-and-future>Promise and Future<a hidden class=anchor aria-hidden=true href=#promise-and-future>#</a></h2><p>Before diving deep into Ray, I&rsquo;ll first give a brief introduction to the async ops in programming in C++.
An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread&rsquo;s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task&rsquo;s result. The result of the delegated task is only required at some point in the future when it is needed.</p><p>An asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.</p><p align=center><img alt="Promise and future" src=images/image.png width=90%>
<em>Promise and future: worker gets the promise instance and main driver gets the future</em><br></p><p>When an asynchronous call is created, an instance of std::future is returned to the creator of the asynchronous call (receiver). Meanwhile, the executor of the asynchronous call (sender) holds an instance of std::promise. The executor uses std::promise to fulfill its promise (a commitment to deliver the result at a future point in time after the operation is completed), while the creator uses std::future to obtain this future value (the result corresponding to the promise fulfilled in the future). The std::promise instance held by the executor and the std::future instance held by the creator are both connected to a shared object. This shared object establishes a communication channel for information synchronization between the creator and the executor of the asynchronous call, enabling both parties to exchange information about the execution status of the asynchronous operation through this channel.</p><p>The executor accesses this channel via its std::promise instance to write values into the channel. The creator uses its std::future instance to retrieve values from the channel. Once the executor completes the execution of the asynchronous operation, it writes the result of the operation into the channel via the std::promise instance. The creator then retrieves the result of the asynchronous operation through its std::future instance.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;iostream&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;thread&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;future&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;vector&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;numeric&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;chrono&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// åˆ›å»ºä¸€ä¸ªpromiseå¯¹è±¡å®ä¾‹
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>promise<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span> _promise;
</span></span><span style=display:flex><span>    <span style=color:#75715e>// ä»promiseå¯¹è±¡å®ä¾‹ä¸­è·å–å¯¹åº”çš„futureå¯¹è±¡å®ä¾‹
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>future<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span> _future <span style=color:#f92672>=</span> _promise.get_future();
</span></span><span style=display:flex><span>    <span style=color:#75715e>// æ„å»ºæµ‹è¯•å¯¹è±¡
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span> test_data <span style=color:#f92672>=</span> {<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>};
</span></span><span style=display:flex><span>    <span style=color:#75715e>// åˆ›å»ºä¸€ä¸ªä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡å¯¹å®¹å™¨å†…çš„æ‰€æœ‰å…ƒç´ æ±‚å’Œï¼Œæ±‚å’Œå®Œæˆä¹‹åé€šè¿‡promiseæ¥è¿”å›ç»“æœ
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>auto</span> sum_of_data <span style=color:#f92672>=</span> [](<span style=color:#66d9ef>const</span> std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;&amp;</span> data, std<span style=color:#f92672>::</span>promise<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span> prom) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>int</span> sum <span style=color:#f92672>=</span> std<span style=color:#f92672>::</span>accumulate(data.begin(), data.end(), <span style=color:#ae81ff>0</span>);
</span></span><span style=display:flex><span>        std<span style=color:#f92672>::</span>this_thread<span style=color:#f92672>::</span>sleep_for(std<span style=color:#f92672>::</span>chrono<span style=color:#f92672>::</span>milliseconds(<span style=color:#ae81ff>500</span>)); <span style=color:#75715e>// ä¼‘çœ 500ms
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        prom.set_value(sum); <span style=color:#75715e>// å®Œæˆæ‰¿è¯ºï¼Œå°†ç»“æœå†™å…¥é€šé“ä¸­
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    };
</span></span><span style=display:flex><span>    <span style=color:#75715e>// å°†è¿™ä¸ªä»»åŠ¡äº¤ç»™ä¸€ä¸ªçº¿ç¨‹ï¼Œè¿›è¡Œå¼‚æ­¥æ‰§è¡Œ
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span><span style=color:#66d9ef>thread</span> work_thread(sum_of_data, test_data, std<span style=color:#f92672>::</span>move(_promise));
</span></span><span style=display:flex><span>    <span style=color:#75715e>// é€šè¿‡futureçš„æ–¹æ³•æ¥è·å–å¼‚æ­¥è°ƒç”¨çš„ç»“æœ
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>cout <span style=color:#f92672>&lt;&lt;</span> <span style=color:#e6db74>&#34;async result is &#34;</span> <span style=color:#f92672>&lt;&lt;</span> _future.get() <span style=color:#f92672>&lt;&lt;</span> std<span style=color:#f92672>::</span>endl;
</span></span><span style=display:flex><span>    <span style=color:#75715e>// ç­‰å¾…work_threadé€€å‡ºé”€æ¯å®Œæˆ
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    work_thread.join();
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// output
</span></span></span><span style=display:flex><span><span style=color:#75715e>// async result is 21
</span></span></span></code></pre></div><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Ray was designed with training RL system in mind. There are three kinds of tasks in RL system training</p><ul><li>Simulation where agents interact with environments and environments omit responses</li><li>Evaluation where agents generate rollout trajectory</li><li>Training where policy is updated for improvement</li></ul><p>To handle these heterogenous tasks, Ray abstracts two kinds of computation:</p><ul><li>Stateless task: functions decorated with <code>@ray.remote</code> in python</li><li>Stateful actor: classes decorated with <code>@ray.remote</code> in python</li></ul><p>Below is one example of task computation in Ray.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Ray task example</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>func</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># driver process</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># create 2 workers, execute func remotely, return 2 futures which each points to a remote op</span>
</span></span><span style=display:flex><span>    futures <span style=color:#f92672>=</span> [func<span style=color:#f92672>.</span>remote(i) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>2</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># blocking</span>
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get(futures)) 
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;The final result is: </span><span style=color:#e6db74>{</span>results<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># [0, 1]</span>
</span></span></code></pre></div><h2 id=ray-actor>Ray Actor<a hidden class=anchor aria-hidden=true href=#ray-actor>#</a></h2><p>To declare an actor, we can annotate a class with @ray.remote, just like declaring a task from a function. Ray actor is different from python class in the following ways:</p><blockquote><ol><li>Add accessor methods for any data members that you need to read or write, because using direct access, such as my_game.state, doesn&rsquo;t work for actors.</li><li>Construct actor instances with my_instance = MyClass.remote(&mldr;).</li><li>Call methods with my_instance.some_method.remote(&mldr;).</li><li>Use ray.get() and ray.wait() to retrieve results, just like you do for task results.</li></ol></blockquote><p>Another way to get an Ray actor is through <code>ray.remote(myClass)</code>. Similarly, we can instantiate the ray actor using <code>myClass().remote()</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> ray
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Counter</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>value <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>increment</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>value <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>value
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># define the accessor as we can&#39;t use direct access</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_counter</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>value
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create an actor from this class.</span>
</span></span><span style=display:flex><span>counter <span style=color:#f92672>=</span> Counter<span style=color:#f92672>.</span>remote()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Call the actor.</span>
</span></span><span style=display:flex><span>obj_ref <span style=color:#f92672>=</span> counter<span style=color:#f92672>.</span>increment<span style=color:#f92672>.</span>remote()
</span></span><span style=display:flex><span>print(ray<span style=color:#f92672>.</span>get(obj_ref))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create ten Counter actors.</span>
</span></span><span style=display:flex><span>counters <span style=color:#f92672>=</span> [Counter<span style=color:#f92672>.</span>remote() <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Increment each Counter once and get the results. These tasks all happen in</span>
</span></span><span style=display:flex><span><span style=color:#75715e># parallel.</span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get([c<span style=color:#f92672>.</span>increment<span style=color:#f92672>.</span>remote() <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> counters])
</span></span><span style=display:flex><span>print(results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Increment the first Counter five times. These tasks are executed serially</span>
</span></span><span style=display:flex><span><span style=color:#75715e># and share state.</span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get([counters[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>increment<span style=color:#f92672>.</span>remote() <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>5</span>)])
</span></span><span style=display:flex><span>print(results)
</span></span></code></pre></div><h3 id=ray-architecture>Ray Architecture<a hidden class=anchor aria-hidden=true href=#ray-architecture>#</a></h3><p>Ray is a general-purpose framework for parallel programming on a cluster.</p><p align=center><img alt="Ray Architecture" src=images/ray_architecture.png width=90%>
Ray Architecture<br></p><h2 id=ray-tips>Ray Tips<a hidden class=anchor aria-hidden=true href=#ray-tips>#</a></h2><h3 id=delay-rayget>Delay ray.get()<a hidden class=anchor aria-hidden=true href=#delay-rayget>#</a></h3><p>With Ray, the invocation of every remote operation (e.g., task, actor method) is asynchronous. This means that the operation returns <em>immediately</em> a promise/future, which is essentially an identifier (ID) of the operationâ€™s result. This is key to achieve parallelism, as it allows the driver program to launch multiple operations in parallel. To get the actual results, the programmer needs to call ray.get() on the IDs of the results. This call blocks until the results are available. As a side effect, this operation also blocks the driver program from invoking other operations, which can hurt parallelism.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>do_some_work</span>(x):
</span></span><span style=display:flex><span>    time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>1</span>) <span style=color:#75715e># Replace this with work you need to do.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> [do_some_work(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>results = &#34;</span>, results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Above output, the program takes around 4 ses:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 4.0149290561676025 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># results =  [0, 1, 2, 3]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> [do_some_work<span style=color:#f92672>.</span>remote(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>results = &#34;</span>, results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Above output, </span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 0.0003619194030761719 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># results =  [ObjectID(0100000000bdf683fc3e45db42685232b19d2a61), ObjectID(01000000da69c40e1c2f43b391443ce23de46cda), ObjectID(010000007fe0954ac2b3c0ab991538043e8f37e0), ObjectID(01000000cf47d5ecd1e26b42624454c795abe89b)]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> [ray<span style=color:#f92672>.</span>get(do_some_work<span style=color:#f92672>.</span>remote(x)) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>results = &#34;</span>, results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Above output, </span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 4.018050909042358 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># results =  [0, 1, 2, 3]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get([do_some_work<span style=color:#f92672>.</span>remote(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)])
</span></span><span style=display:flex><span><span style=color:#75715e># output</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 1.0064549446105957 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># results =  [0, 1, 2, 3]</span>
</span></span></code></pre></div><p>Note that <code>ray.get()</code> is blocking, so calling it after each remote operation means that we wait for that operation to complete, which essentially means that we execute one operation at a time, hence no parallelism!
To enable parallelism, we need to call ray.get() <em>after</em> invoking all tasks. We can easily do so in our example by replacing line â€œresults = [do_some_work.remote(x) for x in range(4)]â€ with:</p><h3 id=avoid-passing-same-object-repeatedly-to-remote-tasks>Avoid passing same object repeatedly to remote tasks<a hidden class=anchor aria-hidden=true href=#avoid-passing-same-object-repeatedly-to-remote-tasks>#</a></h3><p>When we pass a large object as an argument to a remote function, Ray calls ray.put() under the hood to store that object in the local object store. This can significantly improve the performance of a remote task invocation when the remote task is executed locally, as all local tasks share the object store. However, there are cases when automatically calling ray.put() on a task invocation leads to performance issues. One example is passing the same large object as an argument <em>repeatedly</em>, as illustrated by the program below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init(num_cpus <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>no_work</span>(a): 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time() 
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>2000</span>)) 
</span></span><span style=display:flex><span>result_ids <span style=color:#f92672>=</span> [no_work<span style=color:#f92672>.</span>remote(a) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)] 
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get(result_ids) 
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start) 
</span></span><span style=display:flex><span><span style=color:#75715e># output</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 1.0699057579040527 </span>
</span></span></code></pre></div><p>The right way is to put the shared object in the object store as shown below.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init(num_cpus <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>no_work</span>(a): 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time() 
</span></span><span style=display:flex><span>a_id <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>put(np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>2000</span>))) 
</span></span><span style=display:flex><span>result_ids <span style=color:#f92672>=</span> [no_work<span style=color:#f92672>.</span>remote(a_id) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>)] 
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get(result_ids) 
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start) 
</span></span><span style=display:flex><span><span style=color:#75715e># output</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 0.12425804138183594</span>
</span></span></code></pre></div><h3 id=pipeline-data-processing>Pipeline data processing<a hidden class=anchor aria-hidden=true href=#pipeline-data-processing>#</a></h3><p>If we use ray.get() on the results of multiple tasks we will have to wait until the <em>last</em> one of these tasks finishes. This can be an issue if tasks take widely different amounts of time. To illustrate this issue, consider the following example where we run four do_some_work() tasks in parallel, with each task taking a time uniformly distributed between 0 and 4 sec. Next, assume the results of these tasks are processed by process_results(), which takes 1 sec per result. The expected running time is then (1) the time it takes to execute the slowest of the do_some_work() tasks, plus (2) 4 sec which is the time it takes to execute process_results().</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init(num_cpus <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>do_some_work</span>(x): 
</span></span><span style=display:flex><span>    time<span style=color:#f92672>.</span>sleep(random<span style=color:#f92672>.</span>uniform(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>4</span>)) <span style=color:#75715e># Replace this with work you need to do. </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process_results</span>(results): 
</span></span><span style=display:flex><span>    sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> results: 
</span></span><span style=display:flex><span>        time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>1</span>) <span style=color:#75715e># Replace this with some processing code. </span>
</span></span><span style=display:flex><span>        sum <span style=color:#f92672>+=</span> x 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sum 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time() 
</span></span><span style=display:flex><span>data_list <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>get([do_some_work<span style=color:#f92672>.</span>remote(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)]) 
</span></span><span style=display:flex><span>sum <span style=color:#f92672>=</span> process_results(data_list) 
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>result = &#34;</span>, sum) 
</span></span><span style=display:flex><span><span style=color:#75715e># output</span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 7.82636022567749</span>
</span></span><span style=display:flex><span><span style=color:#75715e># result = 6</span>
</span></span></code></pre></div><p>In order to process data as soon as it&rsquo;s available, we can use the pipeline to remove the waiting time. Ray allows us to do exactly this by calling ray.wait() on a list of object IDs. Without specifying any other parameters, this function returns as soon as an object in its argument list is ready. This call has two returns: (1) the ID of the ready object, and (2) the list containing the IDs of the objects not ready yet. The modified program is below.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> ray 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>init(num_cpus <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@ray.remote</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>do_some_work</span>(x): 
</span></span><span style=display:flex><span>    time<span style=color:#f92672>.</span>sleep(random<span style=color:#f92672>.</span>uniform(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>4</span>)) <span style=color:#75715e># Replace this with work you need to do. </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process_incremental</span>(sum, result): 
</span></span><span style=display:flex><span>    time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>1</span>) <span style=color:#75715e># Replace this with some processing code. </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sum <span style=color:#f92672>+</span> result 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time() 
</span></span><span style=display:flex><span>result_ids <span style=color:#f92672>=</span> [do_some_work<span style=color:#f92672>.</span>remote(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>)] 
</span></span><span style=display:flex><span>sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> len(result_ids): 
</span></span><span style=display:flex><span>    done_id, result_ids <span style=color:#f92672>=</span> ray<span style=color:#f92672>.</span>wait(result_ids) 
</span></span><span style=display:flex><span>    sum <span style=color:#f92672>=</span> process_incremental(sum, ray<span style=color:#f92672>.</span>get(done_id[<span style=color:#ae81ff>0</span>])) 
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;duration =&#34;</span>, time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>result = &#34;</span>, sum) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#output </span>
</span></span><span style=display:flex><span><span style=color:#75715e># duration = 4.852453231811523 </span>
</span></span><span style=display:flex><span><span style=color:#75715e># result = 6 </span>
</span></span></code></pre></div><p align=center><img alt="Pipeline execution" src=images/ray_pipeline.png width=90%>
Pipeline execution, image from 1<br></p><p>A full example on how to use Ray for distributed neural network training</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision <span style=color:#f92672>import</span> datasets
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision.transforms <span style=color:#f92672>import</span> ToTensor
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray <span style=color:#f92672>import</span> train
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.train.torch <span style=color:#f92672>import</span> TorchTrainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ray.train <span style=color:#f92672>import</span> ScalingConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_dataset</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> datasets<span style=color:#f92672>.</span>FashionMNIST(
</span></span><span style=display:flex><span>        root<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/tmp/data&#34;</span>,
</span></span><span style=display:flex><span>        train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        transform<span style=color:#f92672>=</span>ToTensor(),
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NeuralNetwork</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>flatten <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Flatten()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear_relu_stack <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>28</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>512</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>512</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>10</span>),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, inputs):
</span></span><span style=display:flex><span>        inputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>flatten(inputs)
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear_relu_stack(inputs)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> logits
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># without distributed training, pure pytorch</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_func</span>():
</span></span><span style=display:flex><span>    num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dataset <span style=color:#f92672>=</span> get_dataset()
</span></span><span style=display:flex><span>    dataloader <span style=color:#f92672>=</span> DataLoader(dataset, batch_size<span style=color:#f92672>=</span>batch_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> NeuralNetwork()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> inputs, labels <span style=color:#f92672>in</span> dataloader:
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>            pred <span style=color:#f92672>=</span> model(inputs)
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> criterion(pred, labels)
</span></span><span style=display:flex><span>            loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;epoch: </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>item()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># train_func()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># distributed training</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_func_distributed</span>():
</span></span><span style=display:flex><span>    num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dataset <span style=color:#f92672>=</span> get_dataset()
</span></span><span style=display:flex><span>    dataloader <span style=color:#f92672>=</span> DataLoader(dataset, batch_size<span style=color:#f92672>=</span>batch_size)
</span></span><span style=display:flex><span>    dataloader <span style=color:#f92672>=</span> train<span style=color:#f92672>.</span>torch<span style=color:#f92672>.</span>prepare_data_loader(dataloader)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> NeuralNetwork()
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> train<span style=color:#f92672>.</span>torch<span style=color:#f92672>.</span>prepare_model(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> inputs, labels <span style=color:#f92672>in</span> dataloader:
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>            pred <span style=color:#f92672>=</span> model(inputs)
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> criterion(pred, labels)
</span></span><span style=display:flex><span>            loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;epoch: </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>item()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># For GPU Training, set `use_gpu` to True.</span>
</span></span><span style=display:flex><span>use_gpu <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> TorchTrainer(
</span></span><span style=display:flex><span>    train_func_distributed,
</span></span><span style=display:flex><span>    scaling_config<span style=color:#f92672>=</span>ScalingConfig(num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, use_gpu<span style=color:#f92672>=</span>use_gpu)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> trainer<span style=color:#f92672>.</span>fit()
</span></span></code></pre></div><h2 id=ray-debugging>Ray Debugging<a hidden class=anchor aria-hidden=true href=#ray-debugging>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># check ray status, if seeing connection error</span>
</span></span><span style=display:flex><span>ray status
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># stop existing ray server</span>
</span></span><span style=display:flex><span>ray stop
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># start new ray server</span>
</span></span><span style=display:flex><span>ray start --head 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># multi-node training</span>
</span></span><span style=display:flex><span><span style=color:#75715e># on head node</span>
</span></span><span style=display:flex><span>ray start --head --dashboard-host<span style=color:#f92672>=</span>0.0.0.0
</span></span><span style=display:flex><span>ray start --head --dashboard-host<span style=color:#f92672>=</span>0.0.0.0 --num-gpus <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span><span style=color:#75715e># on worker node</span>
</span></span><span style=display:flex><span>ray start --address<span style=color:#f92672>=</span>&lt;address&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># submit a job</span>
</span></span><span style=display:flex><span>ray job submit --address<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://127.0.0.1:8265&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--runtime-env<span style=color:#f92672>=</span>my_runtime_env.yaml <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>python3 my_script.py <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>
</span></span><span style=display:flex><span><span style=color:#75715e># to test the setup</span>
</span></span><span style=display:flex><span>ray job submit --address http://localhost:8265 -- python -c <span style=color:#e6db74>&#34;import ray; ray.init(); print(ray.cluster_resources())&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># sometime the error is not that obvious, we can go check ray logs here, e.g.</span>
</span></span><span style=display:flex><span>tail -f /tmp/ray/session_latest/logs/dashboard*
</span></span><span style=display:flex><span>less /tmp/ray/session_latest/logs/dashboard*
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># list all ray jobs</span>
</span></span><span style=display:flex><span>ray job list
</span></span></code></pre></div><p>There is another way to stop ray</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> ray
</span></span><span style=display:flex><span>ray<span style=color:#f92672>.</span>shutdown()
</span></span></code></pre></div><p>There are two ways to initialize ray with environment variables.</p><ul><li>set env variables before ray initialization. It will pick up the env variables at startup.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>read -r -d <span style=color:#e6db74>&#39;&#39;</span> my_config_json <span style=color:#e6db74>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#e6db74>{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;working_dir&#34;: &#34;${WORK_DIR}&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;env_vars&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;PYTHONPATH&#34;: &#34;${SRC_DIR}:$PYTHONPATH&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;LD_LIBRARY_PATH&#34;: &#34;my_ld_path:$LD_LIBRARY_PATH&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># we can use the above config in Ray</span>
</span></span><span style=display:flex><span>ray job submit --address<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;http://127.0.0.1:8265&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--runtime-env-json<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>my_config_json<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>-- python xx
</span></span></code></pre></div><ul><li>set the envs in python codes. Generally, this is a better approach. Because we can easily set environment variables running bash script.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> ray<span style=color:#f92672>.</span>is_initialized():
</span></span><span style=display:flex><span><span style=color:#75715e># this is for local ray cluster</span>
</span></span><span style=display:flex><span>    ray<span style=color:#f92672>.</span>init(runtime_env<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;env_vars&#39;</span>: {<span style=color:#e6db74>&#39;TOKENIZERS_PARALLELISM&#39;</span>: <span style=color:#e6db74>&#39;true&#39;</span>, <span style=color:#e6db74>&#39;NCCL_DEBUG&#39;</span>: <span style=color:#e6db74>&#39;WARN&#39;</span>}})
</span></span></code></pre></div><h3 id=logging>Logging<a hidden class=anchor aria-hidden=true href=#logging>#</a></h3><p>Normally in ray program logs, it might show</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>1234-pid dp_rank<span style=color:#f92672>=</span>1, <span style=color:#f92672>[</span>repeated 7x across cluster<span style=color:#f92672>]</span>
</span></span></code></pre></div><p>Ray by default would not print all the logs because it might be overwhelming in the output. When we debug distributed training/inference, if we want to print all ranks, we can do</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export RAY_LOG_TO_STDERR<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># we can see stdout of worker using</span>
</span></span><span style=display:flex><span>ray logs &lt;worker_id&gt;
</span></span></code></pre></div><p>Sometimes printing might be lost if we don&rsquo;t flush timely, in python we can do</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>dp_rank<span style=color:#e6db74>=}</span><span style=color:#e6db74>&#34;</span>, flush<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>Or we can set</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># env</span>
</span></span><span style=display:flex><span>export PYTHONUNBUFFERED<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># or</span>
</span></span><span style=display:flex><span>python -u my_script.py
</span></span></code></pre></div><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://rise.cs.berkeley.edu/blog/ray-tips-for-first-time-users/>https://rise.cs.berkeley.edu/blog/ray-tips-for-first-time-users/</a></li><li><a href=https://arxiv.org/abs/1712.05889>Ray: A Distributed Framework for Emerging AI Applications</a></li><li><a href=https://github.com/dmatrix/ray-core-tutorial>https://github.com/dmatrix/ray-core-tutorial</a></li><li><a href="https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview?tab=t.0#heading=h.iyrm5j2gcdoq">Ray Whitepaper</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/ml/loss/loss_reduction/><span class=title>Â«</span><br><span>Loss Reduction</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml/multimodality/vlm/><span class=title>Â»</span><br><span>VLM</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Ray on twitter" href="https://twitter.com/intent/tweet/?text=Ray&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fray%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Ray on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fray%2f&amp;title=Ray&amp;summary=Ray&amp;source=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fray%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Ray on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fray%2f&title=Ray"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Ray on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fray%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Ray on whatsapp" href="https://api.whatsapp.com/send?text=Ray%20-%20https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fray%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Ray on telegram" href="https://telegram.me/share/url?text=Ray&amp;url=https%3a%2f%2frich-junwang.github.io%2fen-us%2fposts%2ftech%2fml_infra%2fray%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>