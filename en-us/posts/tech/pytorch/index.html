<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pytorch Multiple-GPU Training | Jun's Blog</title>
<meta name=keywords content><meta name=description content="PyTorch.."><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/pytorch/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/pytorch/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><meta property="og:title" content="Pytorch Multiple-GPU Training"><meta property="og:description" content="PyTorch.."><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/pytorch/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-01-11T00:18:23+08:00"><meta property="article:modified_time" content="2021-01-11T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Pytorch Multiple-GPU Training"><meta name=twitter:description content="PyTorch.."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Pytorch Multiple-GPU Training","item":"https://rich-junwang.github.io/en-us/posts/tech/pytorch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pytorch Multiple-GPU Training","name":"Pytorch Multiple-GPU Training","description":"PyTorch..","keywords":[""],"articleBody":"Using PyTorch for NN model training on single GPU is simple and easy. However, when it comes to multiple GPU training, there could be various of issues. In this blog, Iâ€™ll summarize all kinds of issues I ran into during model training/evaluation.\nLoading a pretrained checkpoint A lot of times when we save a checkpoint of a pretrained model, we also save the trainer (or model state) information. This means when we load model checkpoint again, model will already have a preallocated device. When we use the same number of GPU to continue training, it will work as expected. However, the issue will arise when we have different number of GPUs for two runs. Letâ€™s say, we first trained model on a single GPU, then we want to use multiple GPU to continue the training. When we move model to multiple GPU, there will be something weird. For instance, on GPU 0, you might see multiple process (normally one process per GPU). Or in other cases, you can see GPU 0 has much higher memory usage than other GPUs.\nSolution: when we load model, we only load parameters and strip all state information. This might be tricky sometimes. The simplest way to solve this issue is to wrap the command with with PyTorch distributed data parallel.\npython3 -m torch.distributed.launch --nnodes=1 --nproc_per_node=8 my_script.py my_config_file Install Apex Sometimes to use the latest distributed training feature, we have to install Apex. As Apex is closely coupled with Cuda, we need to follow the next few steps to correctlly install apex.\nFind out the Cuda version used in the system. python -c \"import torch; print(torch.version.cuda)\" Install from source git clone https://github.com/NVIDIA/apex cd apex CUDA_HOME=/usr/local/cuda-{your-version-here}/ pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./ Commonly Used Pytorch Tricks Distributed training is error-prone, so effective ways of debugging is needed. Here I document some of these commands\n# print the whole tensor torch.set_printoptions(profile=\"full\") torch.set_printoptions(linewidth=16000) Dataloader sometimes can be buggy, when there are errors related to dataloader, a good practice is to disable the worker number and disable prefetching.\nLaunch Distributed Run python3 -m torch.distributed.run --nnodes=2 --nproc_per_node 8 --node_rank=${NODE_RANK} --master_port=1234 --master_addr=xxx train.py args.. Pytorch and Numpy Advanced Indexing When selection object is sequence object, ndarray/tensor, it will trigger advanced indexing. To understand how it works, we start from simple.\nx = np.arange(12).reshape(4,3) print(x) #output [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]] (1) Specify integer arrays in each dimension where every element in the array represents a number of indices into that dimension. In the example below, we select (0, 0), (1, 1), (2, 0) elements from the above array. x has two dimensions so we have two arrays to specify the indices in each dimension.\ny = x[[0,1,2], [0,1,0]] print(y) #[0 4 6] (2) The above way of indexing only renders single dimension result. We can use multi-dimension array to get multi-dimension output. Below is one of these examples. This is to select [(0, 0), (0, 2)], [(3, 0), (3, 2)] elements. Note that in each dimension we still only select one index, like 0 from row-dim, and 0 from col-dim.\nrows = np.array([[0,0],[3,3]]) cols = np.array([[0,2],[0,2]]) y = x[rows,cols] print (y) # output [[ 0 2] [ 9 11]] ","wordCount":"541","inLanguage":"en-us","datePublished":"2021-01-11T00:18:23+08:00","dateModified":"2021-01-11T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/pytorch/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>Pytorch Multiple-GPU Training</h1><div class=post-description>PyTorch..</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2021-01-11
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>541 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>2 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/blog/ style=color:var(--secondary)!important>Blog</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#loading-a-pretrained-checkpoint aria-label="Loading a pretrained checkpoint">Loading a pretrained checkpoint</a></li><li><a href=#install-apex aria-label="Install Apex">Install Apex</a></li><li><a href=#commonly-used-pytorch-tricks aria-label="Commonly Used Pytorch Tricks">Commonly Used Pytorch Tricks</a></li><li><a href=#launch-distributed-run aria-label="Launch Distributed Run">Launch Distributed Run</a></li><li><a href=#pytorch-and-numpy-advanced-indexing aria-label="Pytorch and Numpy Advanced Indexing">Pytorch and Numpy Advanced Indexing</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Using PyTorch for NN model training on single GPU is simple and easy. However, when it comes to multiple GPU training, there could be various of issues. In this blog, I&rsquo;ll summarize all kinds of issues I ran into during model training/evaluation.</p><h3 id=loading-a-pretrained-checkpoint>Loading a pretrained checkpoint<a hidden class=anchor aria-hidden=true href=#loading-a-pretrained-checkpoint>#</a></h3><p>A lot of times when we save a checkpoint of a pretrained model, we also save the trainer (or model state) information. This means when we load model checkpoint again, model will already have a preallocated device. When we use the same number of GPU to continue training, it will work as expected. However, the issue will arise when we have different number of GPUs for two runs. Let&rsquo;s say, we first trained model on a single GPU, then we want to use multiple GPU to continue the training. When we move model to multiple GPU, there will be something weird. For instance, on GPU 0, you might see multiple process (normally one process per GPU). Or in other cases, you can see GPU 0 has much higher memory usage than other GPUs.</p><p>Solution: when we load model, we only load parameters and strip all state information. This might be tricky sometimes. The simplest way to solve this issue is to wrap the command with with PyTorch distributed data parallel.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>python3 -m torch.distributed.launch --nnodes=1 --nproc_per_node=8 my_script.py my_config_file 
</span></span></code></pre></div><h3 id=install-apex>Install Apex<a hidden class=anchor aria-hidden=true href=#install-apex>#</a></h3><p>Sometimes to use the latest distributed training feature, we have to install Apex. As Apex is closely coupled with Cuda, we need to follow the next few steps to correctlly install apex.</p><ul><li>Find out the Cuda version used in the system.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>python -c &#34;import torch; print(torch.version.cuda)&#34;
</span></span></code></pre></div><ul><li>Install from source</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>git clone https://github.com/NVIDIA/apex
</span></span><span style=display:flex><span>cd apex
</span></span><span style=display:flex><span>CUDA_HOME=/usr/local/cuda-{your-version-here}/ pip install -v --disable-pip-version-check --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; ./
</span></span></code></pre></div><h3 id=commonly-used-pytorch-tricks>Commonly Used Pytorch Tricks<a hidden class=anchor aria-hidden=true href=#commonly-used-pytorch-tricks>#</a></h3><p>Distributed training is error-prone, so effective ways of debugging is needed. Here I document some of these commands</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span># print the whole tensor
</span></span><span style=display:flex><span>torch.set_printoptions(profile=&#34;full&#34;)
</span></span><span style=display:flex><span>torch.set_printoptions(linewidth=16000)
</span></span></code></pre></div><p>Dataloader sometimes can be buggy, when there are errors related to dataloader, a good practice is to disable the worker number and disable prefetching.</p><h3 id=launch-distributed-run>Launch Distributed Run<a hidden class=anchor aria-hidden=true href=#launch-distributed-run>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 -m torch.distributed.run --nnodes<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> --nproc_per_node <span style=color:#ae81ff>8</span> --node_rank<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NODE_RANK<span style=color:#e6db74>}</span> --master_port<span style=color:#f92672>=</span><span style=color:#ae81ff>1234</span> --master_addr<span style=color:#f92672>=</span>xxx train.py args..
</span></span></code></pre></div><h3 id=pytorch-and-numpy-advanced-indexing>Pytorch and Numpy Advanced Indexing<a hidden class=anchor aria-hidden=true href=#pytorch-and-numpy-advanced-indexing>#</a></h3><p>When selection object is sequence object, ndarray/tensor, it will trigger advanced indexing. To understand how it works, we start from simple.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>12</span>)<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>print(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#output</span>
</span></span><span style=display:flex><span>[[ <span style=color:#ae81ff>0</span>  <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span> [ <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>5</span>]
</span></span><span style=display:flex><span> [ <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>7</span>  <span style=color:#ae81ff>8</span>]
</span></span><span style=display:flex><span> [ <span style=color:#ae81ff>9</span> <span style=color:#ae81ff>10</span> <span style=color:#ae81ff>11</span>]]
</span></span></code></pre></div><p>(1) Specify integer arrays in each dimension where every element in the array represents a number of indices into that dimension. In the example below, we select (0, 0), (1, 1), (2, 0) elements from the above array. <code>x</code> has two dimensions so we have two arrays to specify the indices in each dimension.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>y = x[[0,1,2],  [0,1,0]]
</span></span><span style=display:flex><span>print(y) 
</span></span><span style=display:flex><span>#[0 4 6]
</span></span></code></pre></div><p>(2) The above way of indexing only renders single dimension result. We can use multi-dimension array to get multi-dimension output. Below is one of these examples. This is to select [(0, 0), (0, 2)], [(3, 0), (3, 2)] elements. Note that in each dimension we still only select one index, like 0 from row-dim, and 0 from col-dim.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>rows = np.array([[0,0],[3,3]]) 
</span></span><span style=display:flex><span>cols = np.array([[0,2],[0,2]])
</span></span><span style=display:flex><span>y = x[rows,cols]  
</span></span><span style=display:flex><span>print (y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># output
</span></span><span style=display:flex><span>[[ 0  2]
</span></span><span style=display:flex><span> [ 9 11]]
</span></span></code></pre></div></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/transformer/><span class=title>Â«</span><br><span>Transformer</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/moe_models/><span class=title>Â»</span><br><span>RL Basics</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2024
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>