<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Loss in ML | Jun's Blog</title>
<meta name=keywords content><meta name=description content="Losses in ML"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/loss/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/loss/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="Loss in ML"><meta property="og:description" content="Losses in ML"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/loss/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-18T00:18:23+08:00"><meta property="article:modified_time" content="2019-07-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Loss in ML"><meta name=twitter:description content="Losses in ML"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Loss in ML","item":"https://rich-junwang.github.io/en-us/posts/tech/loss/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Loss in ML","name":"Loss in ML","description":"Losses in ML","keywords":[""],"articleBody":"Sigmoid Sigmoid is one of the most used activation functions. $$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$ It has nice mathematical proprities: $$ \\sigma^\\prime(x) = \\sigma(x) \\left[ 1 - \\sigma(x) \\right] $$ and $$ \\left[log\\sigma(x)\\right]^\\prime = 1 - \\sigma(x) \\\\ \\left[log\\left(1 - \\sigma(x)\\right)\\right]^\\prime = - \\sigma(x) $$\nLogistic Regression For a binary classification problem, for an example $x = (x_1, x_2, \\dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as: $$ \\begin{aligned} h_{\\theta}(1|x) \u0026= \\sigma(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dotsb + \\theta_nx_n) \\\\ \u0026= \\sigma(\\theta^{\\mathrm{T}}x) \\\\ \u0026= \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\end{aligned} $$ Consequently, for the negative class, $$ \\begin{aligned} h_{\\theta}(0|x) \u0026= 1 - \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\\\ \u0026= \\frac{1}{1+e^{\\theta^{\\mathrm{T}}x}} \\\\ \u0026= \\sigma(-\\theta^{\\mathrm{T}}x) \\end{aligned} $$\nSingle sample cost function of logistic regression is expressed as: $$ L(\\theta) = -y_i \\cdot \\log(h_\\theta(x_i)) - (1-y_i) \\cdot \\log(1 - h_\\theta(x_i)) $$ Notice that in the second term $1 - h_\\theta(x_i)$ is the negative class probability\nCross Entropy Cross entropy defines the distance between model output distribution and the groudtruth distribution. $$ H(y,p) = -\\sum_{i}y_i \\log(p_i) $$ Since the $y_i$ is the class label (1 for positive class, 0 for negative), essentially here weâ€™re summing up the negative log probably of the positive label. What is the reason why we say that negative log likehood and cross entropy is equivalent.\nWhen normalization function (we can say activation function of last layer) is softmax function, namely, for each class $s_i$ the probability is given by $$ f(s)_i = \\frac{e^{s_i}}{ \\sum_j ^{C} e^{s_j}} $$ Given the above cross entropy equation, and there is only one positive class, the softmax cross entropy loss is: $$ L = -log(\\frac{e^{s_p}}{ \\sum_j ^{C} e^{s_j}}) $$ here $p$ stands for positive class. If we want to get the gradient of loss with respect to the logits ($s_i$ here), for positive class we can have $$ \\frac{\\partial{L}}{\\partial{s_p}} = \\left(\\frac{e^{s_p}}{ \\sum_j ^{C} e^{s_j}} - 1 \\right) \\\\ \\frac{\\partial{L}}{\\partial{s_n}} = \\left(\\frac{e^{s_n}}{ \\sum_j ^{C} e^{s_j}} - 1 \\right) $$ We can put this in one equation, which is what we commonly see as the graident of cross entropy loss for softmax $$ \\frac{\\partial{L}}{\\partial{s_i}} = p_i - y_i $$ $p_i$ is the probability and $y_i$ is the label, 1 for positive class and 0 for negative class.\nBinary Cross Entropy In the above section, we talked about softmax cross entropy loss, here we talk about binary cross entropy loss which is also called Sigmoid cross entropy loss.\nWe apply sigmoid function to the output logits before BCE. Notice that here we apply the function to each element in the tensor, all the elements are not related to each other, this is why BCE is widely used for multi-label classification task.\nFor each label, we can calculate the loss in the same way as the logistic regression loss.\nimport torch import numpy as np pred = np.array([[-0.4089, -1.2471, 0.5907], [-0.4897, -0.8267, -0.7349], [0.5241, -0.1246, -0.4751]]) # after sigmod, pred becomes # [[0.3992, 0.2232, 0.6435], # [0.3800, 0,3044, 0.3241], # [0.6281, 0.4689, 0.3834]] label = np.array([[0, 1, 1], [0, 0, 1], [1, 0, 1]]) # after cross entropy, pred becomes # [[-0.5095, -1.4997, -0.4408], take neg and avg 0.8167 # [-0.4780, -0.3630, -1.1267], take neg and avg 0.6559 # [-0.4651, -0.6328, -0.9587]] take neg and avg 0.6855 # 0 * ln0.3992 + (1-0) * ln(1-0.3992) = -0.5095 # (0.8167 + 0.6559 + 0.6855) / 3. = 0.7194 pred = torch.from_numpy(pred).float() label = torch.from_numpy(label).float() crition1 = torch.nn.BCEWithLogitsLoss() loss1 = crition1(pred, label) print(loss1) # 0.7193 crition2 = torch.nn.MultiLabelSoftMarginLoss() loss2 = crition2(pred, label) print(loss2) # 0.7193 Noise Contrastive Estimation Noise contrastive estimation or negative sampling is a commonly used computation trick in ML to deal with expansive softmax computation or intractable partition function in computation.\nThe derivation of NCE loss sometimes can be bewildering, but the idea is actually very simple. For example, in word2vec implementation, the negative sampling is to choose 1 positive target and 5 negative target, and calculate the binary cross entropy loss (binary logistic loss) and then do backward propagation.\nContrastive Loss CLIP Loss CLIP loss is the same with the paper from [3]. The negatives here are used for contrastive learning. However, theyâ€™re not using NCE method like word2vec. Itâ€™s more like softmax cross entropy.\nimport torch from torch import nn import torch.nn.functional as F import config as CFG from modules import ImageEncoder, TextEncoder, ProjectionHead class CLIPModel(nn.Module): def __init__( self, temperature=CFG.temperature, image_embedding=CFG.image_embedding, text_embedding=CFG.text_embedding, ): super().__init__() self.image_encoder = ImageEncoder() self.text_encoder = TextEncoder() self.image_projection = ProjectionHead(embedding_dim=image_embedding) self.text_projection = ProjectionHead(embedding_dim=text_embedding) self.temperature = temperature def forward(self, batch): # Getting Image and Text Features image_features = self.image_encoder(batch[\"image\"]) text_features = self.text_encoder( input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"] ) # Getting Image and Text Embeddings (with same dimension) image_embeddings = self.image_projection(image_features) text_embeddings = self.text_projection(text_features) # Calculating the Loss logits = (text_embeddings @ image_embeddings.T) / self.temperature images_similarity = image_embeddings @ image_embeddings.T texts_similarity = text_embeddings @ text_embeddings.T targets = F.softmax( (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1 ) texts_loss = cross_entropy(logits, targets, reduction='none') images_loss = cross_entropy(logits.T, targets.T, reduction='none') loss = (images_loss + texts_loss) / 2.0 # shape: (batch_size) return loss.mean() def cross_entropy(preds, targets, reduction='none'): log_softmax = nn.LogSoftmax(dim=-1) loss = (-targets * log_softmax(preds)).sum(1) if reduction == \"none\": return loss elif reduction == \"mean\": return loss.mean() if __name__ == '__main__': images = torch.randn(8, 3, 224, 224) input_ids = torch.randint(5, 300, size=(8, 25)) attention_mask = torch.ones(8, 25) batch = { 'image': images, 'input_ids': input_ids, 'attention_mask': attention_mask } CLIP = CLIPModel() loss = CLIP(batch) print(\"\") Reference Learning Transferable Visual Models From Natural Language Supervision http://www.cnblogs.com/peghoty/p/3857839.html contrastive learning of medical visual representations from paired images and text https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/CLIP.py ","wordCount":"914","inLanguage":"en-us","datePublished":"2019-06-18T00:18:23+08:00","dateModified":"2019-07-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/loss/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>Loss in ML</h1><div class=post-description>Losses in ML</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2019-06-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>914 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>2 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/ml/ style=color:var(--secondary)!important>ML</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#sigmoid aria-label=Sigmoid>Sigmoid</a></li><li><a href=#logistic-regression aria-label="Logistic Regression">Logistic Regression</a></li><li><a href=#cross-entropy aria-label="Cross Entropy">Cross Entropy</a></li><li><a href=#binary-cross-entropy aria-label="Binary Cross Entropy">Binary Cross Entropy</a></li><li><a href=#noise-contrastive-estimation aria-label="Noise Contrastive Estimation">Noise Contrastive Estimation</a></li><li><a href=#contrastive-loss aria-label="Contrastive Loss">Contrastive Loss</a></li><li><a href=#clip-loss aria-label="CLIP Loss">CLIP Loss</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h3 id=sigmoid>Sigmoid<a hidden class=anchor aria-hidden=true href=#sigmoid>#</a></h3><p>Sigmoid is one of the most used activation functions.
$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$
It has nice mathematical proprities:
$$
\sigma^\prime(x) = \sigma(x) \left[ 1 - \sigma(x) \right]
$$
and
$$
\left[log\sigma(x)\right]^\prime = 1 - \sigma(x) \\
\left[log\left(1 - \sigma(x)\right)\right]^\prime = - \sigma(x)
$$</p><h3 id=logistic-regression>Logistic Regression<a hidden class=anchor aria-hidden=true href=#logistic-regression>#</a></h3><p>For a binary classification problem, for an example $x = (x_1, x_2, \dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as:
$$
\begin{aligned}
h_{\theta}(1|x) &= \sigma(\theta_0 + \theta_1x_1 + \theta_2x_2 + \dotsb + \theta_nx_n) \\
&= \sigma(\theta^{\mathrm{T}}x) \\
&= \frac{1}{1+e^{-\theta^{\mathrm{T}}x}}
\end{aligned}
$$
Consequently, for the negative class,
$$
\begin{aligned}
h_{\theta}(0|x) &= 1 - \frac{1}{1+e^{-\theta^{\mathrm{T}}x}} \\
&= \frac{1}{1+e^{\theta^{\mathrm{T}}x}} \\
&= \sigma(-\theta^{\mathrm{T}}x)
\end{aligned}
$$</p><p>Single sample cost function of logistic regression is expressed as:
$$
L(\theta) = -y_i \cdot \log(h_\theta(x_i)) - (1-y_i) \cdot \log(1 - h_\theta(x_i))
$$
Notice that in the second term $1 - h_\theta(x_i)$ is the negative class probability</p><h3 id=cross-entropy>Cross Entropy<a hidden class=anchor aria-hidden=true href=#cross-entropy>#</a></h3><p>Cross entropy defines the distance between model output distribution and the groudtruth distribution.
$$
H(y,p) = -\sum_{i}y_i \log(p_i)
$$
Since the $y_i$ is the class label (1 for positive class, 0 for negative), essentially here we&rsquo;re summing up the negative log probably of the positive label. What is the reason why we say that negative log likehood and cross entropy is equivalent.</p><p>When normalization function (we can say activation function of last layer) is softmax function, namely, for each class $s_i$ the probability is given by
$$
f(s)_i = \frac{e^{s_i}}{ \sum_j ^{C} e^{s_j}}
$$
Given the above cross entropy equation, and there is only one positive class, the softmax cross entropy loss is:
$$
L = -log(\frac{e^{s_p}}{ \sum_j ^{C} e^{s_j}})
$$
here $p$ stands for positive class.
If we want to get the gradient of loss with respect to the logits ($s_i$ here), for positive class we can have
$$
\frac{\partial{L}}{\partial{s_p}} = \left(\frac{e^{s_p}}{ \sum_j ^{C} e^{s_j}} - 1 \right) \\
\frac{\partial{L}}{\partial{s_n}} = \left(\frac{e^{s_n}}{ \sum_j ^{C} e^{s_j}} - 1 \right)
$$
We can put this in one equation, which is what we commonly see as the graident of cross entropy loss for softmax
$$
\frac{\partial{L}}{\partial{s_i}} = p_i - y_i
$$
$p_i$ is the probability and $y_i$ is the label, 1 for positive class and 0 for negative class.</p><h3 id=binary-cross-entropy>Binary Cross Entropy<a hidden class=anchor aria-hidden=true href=#binary-cross-entropy>#</a></h3><p>In the above section, we talked about softmax cross entropy loss, here we talk about binary cross entropy loss which is also called Sigmoid cross entropy loss.</p><p>We apply sigmoid function to the output logits before BCE. Notice that here we apply the function to each element in the tensor, all the elements are not related to each other, this is why BCE is widely used for <strong>multi-label</strong> classification task.</p><p>For each label, we can calculate the loss in the same way as the logistic regression loss.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.4089</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.2471</span>, <span style=color:#ae81ff>0.5907</span>],
</span></span><span style=display:flex><span>                [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.4897</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8267</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7349</span>],
</span></span><span style=display:flex><span>                [<span style=color:#ae81ff>0.5241</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1246</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4751</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># after sigmod, pred becomes</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [[0.3992, 0.2232, 0.6435],</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [0.3800, 0,3044, 0.3241],</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [0.6281, 0.4689, 0.3834]]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>label <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>                  [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>                  [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># after cross entropy, pred becomes</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [[-0.5095, -1.4997, -0.4408], take neg and avg 0.8167</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [-0.4780, -0.3630, -1.1267], take neg and avg 0.6559</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [-0.4651, -0.6328, -0.9587]] take neg and avg 0.6855</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 0 * ln0.3992 + (1-0) * ln(1-0.3992) = -0.5095</span>
</span></span><span style=display:flex><span><span style=color:#75715e># (0.8167 + 0.6559 + 0.6855) / 3. = 0.7194</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pred <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(pred)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>label <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(label)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>crition1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>BCEWithLogitsLoss()
</span></span><span style=display:flex><span>loss1 <span style=color:#f92672>=</span> crition1(pred, label)
</span></span><span style=display:flex><span>print(loss1) <span style=color:#75715e># 0.7193</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>crition2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>MultiLabelSoftMarginLoss()
</span></span><span style=display:flex><span>loss2 <span style=color:#f92672>=</span> crition2(pred, label)
</span></span><span style=display:flex><span>print(loss2) <span style=color:#75715e># 0.7193</span>
</span></span></code></pre></div><h3 id=noise-contrastive-estimation>Noise Contrastive Estimation<a hidden class=anchor aria-hidden=true href=#noise-contrastive-estimation>#</a></h3><p>Noise contrastive estimation or negative sampling is a commonly used computation trick in ML to deal with expansive softmax computation or intractable partition function in computation.</p><p>The derivation of NCE loss sometimes can be bewildering, but the idea is actually very simple. For example, in word2vec implementation, the negative sampling is to choose 1 positive target and 5 negative target, and calculate the binary cross entropy loss (binary logistic loss) and then do backward propagation.</p><h3 id=contrastive-loss>Contrastive Loss<a hidden class=anchor aria-hidden=true href=#contrastive-loss>#</a></h3><h3 id=clip-loss>CLIP Loss<a hidden class=anchor aria-hidden=true href=#clip-loss>#</a></h3><p>CLIP loss is the same with the paper from [3]. The negatives here are used for contrastive learning. However, they&rsquo;re not using NCE method like word2vec. It&rsquo;s more like softmax cross entropy.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> config <span style=color:#66d9ef>as</span> CFG
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> modules <span style=color:#f92672>import</span> ImageEncoder, TextEncoder, ProjectionHead
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CLIPModel</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(
</span></span><span style=display:flex><span>        self,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span>CFG<span style=color:#f92672>.</span>temperature,
</span></span><span style=display:flex><span>        image_embedding<span style=color:#f92672>=</span>CFG<span style=color:#f92672>.</span>image_embedding,
</span></span><span style=display:flex><span>        text_embedding<span style=color:#f92672>=</span>CFG<span style=color:#f92672>.</span>text_embedding,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_encoder <span style=color:#f92672>=</span> ImageEncoder()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text_encoder <span style=color:#f92672>=</span> TextEncoder()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>image_projection <span style=color:#f92672>=</span> ProjectionHead(embedding_dim<span style=color:#f92672>=</span>image_embedding)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text_projection <span style=color:#f92672>=</span> ProjectionHead(embedding_dim<span style=color:#f92672>=</span>text_embedding)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>temperature <span style=color:#f92672>=</span> temperature
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, batch):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Getting Image and Text Features</span>
</span></span><span style=display:flex><span>        image_features <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>image_encoder(batch[<span style=color:#e6db74>&#34;image&#34;</span>])
</span></span><span style=display:flex><span>        text_features <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>text_encoder(
</span></span><span style=display:flex><span>            input_ids<span style=color:#f92672>=</span>batch[<span style=color:#e6db74>&#34;input_ids&#34;</span>], attention_mask<span style=color:#f92672>=</span>batch[<span style=color:#e6db74>&#34;attention_mask&#34;</span>]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># Getting Image and Text Embeddings (with same dimension)</span>
</span></span><span style=display:flex><span>        image_embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>image_projection(image_features)
</span></span><span style=display:flex><span>        text_embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>text_projection(text_features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Calculating the Loss</span>
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> (text_embeddings <span style=color:#f92672>@</span> image_embeddings<span style=color:#f92672>.</span>T) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>temperature
</span></span><span style=display:flex><span>        images_similarity <span style=color:#f92672>=</span> image_embeddings <span style=color:#f92672>@</span> image_embeddings<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>        texts_similarity <span style=color:#f92672>=</span> text_embeddings <span style=color:#f92672>@</span> text_embeddings<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>        targets <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(
</span></span><span style=display:flex><span>            (images_similarity <span style=color:#f92672>+</span> texts_similarity) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>temperature, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        texts_loss <span style=color:#f92672>=</span> cross_entropy(logits, targets, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;none&#39;</span>)
</span></span><span style=display:flex><span>        images_loss <span style=color:#f92672>=</span> cross_entropy(logits<span style=color:#f92672>.</span>T, targets<span style=color:#f92672>.</span>T, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;none&#39;</span>)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span>  (images_loss <span style=color:#f92672>+</span> texts_loss) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2.0</span> <span style=color:#75715e># shape: (batch_size)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy</span>(preds, targets, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;none&#39;</span>):
</span></span><span style=display:flex><span>    log_softmax <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LogSoftmax(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> (<span style=color:#f92672>-</span>targets <span style=color:#f92672>*</span> log_softmax(preds))<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> reduction <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;none&#34;</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> reduction <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;mean&#34;</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    images <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>)
</span></span><span style=display:flex><span>    input_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>300</span>, size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>25</span>))
</span></span><span style=display:flex><span>    attention_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>25</span>)
</span></span><span style=display:flex><span>    batch <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;image&#39;</span>: images,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;input_ids&#39;</span>: input_ids,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;attention_mask&#39;</span>: attention_mask
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    CLIP <span style=color:#f92672>=</span> CLIPModel()
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> CLIP(batch)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;&#34;</span>)
</span></span></code></pre></div><h3 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h3><ol><li><a href=https://arxiv.org/pdf/2103.00020.pdf>Learning Transferable Visual Models From Natural Language Supervision</a></li><li><a href=http://www.cnblogs.com/peghoty/p/3857839.html>http://www.cnblogs.com/peghoty/p/3857839.html</a></li><li><a href=https://arxiv.org/abs/2010.00747>contrastive learning of medical visual representations from paired images and text</a></li><li><a href=https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/CLIP.py>https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/CLIP.py</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/makefile/><span class=title>Â«</span><br><span>Makefile</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/git/git/><span class=title>Â»</span><br><span>Git</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>