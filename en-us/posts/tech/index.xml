<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>üë®üèª‚Äçüíª Tech on Jun&#39;s Blog</title>
    <link>https://rich-junwang.github.io/en-us/posts/tech/</link>
    <description>Recent content in üë®üèª‚Äçüíª Tech on Jun&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Dec 2022 00:18:23 +0800</lastBuildDate><atom:link href="https://rich-junwang.github.io/en-us/posts/tech/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Large Scale Pretraining</title>
      <link>https://rich-junwang.github.io/en-us/posts/tech/large_scale_pretraining/</link>
      <pubDate>Sun, 18 Dec 2022 00:18:23 +0800</pubDate>
      
      <guid>https://rich-junwang.github.io/en-us/posts/tech/large_scale_pretraining/</guid>
      <description>Large language model pretraining</description>
    </item>
    
    <item>
      <title>Redis scanÂëΩ‰ª§Â≠¶‰π†</title>
      <link>https://rich-junwang.github.io/en-us/posts/tech/tech1/</link>
      <pubDate>Wed, 30 Nov 2022 15:55:23 +0800</pubDate>
      
      <guid>https://rich-junwang.github.io/en-us/posts/tech/tech1/</guid>
      <description>scanÂëΩ‰ª§ËØ¶Ëß£</description>
    </item>
    
    <item>
      <title>Distributed Training Infra</title>
      <link>https://rich-junwang.github.io/en-us/posts/tech/distributed_training/</link>
      <pubDate>Thu, 05 May 2022 00:18:23 +0800</pubDate>
      
      <guid>https://rich-junwang.github.io/en-us/posts/tech/distributed_training/</guid>
      <description>Distributed training infrastructure</description>
    </item>
    
    <item>
      <title>Tech</title>
      <link>https://rich-junwang.github.io/en-us/posts/tech/tech/</link>
      <pubDate>Thu, 05 May 2022 00:17:58 +0800</pubDate>
      
      <guid>https://rich-junwang.github.io/en-us/posts/tech/tech/</guid>
      <description>ML/Coding/AI</description>
    </item>
    
    <item>
      <title>Shell command for my reference</title>
      <link>https://rich-junwang.github.io/en-us/posts/tech/shell-commands/</link>
      <pubDate>Fri, 11 Feb 2022 00:18:23 +0800</pubDate>
      
      <guid>https://rich-junwang.github.io/en-us/posts/tech/shell-commands/</guid>
      <description>Some of my frequently used commands</description>
    </item>
    
    <item>
      <title>Parallelism in LLM Training</title>
      <link>https://rich-junwang.github.io/en-us/posts/tech/parallelism/</link>
      <pubDate>Tue, 08 Feb 2022 12:01:14 -0700</pubDate>
      
      <guid>https://rich-junwang.github.io/en-us/posts/tech/parallelism/</guid>
      <description>Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we&amp;rsquo;ll dive deep into parallel training in recent distributed training paradigms. A lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We&amp;rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency. Data Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is</description>
    </item>
    
    <item>
      <title>SSH Connection</title>
      <link>https://rich-junwang.github.io/en-us/posts/tech/ssh/</link>
      <pubDate>Tue, 11 Jan 2022 00:18:23 +0800</pubDate>
      
      <guid>https://rich-junwang.github.io/en-us/posts/tech/ssh/</guid>
      <description>How to always keep ssh connection alive</description>
    </item>
    
  </channel>
</rss>
