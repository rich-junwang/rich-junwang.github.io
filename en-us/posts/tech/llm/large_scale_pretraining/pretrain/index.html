<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large Scale Pretraining | Jun's Blog</title>
<meta name=keywords content><meta name=description content="Large language model pretraining"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/llm/large_scale_pretraining/pretrain/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/llm/large_scale_pretraining/pretrain/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="Large Scale Pretraining"><meta property="og:description" content="Large language model pretraining"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/llm/large_scale_pretraining/pretrain/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-18T00:18:23+08:00"><meta property="article:modified_time" content="2022-12-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large Scale Pretraining"><meta name=twitter:description content="Large language model pretraining"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"üë®üèª‚Äçüíª Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"Large Scale Pretraining","item":"https://rich-junwang.github.io/en-us/posts/tech/llm/large_scale_pretraining/pretrain/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large Scale Pretraining","name":"Large Scale Pretraining","description":"Large language model pretraining","keywords":[""],"articleBody":"Large language model pretraining is a very challenging task which requires very strong engineering and science skills. People tend to underestimate efforts needed to train a good large model like GPT3 etc. Most people imagine that they can get decent language models given enough computation resources. The fact is even today only OpenAI is providing LM APIs where people can freely play with and get good performances. In this blog, we‚Äôll talk about pretraining from the whole pipeline: data sourcing, collection and processing, tokenization, architecture engineering and evaluation. Hopefully, it would be helpful for foundational model training practitioners.\nData Data is crucial in any ML system. This is true to pretraining as well. As is shown in Gopher paper, a large, diverse and high-quality dataset is needed to train a good model. In the following table, it shows the datasets used in Gopher model training. Now we‚Äôre looking at terabytes scale of training data.\nDatasets used in Gopher [2] An ensuing problem with large amount of data is that data quality is hard to control. In practice, we have to at least make sure the content should be intelligible. We might want to give more training on high-quality datasets such as books and wikipedia [31]. Diversified datasets are necessary but can't guarantee training success as can be seen from `Gopher` paper, model performs well on QA related tasks but suffers on reasoning task. What else is needed? We'll come back to this later. Toxicity Filter There is experiment shows that toxicity filter could have a big impact on model performance. In [37], the authors proposed that instead of using a toxicity filter, inverse toxicity filtering is more helpful. Inverse toxicity filter removes the LEAST toxicity data from training data.\nTokenizer Language models compute probability of any string sequence. How to represent the string sequence is determined by tokenizer. Popular options are byte pair encoding (BPE) or wordpiece. As the majority of models are using BPE today, here we focus on BPE based tokenizer. Tokenizer can impact several things in LLM training: (1) a high compression rate (tokenized token number vs raw token number, the lower the better). Compression rate affects input context length and inference speed. (2) Vocab size. An appropriately sized vocabulary to ensure adequate training of each word embedding.\nAs mentioned in GPT2 paper, BPE effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences. Directly using greedy method to build BPE merging rules can be problematic. For example, word cat can be used in a lot of places like cat?, cat!, cat.. One way to solve this issue is to prevent BPE from generating rules across different character categories (letters, digits, puncts etc).\nAs people are pivoting in-context learning/instruction learning with large models, tokenization efficiency becomes more important. The following tables from Jurassic-1 paper shows the efficiency of tokenizer on several public dataset.\nTokenizer efficiency comparison from [16] Tokenizer determines the size of vocab. Usually when we support multilinguality and code data, the vocab size will be much larger. However, this is not always the case. CodeLLaMa shows very good performance (onpar with GPT4) with a vocab size of 32k. When vocab is too large, some of the tokens may not be trained enough. When vocab size is too small, the compression rate might be limited.\nCompression rate determines the input sequence length to the model. With high compression rate, the input length is shorter. Short sequence length might be able to mitigate exposure bias to some extent.\nOpen tokenizer implementations are: tiktoken.\nModel Architecture All pretrained models are variant of original transformer model. The differences are mainly about it‚Äôs encoder-decoder architecture or decoder-only architecture. First of all, let‚Äôs take a look at the choices of available large models.\nModels Model Size Token Size Architecture GPT3 175B 300B Decoder OPT 175B 300B Decoder PaLM 540B 780B Decoder Gopher 280B 300B Decoder Chinchilla 70B 1400B Decoder Jurassic-1 178B - Decoder Megatron-Turing NLG 530B 270B Decoder LaMDA 137B 2810B Decoder Although all models listed here are auto-regressive decoder only model, they actually differ a bit inside the decoder. For instance, to speed up inference time, PaLM is using multi-query attention. Normally, in multi-head attention, there will be h heads each with a linear project layer for Q, K, V. With multiquery attention, instead of using h different linear project layers for K and V, we can share a single smaller linear project layer for K and a single linear projection layer for V for each head. Then, for different head layers, K and V will be the same. In this way, we can save memory IO and get better latency performance in incremental inference. To speed up training, people also proposed parallel layer architecture as shown below.\nModel Architecture A systematic study of transformer architecture is done in Ref [29]. Most of recent LLM architecture are following design from this paper. People usually call the embedding dim as the width of transformer and number of layers as the depth. There is a optimal depth-to-width allocation for a given self-attention network size as is shown in [34].\nTraining Design Most of today‚Äôs pretraining follow suits of a multi-stage and multi-task training. As is shown by Yao in [1], GPT series model is pretrained in such way as well.\nGPT Model Lineage. Image from [1] From the lineage diagram, we can see that ChatGPT model comes from Codex model which can be seen as a different stage of training. The way of scheduling tasks and data during training can have great impact on the final model performance.\nBatch Size Research [5] shows that there is a critical batch size in pretraining. When training batch size exceeds critical batch size, model performance starts to degrade. Critical batch size is independent of model size and is related to loss.\nGenerally small batch size leads to better validation loss when training with the same number of tokens as more random movement of gradient explores more of loss landscape. Often times, small batch size gives better generalization performance as well as pointed out in [27]. The reason given from the paper is that smaller batch size usually converges to flat minimum as oppose to sharp minimum. Intuitively, this is related to graident update in each step is small for large batch size training.\nFlat and Sharp Minima [27] Learning Rate Scheduling Usually as pointed out in [20], when we scale up batch size, we increase learning rate propotionally. However, when we increase model size (usually followed with batch size increase), the training tends to be more unstable. Thus, in reality, we decrease maximum learning rate when we increase model size (batch size).\nLearning rate scheduling usually involves a (linear) warm-up step to maximum learning rate and followed by a decaying step to 0 or a minimum learning rate. Currently, there are several methods in literature for the decaying step:\nLinear scheduler Plateau-linear schedule Cosine scheduler Regularization One of the most used regularization method is L2 regularization, aka, weight decay [28]. For instance, GPT 3 training uses a weight decay of 0.1. Note that comparing with traditional neural network tuning weight decay number (such as 0.01) GPT3 weight decay is pretty large.\nLength Extrapolation As in-context learning becomes popular, people are asking a question, Can an LLM maintain equally good, if not better, perplexities when longer sequences are used during inference time? This is the so-called length extrapolation [25].\nOptimizer When we select an optimizer, we have to take consideration of memory footprint and stability issues etc. Options are Adafactor, Adam etc. According to Gopher paper, adafactor optimizer has smaller memory footprint, and on smaller scale model (\u003c7B) adafactor works well. However, when model size goes larger, performance suffers because of stability issue.\nEvaluation A lot of large models come out every year and many claims that they could beat GPT3 model in a wide range of benchmarks like SuperGlue, CLUE, MMLU etc. However, when you do benchmark these models in zero-shot setting or some less common tasks (but still very reasonable ones), these models tend to perform really bad. I personally tested GPT3 model (175b) and UL2 model (20b) on text2sql and sql2text task, GPT3 gives way better performance to the extent that you‚Äôll believe UL2 is like garbage. The similar thing happened in evaluation in [24]. You may argue that the model size differs a lot. However, we can think the other way around: the results they claim better than GPT3 is also got from a smaller model and maybe their model training is not easy/efficient to scale to such level. Essentially, what I want to say is that good performance on popular benchmark datasets doesn‚Äôt mean much for large LM pretraining as this is highly related to source of training data, whether or not doing fine-tuning, proper prompting etc. Human evaluation is what really matters.\nStability During the model training, the most commonly seen issue is gradient exploding, aka, gradient becomes NaN. As layers go deeper, this problem happens more often because the way backpropagation works. Over the years, people have proposed many different ways to solve the challenge. As is shown in paper [21], the post-LN shows stability issue without carefully designed warming-up stage. As a result, they are proposing pre-LN to alleviate the problem.\nThe objective function for highly nonlinear deep neural networks often contains sharp nonlinearities in parameter space resulting from the multiplication of several parameters. These nonlinearities give rise to very high derivatives in some places. When the parameters get close to such a cliff region, a gradient descent update can catapult the parameters very far, possibly losing most of the optimization work that had been done [33].\nIt‚Äôs important to monitor stability during training. Common practice is to plot activation norm and gradient norm for each step. When these values spike, we know there is something wrong. It‚Äôs better than looking at loss curve only as loss explosion generally lags behind these two indicators. For instance, when there is bad data, we could have better gauge of when that happens and restart training from that point.\nAdept AI has a lengthy blog post talking about hardware error induced stability issue. The blog mentioned two ways to identify erroneous node(s):\nGrid search: partition nodes into groups and train model on each group in a deterministic way. Find the one that has different training loss curve. Parameter checksum check: for each data parallel run, check parameter checksum to see if they are the same to determine which stage might be wrong. PaLM paper proposed adding z-loss to increase the stability of training. The method is to encourage the logits to stay close to zero. For example, we could add a max-z loss to normalize the logits: $$ L_{max\\_z} = 2e^{-4} * z^2 $$ where $z$ is the maximum logit value [39].\nEfficient Inference Inference speed determines product cost. Over the years, people have proposed various ways to improve inference speed. The multiquery attention mentioned above is one of these approaches. References [1] How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources [2] Gopher: Scaling Language Models: Methods, Analysis \u0026 Insights from Training Gopher [3] UL2: Unifying Language Learning Paradigms [4] Bloom: Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model [5] Scaling Laws for Neural Language Models [6] GPT: Improving Language Understanding by Generative Pre-Training [7] GPT2: Language Models are Unsupervised Multitask Learners [8] GPT3: Language Models are Few-Shot Learners [9] InstructGPT: Training language models to follow instructions with human feedback [10] WebGPT: Browser-assisted question-answering with human feedback [11] OPT: Open Pre-trained Transformer Language Models [12] OPT2: OPT-IML Scaling Language Model Instruction Meta Learning through the Lens of Generalization [13] PaLM: Scaling Language Modeling with Pathways [14] Flan-PaLM: Scaling Instruction-Finetuned Language Models [15] Chinchilla: Training Compute-Optimal Large Language Models [16] Jurassic-1: Technical details and evaluation. [17] Megatron-NLG: Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [18] LaMDA: Language Models for Dialog Applications [19] Codex: Evaluating Large Language Models Trained on Code [20] Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [21] On Layer Normalization in the Transformer Architecture [22] GLM-130B: An Open Bilingual Pre-trained Model [23] T0: Multitask Prompted Training Enables Zero-Shot Task Generalization [24] https://zhuanlan.zhihu.com/p/590240010 [25] RoFormer: Enhanced Transformer with Rotary Position Embedding [26] Receptive Field Alignment Enables Transformer Length Extrapolation [27] On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima [28] Decoupled Weight Decay Regularization [29] Do Transformer Modifications Transfer Across Implementations and Applications? [30] xFormers: A modular and hackable Transformer modelling library [31] LLaMA: Open and Efficient Foundation Language Models [32] What Language Model to Train if You Have One Million GPU Hours? [33] On the difficulty of training Recurrent Neural Networks [34] Limits to Depth-Efficiencies of Self-Attention [35] Baichuan LLM [36] Qwen LLM [37] A Pretrainer‚Äôs Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, \u0026 Toxicity [38] GLaM: Efficient Scaling of Language Models with Mixture-of-Experts [39] Baichuan 2: Open Large-scale Language Models [40] The Falcon Series of Open Language Models [41] Simplifying Transformer Blocks [42] What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? ","wordCount":"2201","inLanguage":"en-us","datePublished":"2022-12-18T00:18:23+08:00","dateModified":"2022-12-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/llm/large_scale_pretraining/pretrain/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="üè† Home"><span>üè† Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="üôãüèª‚Äç‚ôÇÔ∏è About"><span>üôãüèª‚Äç‚ôÇÔ∏è About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="üìö Posts"><span>üìö Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="üß© Tags"><span>üß© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="‚è±Ô∏è Archives"><span>‚è±Ô∏è Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="üîç Search (Alt + /)" accesskey=/><span>üîç Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>üè† Home</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>üìöArticles</a>&nbsp;¬ª&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>üë®üèª‚Äçüíª Tech</a></div><h1 class=post-title>Large Scale Pretraining</h1><div class=post-description>Large language model pretraining</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2022-12-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>2201 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>5 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/llm/ style=color:var(--secondary)!important>LLM</a>
&nbsp;<a href=https://rich-junwang.github.io/en-us/tags/ml/ style=color:var(--secondary)!important>ML</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#data aria-label=Data>Data</a><ul><li><a href=#toxicity-filter aria-label="Toxicity Filter">Toxicity Filter</a></li></ul></li><li><a href=#tokenizer aria-label=Tokenizer>Tokenizer</a></li><li><a href=#model-architecture aria-label="Model Architecture">Model Architecture</a></li><li><a href=#training-design aria-label="Training Design">Training Design</a><ul><li><a href=#batch-size aria-label="Batch Size">Batch Size</a></li><li><a href=#learning-rate-scheduling aria-label="Learning Rate Scheduling">Learning Rate Scheduling</a></li><li><a href=#regularization aria-label=Regularization>Regularization</a></li><li><a href=#length-extrapolation aria-label="Length Extrapolation">Length Extrapolation</a></li><li><a href=#optimizer aria-label=Optimizer>Optimizer</a></li></ul></li><li><a href=#evaluation aria-label=Evaluation>Evaluation</a></li><li><a href=#stability aria-label=Stability>Stability</a></li><li><a href=#efficient-inference aria-label="Efficient Inference">Efficient Inference</a></li></ul><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Large language model pretraining is a very challenging task which requires very strong engineering and science skills. People tend to underestimate efforts needed to train a good large model like GPT3 etc. Most people imagine that they can get decent language models given enough computation resources. The fact is even today only OpenAI is providing LM APIs where people can freely play with and get good performances. In this blog, we&rsquo;ll talk about pretraining from the whole pipeline: data sourcing, collection and processing, tokenization, architecture engineering and evaluation. Hopefully, it would be helpful for foundational model training practitioners.</p><h3 id=data>Data<a hidden class=anchor aria-hidden=true href=#data>#</a></h3><p>Data is crucial in any ML system. This is true to pretraining as well. As is shown in Gopher paper, a large, diverse and high-quality dataset is needed to train a good model. In the following table, it shows the datasets used in <a href=https://arxiv.org/pdf/2112.11446.pdf><code>Gopher</code> model</a> training. Now we&rsquo;re looking at terabytes scale of training data.</p><p align=center><img alt="gopher dataset" src=images/gopher_data.png width=80%><br><em>Datasets used in Gopher [2]</em><br></p>An ensuing problem with large amount of data is that data quality is hard to control. In practice, we have to at least make sure the content should be intelligible. We might want to give more training on high-quality datasets such as books and wikipedia [31]. Diversified datasets are necessary but can't guarantee training success as can be seen from `Gopher` paper, model performs well on QA related tasks but suffers on reasoning task. What else is needed? We'll come back to this later.<h4 id=toxicity-filter>Toxicity Filter<a hidden class=anchor aria-hidden=true href=#toxicity-filter>#</a></h4><p>There is experiment shows that toxicity filter could have a big impact on model performance. In [37], the authors proposed that instead of using a toxicity filter, inverse toxicity filtering is more helpful. Inverse toxicity filter removes the LEAST toxicity data from training data.</p><h3 id=tokenizer>Tokenizer<a hidden class=anchor aria-hidden=true href=#tokenizer>#</a></h3><p>Language models compute probability of any string sequence. How to represent the string sequence is determined by tokenizer. Popular options are byte pair encoding (BPE) or wordpiece. As the majority of models are using BPE today, here we focus on BPE based tokenizer. Tokenizer can impact several things in LLM training: (1) a high compression rate (tokenized token number vs raw token number, the lower the better). Compression rate affects input context length and inference speed. (2) Vocab size. An appropriately sized vocabulary to ensure adequate training of each word embedding.</p><p>As mentioned in GPT2 paper, BPE effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences. Directly using greedy method to build BPE merging rules can be problematic. For example, word <code>cat</code> can be used in a lot of places like <code>cat?</code>, <code>cat!</code>, <code>cat.</code>. One way to solve this issue is to prevent BPE from generating rules across different character categories (letters, digits, puncts etc).</p><p>As people are pivoting in-context learning/instruction learning with large models, tokenization efficiency becomes more important. The following tables from Jurassic-1 paper shows the efficiency of tokenizer on several public dataset.</p><p align=center><img alt="tokenization efficiency" src=images/tokenizer.png width=100%>
<em>Tokenizer efficiency comparison from [16]</em><br></p><p>Tokenizer determines the size of vocab. Usually when we support multilinguality and code data, the vocab size will be much larger. However, this is not always the case. CodeLLaMa shows very good performance (onpar with GPT4) with a vocab size of 32k. When vocab is too large, some of the tokens may not be trained enough. When vocab size is too small, the compression rate might be limited.</p><p>Compression rate determines the input sequence length to the model. With high compression rate, the input length is shorter. Short sequence length might be able to mitigate exposure bias to some extent.</p><p>Open tokenizer implementations are: <a href=https://github.com/openai/tiktoken>tiktoken</a>.</p><h3 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h3><p>All pretrained models are variant of original transformer model. The differences are mainly about it&rsquo;s encoder-decoder architecture or decoder-only architecture. First of all, let&rsquo;s take a look at the choices of available large models.</p><table><thead><tr><th>Models</th><th>Model Size</th><th>Token Size</th><th>Architecture</th><th></th></tr></thead><tbody><tr><td>GPT3</td><td>175B</td><td>300B</td><td>Decoder</td><td></td></tr><tr><td>OPT</td><td>175B</td><td>300B</td><td>Decoder</td><td></td></tr><tr><td>PaLM</td><td>540B</td><td>780B</td><td>Decoder</td><td></td></tr><tr><td>Gopher</td><td>280B</td><td>300B</td><td>Decoder</td><td></td></tr><tr><td>Chinchilla</td><td>70B</td><td>1400B</td><td>Decoder</td><td></td></tr><tr><td>Jurassic-1</td><td>178B</td><td>-</td><td>Decoder</td><td></td></tr><tr><td>Megatron-Turing NLG</td><td>530B</td><td>270B</td><td>Decoder</td><td></td></tr><tr><td>LaMDA</td><td>137B</td><td>2810B</td><td>Decoder</td><td></td></tr></tbody></table><p>Although all models listed here are auto-regressive decoder only model, they actually differ a bit inside the decoder. For instance, to speed up inference time, PaLM is using multi-query attention. Normally, in multi-head attention, there will be h heads each with a linear project layer for Q, K, V. With multiquery attention, instead of using h different linear project layers for K and V, we can share a single smaller linear project layer for K and a single linear projection layer for V for each head. Then, for different head layers, K and V will be the same. In this way, we can save memory IO and get better latency performance in incremental inference. To speed up training, people also proposed parallel layer architecture as shown below.</p><p align=center><img alt=transformer src=images/transformer.png width=80% height=auto/>
<em>Model Architecture</em><br></p>A systematic study of transformer architecture is done in Ref [29]. Most of recent LLM architecture are following design from this paper.<p>People usually call the embedding dim as the width of transformer and number of layers as the depth. There is a optimal depth-to-width
allocation for a given self-attention network size as is shown in [34].</p><h3 id=training-design>Training Design<a hidden class=anchor aria-hidden=true href=#training-design>#</a></h3><p>Most of today&rsquo;s pretraining follow suits of a multi-stage and multi-task training. As is shown by Yao in [1], GPT series model is pretrained in such way as well.</p><p align=center><img alt="gopher dataset" src=images/gpt-lineage.png width=80% height=auto/><br><em>GPT Model Lineage. Image from [1]</em><br></p><p>From the lineage diagram, we can see that <code>ChatGPT</code> model comes from <code>Codex</code> model which can be seen as a different stage of training. The way of scheduling tasks and data during training can have great impact on the final model performance.</p><h4 id=batch-size>Batch Size<a hidden class=anchor aria-hidden=true href=#batch-size>#</a></h4><p>Research [5] shows that there is a critical batch size in pretraining. When training batch size exceeds critical batch size, model performance starts to degrade. Critical batch size is independent of model size and is related to loss.</p><p>Generally small batch size leads to better validation loss when training with the same number of tokens as more random movement of gradient explores more of loss landscape. Often times, small batch size gives better generalization performance as well as pointed out in [27]. The reason given from the paper is that smaller batch size usually converges to flat minimum as oppose to sharp minimum. Intuitively, this is related to graident update in each step is small for large batch size training.</p><p align=center><img alt="flat sharp minimum" src=images/flat_sharp_minimum.png width=80% height=auto/><br><em>Flat and Sharp Minima [27]</em><br></p><h4 id=learning-rate-scheduling>Learning Rate Scheduling<a hidden class=anchor aria-hidden=true href=#learning-rate-scheduling>#</a></h4><p>Usually as pointed out in [20], when we scale up batch size, we increase learning rate propotionally. However, when we increase model size (usually followed with batch size increase), the training tends to be more unstable. Thus, in reality, we decrease maximum learning rate when we increase model size (batch size).</p><p>Learning rate scheduling usually involves a (linear) warm-up step to maximum learning rate and followed by a decaying step to 0 or a minimum learning rate. Currently, there are several methods in literature for the decaying step:</p><ul><li>Linear scheduler</li><li>Plateau-linear schedule</li><li>Cosine scheduler</li></ul><h4 id=regularization>Regularization<a hidden class=anchor aria-hidden=true href=#regularization>#</a></h4><p>One of the most used regularization method is L2 regularization, aka, weight decay [28]. For instance, GPT 3 training uses a weight decay of 0.1. Note that comparing with traditional neural network tuning weight decay number (such as 0.01) GPT3 weight decay is pretty large.</p><h4 id=length-extrapolation>Length Extrapolation<a hidden class=anchor aria-hidden=true href=#length-extrapolation>#</a></h4><p>As in-context learning becomes popular, people are asking a question, Can an LLM maintain equally good, if not better, perplexities when longer sequences are used during inference time? This is the so-called length extrapolation [25].</p><h4 id=optimizer>Optimizer<a hidden class=anchor aria-hidden=true href=#optimizer>#</a></h4><p>When we select an optimizer, we have to take consideration of memory footprint and stability issues etc. Options are <strong>Adafactor</strong>, <strong>Adam</strong> etc. According to <strong>Gopher</strong> paper, adafactor optimizer has smaller memory footprint, and on smaller scale model (&lt;7B) adafactor works well. However, when model size goes larger, performance suffers because of stability issue.</p><h3 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h3><p>A lot of large models come out every year and many claims that they could beat GPT3 model in a wide range of benchmarks like <code>SuperGlue</code>, <code>CLUE</code>, <code>MMLU</code> etc. However, when you do benchmark these models in zero-shot setting or some less common tasks (but still very reasonable ones), these models tend to perform really bad. I personally tested <code>GPT3</code> model (175b) and <code>UL2</code> model (20b) on text2sql and sql2text task, GPT3 gives way better performance to the extent that you&rsquo;ll believe UL2 is like garbage. The similar thing happened in evaluation in [24]. You may argue that the model size differs a lot. However, we can think the other way around: the results they claim better than GPT3 is also got from a smaller model and maybe their model training is not easy/efficient to scale to such level. Essentially, what I want to say is that good performance on popular benchmark datasets doesn&rsquo;t mean much for large LM pretraining as this is highly related to source of training data, whether or not doing fine-tuning, proper prompting etc. Human evaluation is what really matters.</p><h3 id=stability>Stability<a hidden class=anchor aria-hidden=true href=#stability>#</a></h3><p>During the model training, the most commonly seen issue is gradient exploding, aka, gradient becomes <code>NaN</code>. As layers go deeper, this problem happens more often because the way backpropagation works. Over the years, people have proposed many different ways to solve the challenge.
As is shown in paper [21], the post-LN shows stability issue without carefully designed warming-up stage. As a result, they are proposing pre-LN to alleviate the problem.</p><p>The objective function for highly nonlinear deep neural networks often contains sharp nonlinearities in parameter space resulting from the multiplication of several parameters. These nonlinearities give rise to very high derivatives in some places. When the parameters get close to such a cliff region, a gradient descent update can catapult the parameters very far, possibly losing most of the optimization work that had been done [33].</p><p>It&rsquo;s important to monitor stability during training. Common practice is to plot activation norm and gradient norm for each step. When these values spike, we know there is something wrong. It&rsquo;s better than looking at loss curve only as loss explosion generally lags behind these two indicators. For instance, when there is bad data, we could have better gauge of when that happens and restart training from that point.</p><p>Adept AI has a lengthy <a href=https://www.adept.ai/blog/sherlock-sdc>blog post</a> talking about hardware error induced stability issue. The blog mentioned two ways to identify erroneous node(s):</p><ul><li>Grid search: partition nodes into groups and train model on each group in a deterministic way. Find the one that has different training loss curve.</li><li>Parameter checksum check: for each data parallel run, check parameter checksum to see if they are the same to determine which stage might be wrong.</li></ul><p>PaLM paper proposed adding z-loss to increase the stability of training. The method is to encourage the logits to stay close to zero. For example, we could add a max-z loss to normalize the logits:
$$
L_{max\_z} = 2e^{-4} * z^2
$$
where $z$ is the maximum logit value [39].</p><h3 id=efficient-inference>Efficient Inference<a hidden class=anchor aria-hidden=true href=#efficient-inference>#</a></h3><p>Inference speed determines product cost. Over the years, people have proposed various ways to improve inference speed. The multiquery attention mentioned above is one of these approaches.<br></p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <a href=https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1>How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources</a><br>[2] <a href=https://arxiv.org/pdf/2112.11446.pdf>Gopher: Scaling Language Models: Methods, Analysis & Insights from Training Gopher</a><br>[3] <a href=https://arxiv.org/pdf/2205.05131.pdf>UL2: Unifying Language Learning Paradigms</a><br>[4] <a href=https://arxiv.org/abs/2211.02001>Bloom: Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model</a><br>[5] <a href=https://arxiv.org/abs/2001.08361>Scaling Laws for Neural Language Models</a><br>[6] <a href=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf>GPT: Improving Language Understanding by Generative Pre-Training</a><br>[7] <a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>GPT2: Language Models are Unsupervised Multitask Learners</a><br>[8] <a href=https://arxiv.org/abs/2005.14165>GPT3: Language Models are Few-Shot Learners</a><br>[9] <a href=https://arxiv.org/abs/2203.02155>InstructGPT: Training language models to follow instructions with human feedback</a><br>[10] <a href=https://arxiv.org/abs/2112.09332>WebGPT: Browser-assisted question-answering with human feedback</a><br>[11] <a href=https://arxiv.org/abs/2205.01068>OPT: Open Pre-trained Transformer Language Models</a><br>[12] <a href=https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML>OPT2: OPT-IML Scaling Language Model Instruction Meta Learning through the Lens of Generalization</a><br>[13] <a href=https://arxiv.org/abs/2204.02311>PaLM: Scaling Language Modeling with Pathways</a><br>[14] <a href=https://arxiv.org/pdf/2210.11416.pdf>Flan-PaLM: Scaling Instruction-Finetuned Language Models</a><br>[15] <a href=https://arxiv.org/abs/2203.15556>Chinchilla: Training Compute-Optimal Large Language Models</a><br>[16] <a href=https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf>Jurassic-1: Technical details and evaluation.</a><br>[17] <a href=https://arxiv.org/abs/2201.11990>Megatron-NLG: Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model</a><br>[18] <a href=https://arxiv.org/pdf/2201.08239.pdf>LaMDA: Language Models for Dialog Applications</a><br>[19] <a href=https://arxiv.org/abs/2107.03374>Codex: Evaluating Large Language Models Trained on Code</a><br>[20] <a href=https://arxiv.org/abs/1706.02677>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a><br>[21] <a href=https://arxiv.org/abs/2002.04745>On Layer Normalization in the Transformer Architecture</a><br>[22] <a href=https://arxiv.org/abs/2210.02414>GLM-130B: An Open Bilingual Pre-trained Model</a><br>[23] <a href=https://arxiv.org/abs/2110.08207>T0: Multitask Prompted Training Enables Zero-Shot Task Generalization</a><br>[24] <a href=https://zhuanlan.zhihu.com/p/590240010>https://zhuanlan.zhihu.com/p/590240010</a><br>[25] <a href=https://arxiv.org/abs/2104.09864>RoFormer: Enhanced Transformer with Rotary Position Embedding</a><br>[26] <a href=https://arxiv.org/abs/2212.10356>Receptive Field Alignment Enables Transformer Length Extrapolation</a><br>[27] <a href=https://arxiv.org/abs/1609.04836>On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a><br>[28] <a href=https://arxiv.org/pdf/1711.05101.pdf>Decoupled Weight Decay Regularization</a><br>[29] <a href=https://arxiv.org/abs/2102.11972>Do Transformer Modifications Transfer Across Implementations and Applications?</a><br>[30] <a href=https://github.com/facebookresearch/xformers>xFormers: A modular and hackable Transformer modelling library</a><br>[31] <a href="https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/333078981_693988129081760_4712707815225756708_n.pdf?_nc_cat=108&amp;ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=4srK2r5szdYAX8pFEBs&_nc_ht=scontent-sea1-1.xx&amp;oh=00_AfBU6VS0w7YtW_0wD4YO2NbJg-fXXaFGrRh6jEr8Z73xDg&amp;oe=6407B8A2">LLaMA: Open and Efficient Foundation Language Models</a><br>[32] <a href=https://arxiv.org/abs/2210.15424>What Language Model to Train if You Have One Million GPU Hours?</a><br>[33] <a href=https://arxiv.org/pdf/1211.5063.pdf>On the difficulty of training Recurrent Neural Networks</a><br>[34] <a href=https://papers.nips.cc/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf>Limits to Depth-Efficiencies of Self-Attention</a><br>[35] <a href=https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf>Baichuan LLM</a><br>[36] <a href=https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md>Qwen LLM</a><br>[37] <a href=https://arxiv.org/abs/2305.13169>A Pretrainer&rsquo;s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity</a><br>[38] <a href=https://arxiv.org/pdf/2112.06905.pdf>GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</a><br>[39] <a href=https://arxiv.org/pdf/2309.10305.pdf>Baichuan 2: Open Large-scale Language Models</a><br>[40] <a href=https://arxiv.org/abs/2311.16867>The Falcon Series of Open Language Models</a><br>[41] <a href=https://arxiv.org/abs/2311.01906>Simplifying Transformer Blocks</a><br>[42] <a href=https://arxiv.org/abs/2204.05832>What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</a><br></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/llm/instructgpt/><span class=title>¬´</span><br><span>InstructGPT and ChatGPT</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/ml_infra/kubenetes/basics/><span class=title>¬ª</span><br><span>Kubernetes</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'üëâComment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'üëáCollapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href,s=window.getSelection().toString()+`\r

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r
ÁâàÊùÉÂ£∞ÊòéÔºöÊú¨Êñá‰∏∫„ÄåJun's Blog„ÄçÁöÑÂéüÂàõÊñáÁ´†ÔºåÈÅµÂæ™CC 4.0 BY-SAÁâàÊùÉÂçèËÆÆÔºåËΩ¨ËΩΩËØ∑ÈôÑ‰∏äÂéüÊñáÂá∫Â§ÑÈìæÊé•ÂèäÊú¨Â£∞Êòé„ÄÇ\r
ÂéüÊñáÈìæÊé•Ôºö`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>