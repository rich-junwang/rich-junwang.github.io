<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MoE Models | Jun's Blog</title>
<meta name=keywords content><meta name=description content="MoE Models"><meta name=author content="Jun"><link rel=canonical href=https://rich-junwang.github.io/en-us/posts/tech/llm/moe_models/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=16x16 href=https://rich-junwang.github.io/img/Q.gif><link rel=icon type=image/png sizes=32x32 href=https://rich-junwang.github.io/img/Q.gif><link rel=apple-touch-icon href=https://rich-junwang.github.io/img/Q.gif><link rel=mask-icon href=https://rich-junwang.github.io/img/Q.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://rich-junwang.github.io/en-us/posts/tech/llm/moe_models/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js></script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:title" content="MoE Models"><meta property="og:description" content="MoE Models"><meta property="og:type" content="article"><meta property="og:url" content="https://rich-junwang.github.io/en-us/posts/tech/llm/moe_models/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-18T00:18:23+08:00"><meta property="article:modified_time" content="2023-10-18T00:18:23+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="MoE Models"><meta name=twitter:description content="MoE Models"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ğŸ“šArticles","item":"https://rich-junwang.github.io/en-us/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ‘¨ğŸ»â€ğŸ’» Tech","item":"https://rich-junwang.github.io/en-us/posts/tech/"},{"@type":"ListItem","position":3,"name":"MoE Models","item":"https://rich-junwang.github.io/en-us/posts/tech/llm/moe_models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MoE Models","name":"MoE Models","description":"MoE Models","keywords":[""],"articleBody":"The biggest lesson weâ€™ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today weâ€™ll closely examine the Mixtral model to study MoE models.\nIntroduction Most of todayâ€™s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:\nFigure 1. Switch MoE Model In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.\nThe follow code snippet shows how it works for Mixtral MoE model at inference time.\n# æ³¨æ„ï¼šä¸ºäº†å®¹æ˜“ç†è§£ï¼Œæˆ‘å¯¹ä»£ç è¿›è¡Œäº†ç®€åŒ–ï¼ŒåŒæ—¶ä¸è€ƒè™‘batch sizeï¼Œå®é™…ä½¿ç”¨æ—¶è¿˜æ˜¯è¦ç”¨å®˜æ–¹ä»£ç  class MixtralSparseMoeBlock(nn.Module): def __init__(self, config): super().__init__() self.gate = nn.Linear(self.hidden_dim, 8) self.experts = nn.ModuleList([MLP(config) for _ in range(8)]) def forward(self, x): # å¯¹æ¯ä¸ªtokenè®¡ç®—8ä¸ªexpertçš„æƒé‡ï¼Œå¹¶å°†æƒé‡å½’ä¸€åŒ– router_logits = self.gate(x) routing_weights = F.softmax(router_logits, dim=1) # æ¯ä¸ªtokené€‰æ‹©top-2 expertsçš„æƒé‡ã€ç´¢å¼•ï¼Œ å¹¶å°†ç´¢å¼•è½¬ä¸ºsize=(len(tokens), 8)çš„ç‹¬çƒ­ç¼–ç  routing_weights, selected_experts = torch.top2(routing_weights, dim=-1) expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=8) # é‡æ–°å°†top-2 expertçš„æƒé‡å½’ä¸€åŒ–ï¼ˆå› ä¸ºåˆ æ‰å…¶ä½™6ä¸ªexpertï¼Œæƒé‡çš„å’Œä¸ç­‰äº1äº†ï¼‰ routing_weights /= routing_weights.sum(dim=-1) # åˆ›å»ºå½¢çŠ¶å’Œxä¸€è‡´ï¼Œåˆå§‹å€¼ä¸º0çš„çŸ©é˜µï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ªexpertçš„è¾“å‡º final_hidden_states = torch.zeros_like(x) for expert_idx in range(8): # é€‰æ‹©å½“å‰ä½¿ç”¨çš„expert expert_layer = self.experts[expert_idx] # é€‰æ‹©å½“å‰expertå¯¹åº”çš„index idx_list, top_x_list = torch.where(expert_mask[expert_idx]) # é€‰æ‹©éœ€è¦è®¡ç®—çš„çŠ¶æ€ current_state = x[top_x_list] # é€‰æ‹©å½“å‰expertå¯¹æ¯ä¸ªtokençš„æƒé‡ current_routing_weights = routing_weights.t()[top_x_list, idx_list] # å°†é€‰æ‹©çš„çŠ¶æ€è¾“å…¥ç»™ä¸“å®¶æ¨¡å‹è®¡ç®—ï¼Œå¹¶ä¹˜ä¸Šæƒé‡ current_hidden_states = expert_layer(current_state) * current_routing_weights # å°†æ¯ä¸ªexpertçš„è¾“å‡ºæŒ‰ç…§ç´¢å¼•åŠ åˆ°æœ€ç»ˆç»“æœé‡Œ final_hidden_states.index_add_(0, top_x_list, current_hidden_states) return final_hidden_states Dynamic Routing There are a couple of ways to design router to route tokens to each expert. Ideally, we want to design a router that could make each expert specialize one of domains/tasks. Obviously there is no straightforward way to achieve this. In Mixtral, softmax-topk based gating mechanism is used to select experts.\nFor any input $x$ of dimension $[\\text{sequence\\_len}, \\text{dim}]$, it multiplies with a gate matrix $W$ of shape $[\\text{dim}, 8]$, then we get a router representation of shape $[\\text{sequence\\_len}, 8]$. It selects top k (num of experts per token) logits which then go through softmax op to normalize to get k experts weights. In Mixtral, the k is equal to 2.\nMoE training is prone to instability issues because it has extra exponential functions. To deal with mixed precision roundoff errors, people apply z-loss to logits before sending them to router.\n$$ L_z = \\frac{1}{B} \\sum_{i=1}^{B} (log\\sum_{j=1}^{N}e^{x_j^{(i)}})^2 $$\nThis is called router z-loss [9]. In python,\nz_loss = torch.mean(torch.square(torch.logsumexp(logits, dim=-1))) * z_loss_coeff Ref [7] uses the similar kind of approach to stabilize the training. $$ L_{max_z} = 2 e^{-4} * z^2 $$ where $z$ is the max logit value.\nLoad Balancing For dynamic routing, which token is routed to which expert is unknown upfront, so there exists the load balancing issue. Common solution is to add an auxiliary load balancing loss [2].\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$\nHere $N$ is the number of experts, $T$ is the total number of tokens in batch $B$. $f_i$ is the fraction of tokens dispatched to expert $i$.\nand $P_i$ is the fraction of the router probability allocated for expert $i$, i.e. the summation of probability assigning token to expert i for all tokens $$ P_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x) $$\nNote that this loss is added for each MoE layer.\nTraining Directly training MoE could be challenging due to low efficiency. One popular approach is called sparse upcycling to use pretrained dense model to initialize the sparse model and continue to train for certain steps.\nFigure 2. Training MoE Model (To be continued)\nPublic Implementations https://github.com/XueFuzhao/OpenMoE https://github.com/pjlab-sys4nlp/llama-moe https://github.com/NVIDIA/NeMo https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe https://github.com/stanford-futuredata/megablocks References Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity BASE Layers: Simplifying Training of Large, Sparse Models Mixtral of Experts Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference Baichuan 2: Open Large-scale Language Models DeepSeek-V3 Technical Report ST-MoE: Designing Stable and Transferable Sparse Expert Models ","wordCount":"987","inLanguage":"en-us","datePublished":"2023-10-18T00:18:23+08:00","dateModified":"2023-10-18T00:18:23+08:00","author":[{"@type":"Person","name":"Jun"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://rich-junwang.github.io/en-us/posts/tech/llm/moe_models/"},"publisher":{"@type":"Organization","name":"Jun's Blog","logo":{"@type":"ImageObject","url":"https://rich-junwang.github.io/img/Q.gif"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rich-junwang.github.io/en-us/ accesskey=h title="Jun's Blog (Alt + H)"><img src=https://rich-junwang.github.io/img/Q.gif alt=logo aria-label=logo height=35>Jun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://rich-junwang.github.io/en-us/ title="ğŸ  Home"><span>ğŸ  Home</span></a></li><li><a href=https://rich-junwang.github.io/en-us/about title="ğŸ™‹ğŸ»â€â™‚ï¸ About"><span>ğŸ™‹ğŸ»â€â™‚ï¸ About</span></a></li><li><a href=https://rich-junwang.github.io/en-us/posts title="ğŸ“š Posts"><span>ğŸ“š Posts</span></a></li><li><a href=https://rich-junwang.github.io/en-us/tags title="ğŸ§© Tags"><span>ğŸ§© Tags</span></a></li><li><a href=https://rich-junwang.github.io/en-us/archives/ title="â±ï¸ Archives"><span>â±ï¸ Archives</span></a></li><li><a href=https://rich-junwang.github.io/en-us/search title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class="main page"><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}</style><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://rich-junwang.github.io/en-us/>ğŸ  Home</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/>ğŸ“šArticles</a>&nbsp;Â»&nbsp;<a href=https://rich-junwang.github.io/en-us/posts/tech/>ğŸ‘¨ğŸ»â€ğŸ’» Tech</a></div><h1 class=post-title>MoE Models</h1><div class=post-description>MoE Models</div><div class=post-meta><style>i[id*=post_meta_style]{display:flex;align-items:center;margin:0 0 10px}.parent-post-meta{display:flex;flex-wrap:wrap;opacity:.8}</style><span class=parent-post-meta><span id=post_meta_style_1><span class="fa fa-calendar-check-o"></span>
<span>2023-10-18
&nbsp;&nbsp;
</span></span><span id=post_meta_style_3><span class="fa fa-file-word-o"></span>
<span>987 words
&nbsp;&nbsp;
</span></span><span id=post_meta_style_4><span class="fa fa-clock-o"></span>
<span>2 min
&nbsp;&nbsp;
</span></span><span id=post_meta_style_5><span class="fa fa-user-o"></span>
<span>Jun
&nbsp;&nbsp;
</span></span><span id=post_meta_style_6><span class="fa fa-tags" style=opacity:.8></span>
<span><span class=post-tags-meta><a href=https://rich-junwang.github.io/en-us/tags/ml/ style=color:var(--secondary)!important>ML</a>
</span></span></span></span><span style=opacity:.8><span id=post_meta_style_7>&nbsp;&nbsp;
<span class="fa fa-eye"></span>
<span><span id=busuanzi_container_page_pv><span id=busuanzi_value_page_pv></span></span>
&nbsp;&nbsp;
</span></span><span id=post_meta_style_8><span class="fa fa-commenting-o"></span>
<span><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>let url=document.documentURI,dnsUrl="https://rich-junwang.github.io/",urlSplit=url.split(dnsUrl),finalUrl=urlSplit[1];finalUrl[0]!=="/"&&(finalUrl="/"+finalUrl),twikoo.getCommentsCount({envId:null,region:null,urls:[finalUrl],includeReply:!1}).then(function(e){let t=e[0].count;const n=document.getElementById("comment_count");n.innerText=t}).catch(function(e){console.error(e)})</script><span id=comment_count></span></span></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#dynamic-routing aria-label="Dynamic Routing">Dynamic Routing</a></li><li><a href=#load-balancing aria-label="Load Balancing">Load Balancing</a></li><li><a href=#training aria-label=Training>Training</a></li><li><a href=#public-implementations aria-label="Public Implementations">Public Implementations</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>The biggest lesson we&rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we&rsquo;ll closely examine the Mixtral model to study MoE models.</p><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>Most of today&rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:</p><p align=center><img alt="MoE model" src=images/moe.png width=80% height=auto/>
Figure 1. Switch MoE Model<br></p><p>In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.</p><p>The follow code snippet shows how it works for Mixtral MoE model at inference time.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># æ³¨æ„ï¼šä¸ºäº†å®¹æ˜“ç†è§£ï¼Œæˆ‘å¯¹ä»£ç è¿›è¡Œäº†ç®€åŒ–ï¼ŒåŒæ—¶ä¸è€ƒè™‘batch sizeï¼Œå®é™…ä½¿ç”¨æ—¶è¿˜æ˜¯è¦ç”¨å®˜æ–¹ä»£ç </span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MixtralSparseMoeBlock</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gate <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>hidden_dim, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>experts <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([MLP(config) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>8</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># å¯¹æ¯ä¸ªtokenè®¡ç®—8ä¸ªexpertçš„æƒé‡ï¼Œå¹¶å°†æƒé‡å½’ä¸€åŒ–</span>
</span></span><span style=display:flex><span>        router_logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>gate(x) 
</span></span><span style=display:flex><span>        routing_weights <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(router_logits, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) 
</span></span><span style=display:flex><span>        <span style=color:#75715e># æ¯ä¸ªtokené€‰æ‹©top-2 expertsçš„æƒé‡ã€ç´¢å¼•ï¼Œ å¹¶å°†ç´¢å¼•è½¬ä¸ºsize=(len(tokens), 8)çš„ç‹¬çƒ­ç¼–ç </span>
</span></span><span style=display:flex><span>        routing_weights, selected_experts <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>top2(routing_weights, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>) 
</span></span><span style=display:flex><span>        expert_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>one_hot(selected_experts, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># é‡æ–°å°†top-2 expertçš„æƒé‡å½’ä¸€åŒ–ï¼ˆå› ä¸ºåˆ æ‰å…¶ä½™6ä¸ªexpertï¼Œæƒé‡çš„å’Œä¸ç­‰äº1äº†ï¼‰</span>
</span></span><span style=display:flex><span>        routing_weights <span style=color:#f92672>/=</span> routing_weights<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)  
</span></span><span style=display:flex><span>            <span style=color:#75715e># åˆ›å»ºå½¢çŠ¶å’Œxä¸€è‡´ï¼Œåˆå§‹å€¼ä¸º0çš„çŸ©é˜µï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ªexpertçš„è¾“å‡º</span>
</span></span><span style=display:flex><span>        final_hidden_states <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros_like(x) 
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> expert_idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰ä½¿ç”¨çš„expert</span>
</span></span><span style=display:flex><span>            expert_layer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>experts[expert_idx] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰expertå¯¹åº”çš„index</span>
</span></span><span style=display:flex><span>            idx_list, top_x_list <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>where(expert_mask[expert_idx]) 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©éœ€è¦è®¡ç®—çš„çŠ¶æ€</span>
</span></span><span style=display:flex><span>            current_state <span style=color:#f92672>=</span> x[top_x_list] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># é€‰æ‹©å½“å‰expertå¯¹æ¯ä¸ªtokençš„æƒé‡</span>
</span></span><span style=display:flex><span>            current_routing_weights <span style=color:#f92672>=</span> routing_weights<span style=color:#f92672>.</span>t()[top_x_list, idx_list] 
</span></span><span style=display:flex><span>            <span style=color:#75715e># å°†é€‰æ‹©çš„çŠ¶æ€è¾“å…¥ç»™ä¸“å®¶æ¨¡å‹è®¡ç®—ï¼Œå¹¶ä¹˜ä¸Šæƒé‡</span>
</span></span><span style=display:flex><span>            current_hidden_states <span style=color:#f92672>=</span> expert_layer(current_state) <span style=color:#f92672>*</span> current_routing_weights 
</span></span><span style=display:flex><span>            <span style=color:#75715e># å°†æ¯ä¸ªexpertçš„è¾“å‡ºæŒ‰ç…§ç´¢å¼•åŠ åˆ°æœ€ç»ˆç»“æœé‡Œ</span>
</span></span><span style=display:flex><span>            final_hidden_states<span style=color:#f92672>.</span>index_add_(<span style=color:#ae81ff>0</span>, top_x_list, current_hidden_states) 
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> final_hidden_states
</span></span></code></pre></div><h3 id=dynamic-routing>Dynamic Routing<a hidden class=anchor aria-hidden=true href=#dynamic-routing>#</a></h3><p>There are a couple of ways to design router to route tokens to each expert. Ideally, we want to design a router that could make each expert specialize one of domains/tasks. Obviously there is no straightforward way to achieve this. In Mixtral, softmax-topk based gating mechanism is used to select experts.</p><p>For any input $x$ of dimension $[\text{sequence\_len}, \text{dim}]$, it multiplies with a gate matrix $W$ of shape $[\text{dim}, 8]$, then we get a router representation of shape $[\text{sequence\_len}, 8]$. It selects top k (num of experts per token) logits which then go through softmax op to normalize to get k experts weights. In Mixtral, the k is equal to 2.</p><p>MoE training is prone to instability issues because it has extra exponential functions. To deal with mixed precision roundoff errors, people apply z-loss to logits before sending them to router.</p><p>$$
L_z = \frac{1}{B} \sum_{i=1}^{B} (log\sum_{j=1}^{N}e^{x_j^{(i)}})^2
$$</p><p>This is called router z-loss [9]. In python,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>z_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean(torch<span style=color:#f92672>.</span>square(torch<span style=color:#f92672>.</span>logsumexp(logits, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>))) <span style=color:#f92672>*</span> z_loss_coeff
</span></span></code></pre></div><p>Ref [7] uses the similar kind of approach to stabilize the training.
$$
L_{max_z} = 2 e^{-4} * z^2
$$
where $z$ is the max logit value.</p><h3 id=load-balancing>Load Balancing<a hidden class=anchor aria-hidden=true href=#load-balancing>#</a></h3><p>For dynamic routing, which token is routed to which expert is unknown upfront, so there exists the load balancing issue. Common solution is to add an auxiliary load balancing loss [2].</p><p>$$
\text{loss} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
$$</p><p>Here $N$ is the number of experts, $T$ is the total number of tokens in batch $B$. $f_i$ is the fraction of tokens dispatched to expert $i$.</p><p>and $P_i$ is the fraction of the router probability allocated for expert $i$, i.e. the summation of probability assigning token to expert i for all tokens
$$
P_i = \frac{1}{T} \sum_{x \in \mathcal{B}} p_i(x)
$$</p><p>Note that this loss is added for each MoE layer.</p><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><p>Directly training MoE could be challenging due to low efficiency. One popular approach is called sparse upcycling to use pretrained dense model to initialize the sparse model and continue to train for certain steps.</p><p align=center><img alt="Sparse upcycling" src=images/upcycle.png width=80% height=auto/>
Figure 2. Training MoE Model<br></p><p>(To be continued)</p><h3 id=public-implementations>Public Implementations<a hidden class=anchor aria-hidden=true href=#public-implementations>#</a></h3><ul><li><a href=https://github.com/XueFuzhao/OpenMoE>https://github.com/XueFuzhao/OpenMoE</a></li><li><a href=https://github.com/pjlab-sys4nlp/llama-moe>https://github.com/pjlab-sys4nlp/llama-moe</a></li><li><a href=https://github.com/NVIDIA/NeMo>https://github.com/NVIDIA/NeMo</a></li><li><a href=https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe>https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe</a></li><li><a href=https://github.com/stanford-futuredata/megablocks>https://github.com/stanford-futuredata/megablocks</a></li></ul><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ol><li><a href=https://arxiv.org/abs/1701.06538>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a><br></li><li><a href=https://arxiv.org/pdf/2101.03961.pdf>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a><br></li><li><a href=https://arxiv.org/pdf/2103.16716.pdf>BASE Layers: Simplifying Training of Large, Sparse Models</a><br></li><li><a href=https://arxiv.org/pdf/2401.04088.pdf>Mixtral of Experts</a><br></li><li><a href=https://arxiv.org/abs/2212.05055>Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints</a><br></li><li><a href=https://arxiv.org/pdf/2110.03742.pdf>Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference</a></li><li><a href=https://arxiv.org/abs/2309.10305>Baichuan 2: Open Large-scale Language Models</a></li><li><a href=https://arxiv.org/html/2412.19437v1>DeepSeek-V3 Technical Report</a></li><li><a href=https://arxiv.org/abs/2202.08906>ST-MoE: Designing Stable and Transferable Sparse Expert Models</a></li></ol></div><footer class=post-footer><nav class=paginav><a class=prev href=https://rich-junwang.github.io/en-us/posts/tech/llm/model_eval/><span class=title>Â«</span><br><span>Model Evaluation</span>
</a><a class=next href=https://rich-junwang.github.io/en-us/posts/tech/llm/rag/><span class=title>Â»</span><br><span>Retrieval Augmented Generation</span></a></nav></footer></div><style>.comments_details summary::marker{font-size:20px;content:'ğŸ‘‰Comment';color:var(--content)}.comments_details[open] summary::marker{font-size:20px;content:'ğŸ‘‡Collapse';color:var(--content)}</style><div><details class=comments_details><summary style="cursor:pointer;margin:50px 0 20px;width:130px"><span style=font-size:20px;color:var(--content)>...</span></summary><div id=tcomment></div></details><script src=https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js></script><script>twikoo.init({envId:null,el:"#tcomment",lang:"en-us",region:null,path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>Copyright
&copy;
2020-2025
<a href=https://rich-junwang.github.io/en-us/ style=color:#939393>Jun's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span id=busuanzi_container><span class="fa fa-user"></span> <span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye"></span> <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>200||document.documentElement.scrollTop>200?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{(function(){document.cookie="change-themes="+escape("false")})(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.body.addEventListener("copy",function(e){if(window.getSelection().toString()&&window.getSelection().toString().length>50){let t=e.clipboardData||window.clipboardData;if(t){e.preventDefault();let n=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href,s=window.getSelection().toString()+`\r

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;t.setData("text/html",n),t.setData("text/plain",s)}}})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function i(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€ŒJun's Blogã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚\r
åŸæ–‡é“¾æ¥ï¼š`+location.href;navigator.clipboard.writeText(t),i();return}const n=document.createRange();n.selectNodeContents(e);const s=window.getSelection();s.removeAllRanges(),s.addRange(n);try{document.execCommand("copy"),i()}catch{}s.removeRange(n)});let l=e.className.replaceAll("language-",""),n=document.createElement("div"),a=document.createElement("div"),r=document.createElement("div"),c=document.createElement("div"),o=document.createElement("div");o.innerText=l,n.setAttribute("class","mac-tool"),a.setAttribute("class","mac bb1"),r.setAttribute("class","mac bb2"),c.setAttribute("class","mac bb3"),o.setAttribute("class","language-type"),n.appendChild(a),n.appendChild(r),n.appendChild(c),n.appendChild(o),s.classList.contains("highlight")?(s.appendChild(t),s.appendChild(n)):s.parentNode.firstChild==s||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?(e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t),s.appendChild(n)):(e.parentNode.appendChild(t),s.appendChild(n)))})</script></body></html>