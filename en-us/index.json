[{"content":"In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.\nGPU Compute Model and Memory Hierarchy The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.\nFigure 1. GPU memory Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit.\nFigure 2. GPU memory hierarchy IO-aware Computation First let\u0026rsquo;s take a look at the vallina attention computation.\nFigure 3. Vallina attention computation Essentially, each of the operation follows the three steps of operation below.\nRead op — Move tensor from HBM to SRAM Compute op - Perform compute intensive task on SRAM write op - move tensor back from SRAM to HBM The breakdown of these computation is as follows. Apparently, all these ops in the vallina attention can be saved.\nFigure 4. Vallina attention computation break down However, it's hard to save giant attention matrix of size `[N x N]` in the cache. The idea to solve this challenge is to use tiling. Concretely, we slice the matrices into smaller blocks and in each of **Q** **K** computation, we do it in a small block scale. The output of the small block thus can be saved on the cache. This sounds perfectly except that softmax op is not possible with small block computation. Lucklily there are already some studies dealing with this [1-2]. Before talking about this, we first revisit stable softmax computation. Blockwise Softmax Underflow in numerica computation can cause precision issue. Overflow can be more problematic because it usually leads to divergence of training job (some may argue silent error is more detrimental :)). Softmax operation involves exponential computation which without careful handling can easily lead to overflow (such as exp(2000)).\n$$ \\text{softmax}(x)_i = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}} $$\nSimilary, the cross entropy can be computed as\n$$ H(p, q) = -\\sum_i p_i\\log(q_i) = -1\\cdot\\log(q_y) -\\sum_{i \\neq y} 0\\cdot\\log(q_i) = -\\log(q_y) = -\\log(\\text{softmax}(\\hat{y})_y)$$\n$$\\log(\\text{softmax}(x)_i) = \\log(\\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}) = x_i - \\max(x) - \\log(\\sum_j e^{x_j - \\max(x)})$$\nBy simply extracting the max value, we limit the exponential values to be in [0, 1]. In Flashattention paper, the softmax is represented as follows:\nFigure 5. Softmax Then blockwise softmax can be computed as follows:\nFigure 6. Blockwise Softmax With saving some summary (i.e. max) statistics, the softmax op can be decomposed into blocks.\nRecomputation in Backpropagation With the fused kernel, we effectively do the computation outside Pytorch computation graph. Thus, we can\u0026rsquo;t use the AutoGrad for gradient computation in backpropagation. Consequently, we have to define the backpropagation by ourselves. The way to solve this is very simple as well. We just define our own backpropagation ops for fused kernel like gradient checkpointing.\nReferences [1] SELF-ATTENTION DOES NOT NEED O(n^2) MEMORY [2] Online normalizer calculation for softmax [3] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/flash_attn/","summary":"In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy. GPU Compute Model and Memory Hierarchy The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job. Figure 1. GPU memory Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit.","title":"Flash Attention"},{"content":"Recently ChatGPT model has demonstrated remarkable success of large pretrained language model being able to generate coherent, logical and meaningful conversations. While as of this writing, the corresponding paper is still not available yet. In this blog, I\u0026rsquo;ll dive deep into InstructGPT model to see what\u0026rsquo;s under the hood of this model.\nIssues with Traditional LM Language modeling objective is trying to predict next token given all previous tokens. However, when we\u0026rsquo;re prompting LM in inference time, we hope LM can generate things based on our instructions/intent instead of merely predicting the most likely tokens. This is the so-called misalignment between training and inference.\nSolution Using reinforcement learning to learn human feedback. For this purpose, they have to collect a dataset. The way to collect the dataset is as follows:\nselect some contract labeler collect human written prompt-answer pairs. Prompts are either from GPT3 API or from human annotation. collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. The following diagram from the paper demonstrated how these steps unfold during the training.\nIn summary, there are three steps:\nUse labeled data to fine-tune GPT3 model Train a reward model Use RL to optimize GPT3 parameters In the first step, we got data from annotators and use this data to fine-tune GPT3 model. In the second step, they prepare some questions and GPT3 model gives multiple predictions for each question and annotators are asked to rank the generated predictions. This data is used to train reward model. The reward model is used for prediction and predict which one is most close to human choice. Reward model gives a score and the closer to human choice, the higher of the score.\nFinally, use policy-based RM algorithm to do further optimization. The whole process is shown in the diagram below. It uses reward mechanism to train model. The reward can be seen as the loss function in traditional ML training. Reward function is much more versatile than loss function (Think about DotaRL and AlphaGo). The consequence is that reward function may not be differentiable, thus can\u0026rsquo;t be used for back-propagation. People can sample rewards to proximate this loss function.\nRL algorithm. Image from [4] PPO From the repo in [4], the three steps of PPO are as follows:\nRollout: The language model generates a response or continuation based on query which could be the start of a sentence. Evaluation: The query and response are evaluated with a function, model, human feedback or some combination of them. The important thing is that this process should yield a scalar value for each query/response pair. Optimization: In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and and a reference model, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses don\u0026rsquo;t deviate to far from the reference language model. The active language model is then trained with PPO. (To be continued)\nReferences [1] Learning to summarize from human feedback [2] InstructGPT: Training language models to follow instructions with human feedback [3] Fine-Tuning Language Models from Human Preferences [4] https://github.com/lvwerra/trl [5] https://zhuanlan.zhihu.com/p/590311003 [6] Super-natural instructions: generalization via declarative instructions on 1600+ NLP tasks [7] Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/instructgpt/","summary":"Recently ChatGPT model has demonstrated remarkable success of large pretrained language model being able to generate coherent, logical and meaningful conversations. While as of this writing, the corresponding paper is still not available yet. In this blog, I\u0026rsquo;ll dive deep into InstructGPT model to see what\u0026rsquo;s under the hood of this model. Issues with Traditional LM Language modeling objective is trying to predict next token given all previous tokens. However, when we\u0026rsquo;re prompting LM in inference time, we hope LM can generate things based on our instructions/intent instead of merely predicting the most likely tokens. This is the so-called misalignment between training and inference. Solution Using reinforcement learning to learn human feedback. For this purpose, they have to collect a dataset. The way to collect the dataset is as follows: select some contract labeler collect human written prompt-answer pairs. Prompts","title":"InstructGPT and ChatGPT"},{"content":"Large language model pretraining is a very challenging task which requires very strong engineering and science skills. People tend to underestimate efforts needed to train a good large model like GPT3 etc. Most people imagine that they can get decent language models given enough computation resources. The fact is even today only OpenAI is providing LM APIs where people can freely play with and get good performances. In this blog, we\u0026rsquo;ll talk about pretraining from the whole pipeline: data sourcing, collection and processing, tokenization, architecture engineering and evaluation. Hopefully, it would be helpful for foundational model training practioners.\nData Data is crucial in any ML system. This is true to pretraining as well. As is shown in Gopher paper, a large, diverse and high-quality dataset is needed to train a good model. In the following table, it shows the datasets used in Gopher model training. Now we\u0026rsquo;re looking at terabytes scale of training data.\nDatasets used in Gopher [2] An ensuing problem with large amount of data is that data quality is hard to control. In practice, we have to at least make sure the content should be intelligible. We might want to give more training on high-quality datasets such as books and wikipedia [31]. Diversified datasets are necessary but can't guarantee training success as can be seen from `Gopher` paper, model performs well on QA related tasks but suffers on reasoning task. What else is needed? We'll come back to this later. Tokenizer Language models compute probability of any string sequence. How to represent the string sequence is determined by tokenizer. Popular options are byte pair encoding (BPE) or wordpiece. As the majority of models are using BPE today, here we focus on BPE based tokenizer. Tokenizer can impact several things in LLM training: (1) a high compression rate (tokenized token numer vs raw token number, the lower the better). Compression rate affects input context length and inference speed. (2) Vocab size. An appropriately sized vocabulary to ensure adequate training of each word embedding.\nAs mentioned in GPT2 paper, BPE effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences. Directly using greedy method to build BPE merging rules can be problematic. For example, word cat can be used in a lot of places like cat?, cat!, cat.. One way to solve this issue is to prevent BPE from generating rules across different character categories (letters, digits, puncts etc).\nAs people are pivoting in-context learing/instruction learning with large models, tokenization efficiency becomes more important. The following tables from Jurassic-1 paper shows the efficiency of tokenizer on several public dataset.\nTokenizer efficiency comparison from [16] Tokenizer determines the size of vocab. Usually when we support multilinguality and code data, the vocab size will be much larger. However, this is not always the case. CodeLLaMa shows very good performance (onpar with GPT4) with a vocab size of 32k. When vocab is too large, some of the tokens may not be trained enough. When vocab size is too small, the compression rate might be limited.\nCompression rate determines the input sequence length to the model. With high compression rate, the input length is shorter. Short sequence length might be able to mitigate exposure bias to some extent.\nOpen tokenizer implementations are: tiktoken.\nModel Architecture All pretrained models are variant of original transformer model. The differences are mainly about it\u0026rsquo;s encoder-decoder architecture or decoder-only architecture. First of all, let\u0026rsquo;s take a look at the choices of available large models.\n| Models | Model Size | Token Size | Architecture | |\u0026mdash;-|:\u0026mdash;-:| :\u0026mdash;-:| | GPT3 | 175B | 300B | Decoder | | OPT | 175B| 300B | Decoder | | PaLM | 540B| 780B | Decoder | | Gopher | 280B| 300B | Decoder | | Chinchilla | 70B| 1400B | Decoder | | Jurassic-1 | 178B| - | Decoder | | Megatron-Turing NLG | 530B| 270B | Decoder | | LaMDA | 137B| 2810B | Decoder | {:.mbtablestyle} Although all models listed here are autoregressive decoder only model, they actually differ a bit inside the decoder. For instance, to speed up inference time, PaLM is using multi-query attention. Normally, in mutlhead attention, there will be h heads each with a linear project layer for Q, K, V. With multiquery attention, instead of using h different linear project layers for K and V, we can share a single smaller linear project layer for K and a single linear projection layer for V for each head. Then, for different head layers, K and V will be the same. In this way, we can save memory IO and get better latency performance in incremental inference.\nA systematic study of transformer architecture is done in Ref [29]. Most of recent LLM architecture are following design from this paper.\nPeople usually call the embedding dim as the width of transformer and number of layers as the depth. There is a optimal depth-to-width allocation for a given self-attention network size as is shown in [34].\nTraining Design Most of today\u0026rsquo;s pretraining follow suits of a multi-stage and multi-task training. As is shown by Yao in [1], GPT series model is pretrained in such way as well.\nGPT Model Lineage. Image from [1] From the lineage diagram, we can see that ChatGPT model comes from Codex model which can be seen as a different stage of training. The way of scheduling tasks and data during training can have great impact on the final model performance.\nBatch Size Research [5] shows that there is a critical batch size in pretraining. When training batch size exceeds critical batch size, model performance starts to degrade. Critical batch size is independent of model size and is related to loss.\nGenerally small batch size leads to better validation loss when training with the same number of tokens as more random movement of gradient explores more of loss landscape. Often times, small batch size gives better genelization performance as well as pointed out in [27]. The reason given from the paper is that smaller batch size usually converges to flat minimum as oppose to sharp minimum. Intuitively, this is related to graident update in each step is small for large batch size training.\nFlat and Sharp Minima [27] Learning Rate Scheduling Usually as pointed out in [20], when we scale up batch size, we increase learning rate propotionally. However, when we increase model size (usually followed with batch size increase), the training tends to be more unstable. Thus, in reality, we decrease maximum learning rate when we increase model size (batch size).\nLearning rate scheduling usually involves a (linear) warm-up step to maximum learning rate and followed by a decaying step to 0 or a minimum learning rate. Currently, there are several methods in literature for the decaying step:\nLinear scheduler Plateau-linear schedule Cosine scheduler Regularization One of the most used regularization method is L2 regularization, aka, weight decay [28]. For instance, GPT 3 training uses a weight decay of 0.1. Note that comparing with traditional neural network tuning weight decay number (such as 0.01) GPT3 weight decay is pretty large.\nLength Extrapolation As in-context learning becomes popular, people are asking a question, Can an LLM maintain equally good, if not better, perplexities when longer sequences are used during inference time? This is the so-called length extrapolation [25].\nOptimizer When we select an optimizer, we have to take consideration of memory footprint and stability issues etc. Options are Adafactor, Adam etc. According to Gopher paper, adafactor optimizer has smaller memory footprint, and on smaller scale model (\u0026lt;7B) adafactor works well. However, when model size goes larger, performance suffers because of stability issue.\nEvaluation A lot of large models come out every year and many claims that they could beat GPT3 model in a wide range of benchmarks like SuperGlue, CLUE, MMLU etc. However, when you do benchmark these models in zero-shot setting or some less common tasks (but still very reasonable ones), these models tend to perform really bad. I personally tested GPT3 model (175b) and UL2 model (20b) on text2sql and sql2text task, GPT3 gives way better performance to the extent that you\u0026rsquo;ll believe UL2 is like garbage. The similar thing happened in evaluation in [24]. You may argue that the model size differs a lot. However, we can think the other way around: the results they claim better than GPT3 is also got from a smaller model and maybe their model training is not easy/efficient to scale to such level. Essentially, what I want to say is that good performance on popular benchmark datasets doesn\u0026rsquo;t mean much for large LM pretraining as this is highly related to source of training data, whether or not doing fine-tuning, proper prompting etc. Human evaluation is what really matters.\nStability During the model training, the most commonly seen issue is gradient exploding, aka, gradient becomes NaN. As layers go deeper, this problem happens more often because the way backpropagation works. Over the years, people have proposed many different ways to solve the challenge. As is shown in paper [21], the post-LN shows stability issue without carefully designed warming-up stage. As a result, they are proposing pre-LN to alleviate the problem.\nThe objective function for highly nonlinear deep neural networks often contains sharp nonlinearities in parameter space resulting from the multiplication of several parameters. These nonlinearities give rise to very high derivatives in some places. When the parameters get close to such a cliff region, a gradient descent update can catapult the parameters very far, possibly losing most of the optimization work that had been done [33].\nIt\u0026rsquo;s important to monitor stability during training. Common practice is to plot activation norm and gradient norm for each step. When these values spike, we know there is something wrong. It\u0026rsquo;s better than looking at loss curve only as loss explosion generally lags behind these two indicators. For instance, when there is bad data, we could have better gauge of when that happens and restart training from that point.\nAdept AI has a lengthy blog post talking about hardware error induced stability issue. The blog mentioned two ways to identify erroneous node(s):\nGrid search: partition nodes into groups and train model on each group in a deterministic way. Find the one that has different training loss curve. Parameter checksum check: for each data parallel run, check parameter checksum to see if they are the same to determine which stage might be wrong. Efficient Inference Inference speed determines product cost. Over the years, people have proposed various ways to improve inference speed. The multiquery attention mentioned above is one of these approaches. References [1] How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources [2] Gopher: Scaling Language Models: Methods, Analysis \u0026amp; Insights from Training Gopher [3] UL2: Unifying Language Learning Paradigms [4] Bloom: Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model [5] Scaling Laws for Neural Language Models [6] GPT: Improving Language Understanding by Generative Pre-Training [7] GPT2: Language Models are Unsupervised Multitask Learners [8] GPT3: Language Models are Few-Shot Learners [9] InstructGPT: Training language models to follow instructions with human feedback [10] WebGPT: Browser-assisted question-answering with human feedback [11] OPT: Open Pre-trained Transformer Language Models [12] OPT2: OPT-IML Scaling Language Model Instruction Meta Learning through the Lens of Generalization [13] PaLM: Scaling Language Modeling with Pathways [14] Flan-PaLM: Scaling Instruction-Finetuned Language Models [15] Chinchilla: Training Compute-Optimal Large Language Models [16] Jurassic-1: Technical details and evaluation. [17] Megatron-NLG: Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [18] LaMDA: Language Models for Dialog Applications [19] Codex: Evaluating Large Language Models Trained on Code [20] Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [21] On Layer Normalization in the Transformer Architecture [22] GLM-130B: An Open Bilingual Pre-trained Model [23] T0: Multitask Prompted Training Enables Zero-Shot Task Generalization [24] https://zhuanlan.zhihu.com/p/590240010 [25] RoFormer: Enhanced Transformer with Rotary Position Embedding [26] Receptive Field Alignment Enables Transformer Length Extrapolation [27] On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima [28] Decoupled Weight Decay Regularization [29] Do Transformer Modifications Transfer Across Implementations and Applications? [30] xFormers: A modular and hackable Transformer modelling library [31] LLaMA: Open and Efficient Foundation Language Models [32] What Language Model to Train if You Have One Million GPU Hours? [33] On the difficulty of training Recurrent Neural Networks [34] Limits to Depth-Efficiencies of Self-Attention [35] Baichuan LLM [36] Qwen LLM\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/large_scale_pretraining/","summary":"Large language model pretraining is a very challenging task which requires very strong engineering and science skills. People tend to underestimate efforts needed to train a good large model like GPT3 etc. Most people imagine that they can get decent language models given enough computation resources. The fact is even today only OpenAI is providing LM APIs where people can freely play with and get good performances. In this blog, we\u0026rsquo;ll talk about pretraining from the whole pipeline: data sourcing, collection and processing, tokenization, architecture engineering and evaluation. Hopefully, it would be helpful for foundational model training practioners. Data Data is crucial in any ML system. This is true to pretraining as well. As is shown in Gopher paper, a large, diverse and high-quality dataset is needed to train a good model. In the following table, it shows the datasets","title":"Large Scale Pretraining"},{"content":"Basics Kubernetes, also known as “k8s”, is an open source platform solution provided by Google for scheduling and automating the deployment, management, and scaling of containerized applications. Kubernetes has the ability of scheduling and running application containers on a cluster of physical or virtual machines. In managing the applications, the concepts of \u0026rsquo;labels\u0026rsquo; and \u0026lsquo;pods\u0026rsquo; are used to group the containers which make up an application. Currently, it supports Docker for containers.\nkubernetes architecture, image from [1] Pod is a type of abstraction on top of container. Deployment defines the pod. Deployment is used for stateless apps and StatefulSet is used for stateful apps or database. KubeCTL talks with API server to create components or delete components, in other words configuring the cluster. More about this basics is here\nBasic Operations To find out all the pods, using the following command\nkubectl get pods kubectl get pods | grep username kubectl get pods -n my_namespace_name # get pod understand a # get all the nodes kubectl get nodes # get services kubectl get services # create deployment. (blueprint of pod) kubectl create deployment my_pod_name --image=my_image kubectl get deployment To get all the containers running the pod, using the following command\nkubectl get pods my_pod_name -o custom-columns=\u0026#39;NAME:.metadata.name,CONTAINERS:.spec.containers[*].name\u0026#39; kubectl describe pod my_pod_name -n my_namespace_name View logs of job running in the pod\nkubectl logs my_pod_name kubectl logs -f my_pod_name # similar to attach kubectl attach my_pod_name # works with tqdm Log into the pod\nkubectl exec -it my_pod_name -- /bin/bash We can use kubectl to copy files to/from the pod. Be careful that your container may not support ~ this kind of path expansion.\nkubectl cp src_file_path pod:dest_file_path To use rsync is not that straightforward, I\u0026rsquo;m using the tool from here.\n# save the file as krsync, and put it to /usr/bin, and chmod +x to the file #!/bin/bash if [ -z \u0026#34;$KRSYNC_STARTED\u0026#34; ]; then export KRSYNC_STARTED=true exec rsync --blocking-io --rsh \u0026#34;$0\u0026#34; $@ fi # Running as --rsh namespace=\u0026#39;\u0026#39; pod=$1 shift # If use uses pod@namespace rsync passes as: {us} -l pod namespace ... if [ \u0026#34;X$pod\u0026#34; = \u0026#34;X-l\u0026#34; ]; then pod=$1 shift namespace=\u0026#34;-n $1\u0026#34; shift fi exec kubectl $namespace exec -i $pod -- \u0026#34;$@\u0026#34; Then use the following command to sync files. Note that you have to install rsync on the pod.\nkrsync -av --progress --stats src-dir/ pod:/dest-dir # with namespace krsync -av --progress --stats src-dir/ pod@namespace:/dest-dir To make it easier to use, we can add the following to the .zshrc file\nfunction krsync_watch_and_sync_to { fswatch -o . | xargs -n1 -I{} krsync -av --progress --stats *(D) $1 } Sometimes we have to change file ownership. Check out more here\nchown -R 33:33 /data/uploads References [1] Setting up a Kubernetes cluster using Docker in Docker [2] https://kubernetes.io/docs/reference/kubectl/cheatsheet/ ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/kubenetes/","summary":"Basics Kubernetes, also known as “k8s”, is an open source platform solution provided by Google for scheduling and automating the deployment, management, and scaling of containerized applications. Kubernetes has the ability of scheduling and running application containers on a cluster of physical or virtual machines. In managing the applications, the concepts of \u0026rsquo;labels\u0026rsquo; and \u0026lsquo;pods\u0026rsquo; are used to group the containers which make up an application. Currently, it supports Docker for containers. kubernetes architecture, image from [1] Pod is a type of abstraction on top of container. Deployment defines the pod. Deployment is used for stateless apps and StatefulSet is used for stateful apps or database. KubeCTL talks with API server to create components or delete components, in other words configuring the cluster. More about this basics is here Basic Operations To find out all","title":"Kubernetes"},{"content":"","permalink":"https://rich-junwang.github.io/en-us/posts/blog/blog/","summary":"","title":"Blog"},{"content":"Distributed infrastructure is a big and interesting topic. I don\u0026rsquo;t work on infrastructure side, but I run into the concepts a lot, so I create this blog to help me understand more about infrastructure.\nMost of today\u0026rsquo;s distributed framework involves three parts, collective communication, data loading and preprocessing and distributed scheduler. We\u0026rsquo;ll look into these three parts resepectively.\nCollective Communication We can start with point to point communication. Normally point to point communication refers to two processes communication and it\u0026rsquo;s one to one communication. Accordingly, collective communication refers to 1 to many or many to many communication. In distributed system, there are large amount of communications among the nodes.\nThere are some common communication ops, such as Broadcast, Reduce, Allreduce, Scatter, Gather, Allgather etc.\nBroadcast and Scatter Broadcast is to distribute data from one node to other nodes. Scatter is to distribute a portion of data to different nodes.\nMPI broadcast and scatter Reduce and Allreduce Reduce is a collections of ops. Specifically, the operator will process an array from each process and get reduced number of elements.\nMPI reduce MPI reduce Allreduce means that the reduce operation will be conducted throughout all nodes.\nMPI Allreduce ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/distributed_training/","summary":"Distributed infrastructure is a big and interesting topic. I don\u0026rsquo;t work on infrastructure side, but I run into the concepts a lot, so I create this blog to help me understand more about infrastructure. Most of today\u0026rsquo;s distributed framework involves three parts, collective communication, data loading and preprocessing and distributed scheduler. We\u0026rsquo;ll look into these three parts resepectively. Collective Communication We can start with point to point communication. Normally point to point communication refers to two processes communication and it\u0026rsquo;s one to one communication. Accordingly, collective communication refers to 1 to many or many to many communication. In distributed system, there are large amount of communications among the nodes. There are some common communication ops, such as Broadcast, Reduce, Allreduce, Scatter, Gather, Allgather etc. Broadcast and Scatter Broadcast is to distribute data from one node to other nodes. Scatter is","title":"Distributed Training Infra"},{"content":"","permalink":"https://rich-junwang.github.io/en-us/posts/life/life/","summary":"","title":"Life"},{"content":"","permalink":"https://rich-junwang.github.io/en-us/posts/read/read/","summary":"","title":"Read"},{"content":"A lot of people are talking about leetcode problem solving these days. If you go to the popular Chinese overseas BBS website, you can find literally under every post there are people talking about solving leetcode problems. In this blog, I want to share my two cents on this issue.\nI personally don\u0026rsquo;t like solving leetcode problems which I guess most people share with my feelings. I don\u0026rsquo;t take any pride in being ranked as the top K problem solver. My opinion is that it\u0026rsquo;s huge waste of time. There are definitely some good parts in doing this. If you\u0026rsquo;re not familiar with a programming langauge, you can learn a bit from good solutions in the disucssion section. Through solving the problem, you\u0026rsquo;ll get familiar with the specific programming language you use. Through the thinking process, you\u0026rsquo;ll learn how to convert logics into codes.\nHowever, focusing on these Fake problems will cost a person gigantic amount of time. There are more important things to learn. In my opinion, a pragmatic engineer should focus on the following four quadrants to improve himself:\nTechnical skills. A good understanding of a wide range of topics such as ML, system design etc. Real problem solving skills. When tasked with a real problem, which is the best route to solve the challenge. Communication skills. How to use concise and precise words to convey your ideas and onboard others with your thoughts. Business acumen. How customers can get benefits from our product, what\u0026rsquo;s our moat, and is our solution going to bring revenue to company. At the end of the day, we want to ask ourselves what kind of innovations/changes we have brought to this world. In my view, that\u0026rsquo;s what defines our value. In the meanwhile, I feel it\u0026rsquo;s a lot more fun in solving the real world problem and tackle the real issues.\nAll big techs today are relying on leetcode to select best talents which, in my opinion, is quite unfortunate.(To be continued)\n\u0026lt;div\u0026gt; 科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码 科技代码 科技代码 \u0026lt;/div\u0026gt; ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/tech/","summary":"A lot of people are talking about leetcode problem solving these days. If you go to the popular Chinese overseas BBS website, you can find literally under every post there are people talking about solving leetcode problems. In this blog, I want to share my two cents on this issue. I personally don\u0026rsquo;t like solving leetcode problems which I guess most people share with my feelings. I don\u0026rsquo;t take any pride in being ranked as the top K problem solver. My opinion is that it\u0026rsquo;s huge waste of time. There are definitely some good parts in doing this. If you\u0026rsquo;re not familiar with a programming langauge, you can learn a bit from good solutions in the disucssion section. Through solving the problem, you\u0026rsquo;ll get familiar with the specific programming language you use. Through the thinking process, you\u0026rsquo;ll learn how to","title":"Tech \u0026 Talents"},{"content":"Git Git merge Suppose we\u0026rsquo;re on master branch, if we want to override the changes in the master branch with feature branch, we can use the following command\ngit merge -X theirs feature to keep the master branch changes:\ngit merge -X ours feature If we want to rebase of current branch onto the master, and want to keep feature branch\ngit rebase master -X theirs if we want to keep master branch changes over our feature branch, the\ngit rebase master -X ours To summarize, we can have the following table:\n\u0026nbsp; Currently on Command \u0026nbsp; \u0026nbsp; Strategy \u0026nbsp; \u0026nbsp; Outcome \u0026nbsp;master git merge feature \u0026nbsp; -Xtheirs \u0026nbsp; \u0026nbsp; Keep changes from feature branch master git merge feature -Xours keep changes from master branch \u0026nbsp;feature git rebase master \u0026nbsp; -Xtheirs \u0026nbsp; \u0026nbsp; Keep changes from feature branch feature git rebase master -Xours keep changes from master branch {:.mbtablestyle} #### Git Diff To check two branch difference, suppose we\u0026rsquo;re on branch1, then we can do,\ngit diff HEAD..master Delete a remote branch Delete a remote branch\ngit push origin -d remote_branch_name Git rebase To fixup, squash, edit, drop, reword and many other operations on the previous N commit:\ngit rebase -i HAED~N Git commit git commit --amend --no-edit Undo git add The simplest way to undo a git add is to use git reset. It removes staged file, but will keeop the local changes there.\ngit reset file_path Git check difference Use the following command to checkout COMMIT (commit hash) ancestor and COMMIT difference\ngit diff COMMIT~ COMMIT git diff HEAD~ HEAD Git rebase resolve conflicts Sometimes when we do git rebase and we have many commits, we have to resolve a lot of conflicts, which can be really frustrating. One quick way might be to squash commits first to have a single commit, then do rebase. Another way is what we describe below.\nFirst, checkout temp branch from feature branch and start a standard merge\ngit checkout -b temp git merge origin/master git commit -m \u0026#34;Merge branch \u0026#39;origin/master\u0026#39; into \u0026#39;temp\u0026#39;\u0026#34; You will have to resolve conflicts, but only once and only real ones. Then stage all files and finish merge. Then return to your feature branch and start rebase, but with automatically resolving any conflicts.\ngit checkout feature git rebase origin/master -X theirs Branch has been rebased, but project is probably in invalid state. We just need to restore project state, so it will be exact as on branch \u0026rsquo;temp\u0026rsquo;. Technically we just need to copy its tree (folder state) via low-level command git commit-tree. Plus merging into current branch just created commit.\ngit merge --ff $(git commit-tree temp^{tree} -m \u0026#34;Fix after rebase\u0026#34; -p HEAD) git branch -D temp More details is here: https://github.com/capslocky/git-rebase-via-merge. Thanks to the original author!\nFind Command The original post is here: https://www.baeldung.com/linux/find-exec-command\n1. Basics The find command is comprised of two main parts, the expression and the action. When we initially use find, we usually start with the expression part. This is the part that allows us to specify a filter that defines which files to select.\nA classic example would be:\n$ find Music/ -name *.mp3 -type f Music/Gustav Mahler/01 - Das Trinklied vom Jammer der Erde.mp3 Music/Gustav Mahler/02 - Der Einsame im Herbst.mp3 The action part in this example is the default action, -print. This action prints the resulting paths with newline characters in between. It’ll run if no other action is specified.\nIn contrast, the -exec action allows us to execute commands on the resulting paths. Let’s say we want to run the file command on the list of mp3 files we just found to determine their filetype. We can achieve this by running the following command:\n$ find Music/ -name *.mp3 -exec file {} \\; Music/Gustav Mahler/01 - Das Trinklied vom Jammer der Erde.mp3: Audio file with ID3 version 2.4.0, contains:MPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo Let’s dissect the arguments passed to the -exec flag, which include: A command: file A placeholder: {} A command delimiter: ; Now we’ll walk through each of these three parts in-depth.\n2. The Command Any command that can be executed by our shell is acceptable here. We should note that this isn’t our shell executing the command, rather we’re using Linux’s exec directly to execute the command. This means that any shell expansion won’t work here, as we don’t have a shell. Another effect is the unavailability of shell functions or aliases.\nAs a workaround for our missing shell functions, we can export them and call bash -c with our requested function on our file. To see this in action, we’ll continue with our directory of Mahler’s mp3 files. Let’s create a shell function that shows the track name and some details about the quality:\nfunction mp3info() { TRACK_NAME=$(basename \u0026#34;$1\u0026#34;) FILE_DATA=$(file \u0026#34;$1\u0026#34; | awk -F, \u0026#39;{$1=$2=$3=$4=\u0026#34;\u0026#34;; print $0 }\u0026#39;) echo \u0026#34;${TRACK_NAME%.mp3} : $FILE_DATA\u0026#34; } If we try to run the mp3info command on all of our files, -exec will complain that it doesn’t know about mp3info:\nfind . -name \u0026#34;*.mp3\u0026#34; -exec mp3info {} \\; find: ‘mp3info’: No such file or directory As mentioned earlier, to fix this, we’ll need to export our shell function and run it as part of a spawned shell:\n$ export -f mp3info $ find . -name \u0026#34;*.mp3\u0026#34; -exec bash -c \u0026#34;mp3info \\\u0026#34;{}\\\u0026#34;\u0026#34; \\; 01 - Das Trinklied vom Jammer der Erde : 128 kbps 44.1 kHz Stereo 02 - Der Einsame im Herbst : 128 kbps 44.1 kHz Stereo 03 - Von der Jugend : 128 kbps 44.1 kHz Stereo Note that because some of our file names hold spaces, we need to quote the results placeholder.\n3. The Results Placeholder The results placeholder is denoted by two curly braces {}.\nWe can use the placeholder multiple times if necessary:\nfind . -name \u0026#34;*.mp3\u0026#34; -exec bash -c \u0026#34;basename \\\u0026#34;{}\\\u0026#34; \u0026amp;\u0026amp; file \\\u0026#34;{}\\\u0026#34; | awk -F: \u0026#39;{\\$1=\\\u0026#34;\\\u0026#34;; print \\$0 }\u0026#39;\u0026#34; \\; 01 - Das Trinklied vom Jammer der Erde.mp3 Audio file with ID3 version 2.4.0, contains MPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo 02 - Der Einsame im Herbst.mp3 Audio file with ID3 version 2.4.0, contains MPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo 03 - Von der Jugend.mp3 Audio file with ID3 version 2.4.0, contains MPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo In the above example, we ran both the basename, as well as the file commands. To allow us to concatenate the commands, we spawned a separate shell, as explained above.\n4. The Delimiter We need to provide the find command with a delimiter so it’ll know where our -exec arguments stop. Two types of delimiters can be provided to the -exec argument: the semi-colon(;) or the plus sign (+). As we don’t want our shell to interpret the semi-colon, we need to escape it (;).\nThe delimiter determines the way find handles the expression results. If we use the semi-colon (;), the -exec command will be repeated for each result separately. On the other hand, if we use the plus sign (+), all of the expressions’ results will be concatenated and passed as a whole to the -exec command, which will run only once.\nLet’s see the use of the plus sign with another example:\n$ find . -name \u0026#34;*.mp3\u0026#34; -exec echo {} + ./Gustav Mahler/01 - Das Trinklied vom Jammer der Erde.mp3 ./Gustav Mahler/02 - Der Einsame im Herbst.mp3 ./Gustav Mahler/03 - Von der Jugend.mp3 ./Gustav Mahler/04 - Von der Schönheit.mp3 ./Gustav Mahler/05 - Der Trunkene im Frühling.mp3 ./Gustav Mahler/06 - Der Abschied.mp3 When running echo, a newline is generated for every echo call, but since we used the plus-delimiter, only a single echo call was made. Let’s compare this result to the semi-colon version:\n$ find . -name \u0026#34;*.mp3\u0026#34; -exec echo {} \\; ./Gustav Mahler/01 - Das Trinklied vom Jammer der Erde.mp3 ./Gustav Mahler/02 - Der Einsame im Herbst.mp3 From a performance point of view, we usually prefer to use the plus-sign delimiter, as running separate processes for each file can incur a serious penalty in both RAM and processing time.\nHowever, we may prefer using the semi-colon delimiter in one of the following cases:\nThe tool run by -exec doesn’t accept multiple files as an argument. Running the tool on so many files at once might use up too much memory. We want to start getting some results as soon as possible, even though it’ll take more time to get all the results. One of the commands I use often with is\nfind my_directory/ -type f -exec lfs hsm_restore {} \\; Xargs Command There are commands that only take input as arguments like cp, rm, echo etc. We can use xargs to convert input coming from standard input to arguements.\n$find . -type f -name \u0026#34;*.log\u0026#34; | xargs -n 1 echo rm rm ./log/file5.log rm ./log/file6.log -n 1 argument, xargs turns each line into a command of its own.\n-I option takes a string that gets replaced with the supplied input before the command executes. Commond choices are {} and %.\nfind ./log -type f -name \u0026#34;*.log\u0026#34; | xargs -I % mv % backup/ aws s3 ls --recursive s3://my-bucket/ | grep \u0026#34;my_test\u0026#34; | cut -d\u0026#39; \u0026#39; -f4 | xargs -I{} aws s3 rm s3://my-bucket/{} -P option specify the number of parallel processes used in executing the commands over the input argument list.\nThe command below parallelly encodes a series of wav files to mp3 format: $find . -type f -name \u0026lsquo;*.wav\u0026rsquo; -print0 |xargs -0 -P 3 -n 1 mp3 -V8\nWhen combining find with xargs, it\u0026rsquo;s usually faster than using exec mentioned above.\nrsync command When use the following command, be careful about the relative path. In this command, we\u0026rsquo;re using 16 processes.\nls /my_model/checkpoints/source_dir | xargs -n16 -P -I% rsync -aP % target_dir Tips and Tricks Sometimes we need to copy multiple files from a directory. In order to copy multiple ones without explicitly listing all the absolute paths, we can use the following way. However, to use the autocomplete, we need to type left { first without the right one. cp /root/local/libs/{a.py, b.py} target_dir ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/shell-commands/","summary":"Git Git merge Suppose we\u0026rsquo;re on master branch, if we want to override the changes in the master branch with feature branch, we can use the following command git merge -X theirs feature to keep the master branch changes: git merge -X ours feature If we want to rebase of current branch onto the master, and want to keep feature branch git rebase master -X theirs if we want to keep master branch changes over our feature branch, the git rebase master -X ours To summarize, we can have the following table: \u0026nbsp; Currently on Command \u0026nbsp; \u0026nbsp; Strategy \u0026nbsp; \u0026nbsp; Outcome \u0026nbsp;master git merge feature \u0026nbsp; -Xtheirs \u0026nbsp; \u0026nbsp; Keep changes from feature branch master git merge feature -Xours keep changes from master branch \u0026nbsp;feature git rebase master \u0026nbsp; -Xtheirs \u0026nbsp; \u0026nbsp; Keep changes from feature branch feature git","title":"Shell Commands"},{"content":"Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we\u0026rsquo;ll dive deep into parallel training in recent distributed training paradigms.\nA lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We\u0026rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency.\nData Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is fed with one shard of data. Forward and backward computation is in parallel (simutaneously) and then there is a synchronization step where gradients are averaged across workers to update parameters. The DP computation can be summarized as the following three steps:\nEach machine computes local gradients given local inputs and a consistent global view of the parameters. LocalGrad_i = f(Inputs_i, Targets_i, Params) Sum up all the local gradients and distribute that sum to each machine, so there is a consistent global view of the gradients. GlobalGrad = all_reduce(LocalGrad_i) Each machine can now locally update the parameters and optimizer state under the assumption that the exact same calculation will happen on all machines. NewParams, NewOptimState = g(Params, OldOptimState, GlobalGrad) Pipeline Parallelism Pipeline parallelism (PP) is from model parallelism. Model parallelism is initially proposed to solve that challenge that one model can\u0026rsquo;t fit into one GPU. The idea is we can vertically slice model into different layers (e.g. one or more layers in transformer models) and put different layers in different GPUs. The issue with this method is that because sequential computation order of layers, if we feed single large batch data into one of the workers, all other workers are idle. This is the so-called bubble waiting time.\nTo solve the problem, we can reuse the data parallelism idea. Instead of feeding a single large batch into a model shard, we can partition data into small chunks. Each chunk of data goes through different model shards (workers) in a pipeline way. The following figure illustrates how this works.\nPipeline parallelism. image from [4] Tensor Parallelism The bottleneck of neural network training is compute. Among all the computation parts, the general matrix multiplication (GEMM) consumes the most of time. One way to parallize the matrix multiplication is to use matrix decomposition. Specifically, we can split a matrix into two or multiple parts based on row or column. Then we can aggregate results after the computation of each parts in the end. This is the core idea of tensor parallelism (TP).\nAs these three parallelism is orthogonal to each other, it\u0026rsquo;s easy to combine them together. The following diagram shows how to combine pipeline parallelism with data parallelism.\nCombination of pipeline parallelism and data parallelism. Image from Deepspeed tutorial ZeRO DP Zero Redundancy Optimizer (ZeRO) is an optimizied data parallelism proposed by Deepspeed team. The idea is instead of replicating the whole model, optimizer on each of workers, we can only store needed part.\nZero DP. Image from Deepspeed Parallelism in Megatron Megatron-LM and NeMo are the open source libraries from Nvidia for the distributed training. In these two libs, there are two constraints for the parallelism settings.\n- world_size = TP * PP * DP - global_batch_size % (PP * DP) == 0 Sequence Parallel For operations such as layer normation, the operation can be paralleized on the sequence dimension. Remember that layernorm is normalization over the feature dimenstion, ie. a token representation of 2048 will be normalized over 2048 numbers. In light of this, sequence parallel is proposed to reduce GPU memory consumption.\nSequence parallelism References [1] https://huggingface.co/blog/bloom-megatron-deepspeed [2] https://github.com/NVIDIA/NeMo [3] https://openai.com/blog/techniques-for-training-large-neural-networks/ [4] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism [5] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism [6] https://www.deepspeed.ai/tutorials/pipeline/\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/parallelism/","summary":"Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we\u0026rsquo;ll dive deep into parallel training in recent distributed training paradigms. A lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We\u0026rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency. Data Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is","title":"Parallelism in LLM Training"},{"content":"Keep SSH Connected There is always one issue that bothers me when using SSH to access server (e.g. EC2) which is that the ssh connection can disconnect very soon. I tried to make changes in the local ssh config: ~/.ssh/config\nHost remotehost HostName remotehost.com ServerAliveInterval 50 Then do a permission change\nchmod 600 ~/.ssh/config However, this doesn\u0026rsquo;t work for me on Mac, and I don\u0026rsquo;t know why. :(\nThen I tried to make changes on server side. In /etc/ssh/sshd_config, add or uncomment the following lines:\nClientAliveInterval 50 ClientAliveCountMax 10 Then restart or reload SSH server to help it recognize the configuration change\nsudo service ssh restart # for ubuntu linux sudo service sshd restart # for other linux dist Finally, log out and try to login again\nlogout This time it works! :)\nAdding SSH Public Key to Server Adding ssh public key to server sometimes can make the connections eaiser. The command is simple:\ncat ~/.ssh/id_ras.pub | ssh -i \u0026#34;my-keypair.pem\u0026#34; ubuntu@myserver \u0026#39;cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026#39; ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/ssh/","summary":"Keep SSH Connected There is always one issue that bothers me when using SSH to access server (e.g. EC2) which is that the ssh connection can disconnect very soon. I tried to make changes in the local ssh config: ~/.ssh/config Host remotehost HostName remotehost.com ServerAliveInterval 50 Then do a permission change chmod 600 ~/.ssh/config However, this doesn\u0026rsquo;t work for me on Mac, and I don\u0026rsquo;t know why. :( Then I tried to make changes on server side. In /etc/ssh/sshd_config, add or uncomment the following lines: ClientAliveInterval 50 ClientAliveCountMax 10 Then restart or reload SSH server to help it recognize the configuration change sudo service ssh restart # for ubuntu linux sudo service sshd restart # for other linux dist Finally, log out and try to login again logout This time it works! :) Adding SSH Public Key to Server Adding","title":"SSH Connection"},{"content":"1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找\nscan总共有这几种命令：scan、sscan、hscan、zscan，分别用于迭代数据库中的：数据库中所有键、集合键、哈希键、有序集合键，命令具体结构如下：\nscan cursor [MATCH pattern] [COUNT count] [TYPE type] sscan key cursor [MATCH pattern] [COUNT count] hscan key cursor [MATCH pattern] [COUNT count] zscan key cursor [MATCH pattern] [COUNT count] 2. scan scan cursor [MATCH pattern] [COUNT count] [TYPE type]，cursor表示游标，指查询开始的位置，count默认为10，查询完后会返回下一个开始的游标，当返回0的时候表示所有键查询完了\n127.0.0.1:6379[2]\u0026gt; scan 0 1) \u0026#34;3\u0026#34; 2) 1) \u0026#34;mystring\u0026#34; 2) \u0026#34;myzadd\u0026#34; 3) \u0026#34;myhset\u0026#34; 4) \u0026#34;mylist\u0026#34; 5) \u0026#34;myset2\u0026#34; 6) \u0026#34;myset1\u0026#34; 7) \u0026#34;mystring1\u0026#34; 8) \u0026#34;mystring3\u0026#34; 9) \u0026#34;mystring4\u0026#34; 10) \u0026#34;myset\u0026#34; 127.0.0.1:6379[2]\u0026gt; scan 3 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;myzadd1\u0026#34; 2) \u0026#34;mystring2\u0026#34; 3) \u0026#34;mylist2\u0026#34; 4) \u0026#34;myhset1\u0026#34; 5) \u0026#34;mylist1\u0026#34; MATCH可以采用模糊匹配找出自己想要查找的键，这里的逻辑是先查出20个，再匹配，而不是先匹配再查询，这里加上count 20是因为默认查出的10个数中可能不能包含所有的相关项，所以把范围扩大到查20个，我这里测试的键总共有15个\n127.0.0.1:6379[2]\u0026gt; scan 0 match mylist* count 20 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;mylist\u0026#34; 2) \u0026#34;mylist2\u0026#34; 3) \u0026#34;mylist1\u0026#34; TYPE可以根据具体的结构类型来匹配该类型的键\n127.0.0.1:6379[2]\u0026gt; scan 0 count 20 type list 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;mylist\u0026#34; 2) \u0026#34;mylist2\u0026#34; 3) \u0026#34;mylist1\u0026#34; 3. sscan sscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是集合类型的key\n127.0.0.1:6379[2]\u0026gt; sadd myset1 a b c d (integer) 4 127.0.0.1:6379[2]\u0026gt; smembers myset1 1) \u0026#34;d\u0026#34; 2) \u0026#34;a\u0026#34; 3) \u0026#34;c\u0026#34; 4) \u0026#34;b\u0026#34; 127.0.0.1:6379[2]\u0026gt; sscan myset1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;d\u0026#34; 2) \u0026#34;c\u0026#34; 3) \u0026#34;b\u0026#34; 4) \u0026#34;a\u0026#34; 127.0.0.1:6379[2]\u0026gt; sscan myset1 0 match a 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;a\u0026#34; 4. hscan hscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是哈希类型的key\n127.0.0.1:6379[2]\u0026gt; hset myhset1 kk1 vv1 kk2 vv2 kk3 vv3 (integer) 3 127.0.0.1:6379[2]\u0026gt; hgetall myhset1 1) \u0026#34;kk1\u0026#34; 2) \u0026#34;vv1\u0026#34; 3) \u0026#34;kk2\u0026#34; 4) \u0026#34;vv2\u0026#34; 5) \u0026#34;kk3\u0026#34; 6) \u0026#34;vv3\u0026#34; 127.0.0.1:6379[2]\u0026gt; hscan myhset1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;kk1\u0026#34; 2) \u0026#34;vv1\u0026#34; 3) \u0026#34;kk2\u0026#34; 4) \u0026#34;vv2\u0026#34; 5) \u0026#34;kk3\u0026#34; 6) \u0026#34;vv3\u0026#34; 5. zscan zscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是有序集合类型的key\n127.0.0.1:6379[2]\u0026gt; zadd myzadd1 1 zz1 2 zz2 3 zz3 (integer) 3 127.0.0.1:6379[2]\u0026gt; zrange myzadd1 0 -1 withscores 1) \u0026#34;zz1\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;zz2\u0026#34; 4) \u0026#34;2\u0026#34; 5) \u0026#34;zz3\u0026#34; 6) \u0026#34;3\u0026#34; 127.0.0.1:6379[2]\u0026gt; zscan myzadd1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;zz1\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;zz2\u0026#34; 4) \u0026#34;2\u0026#34; 5) \u0026#34;zz3\u0026#34; 6) \u0026#34;3\u0026#34; ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/redis/","summary":"1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找 scan总共有这几种命令：sca","title":"Redis scan命令学习"},{"content":"About Me\nHi there, this is Jun Wang. Welcome to my website. I'm a programmer. I love programming. Currently, I\u0026rsquo;m an Applied Scientist at Amazon. My research focuses on LLMs, foundational model training, machine learning and its applications in natural language processing. I aim to design systems that understand natural language in a way that makes sense to humans. Generally, I’m interested in areas such as semantic parsing, question answering and knowledge representation learning and reasoning, language grounding, and methods insipred by cognitive science.\nOpinions are my own and I do not speak for my employer or my team. On this webpage, I'd like to share my study and my paper reading. Feel free to reach out to me if you have any comments/suggestions\nIn my spare time, I like hiking a lot. I'm also very much into reading books of history, biography etc. Drop me a line if you want to chat more! Publications\nAll my publications can be found on google scholar Professional Services\nReviewer for NeurIPs 2023 Reviewer for AMLC 2020, 2021 Reviewer for ICLR 2022 Reviewer for AAAI 2021 ","permalink":"https://rich-junwang.github.io/en-us/about/","summary":"About Me Hi there, this is Jun Wang. Welcome to my website. I'm a programmer. I love programming. Currently, I\u0026rsquo;m an Applied Scientist at Amazon. My research focuses on LLMs, foundational model training, machine learning and its applications in natural language processing. I aim to design systems that understand natural language in a way that makes sense to humans. Generally, I’m interested in areas such as semantic parsing, question answering and knowledge representation learning and reasoning, language grounding, and methods insipred by cognitive science. Opinions are my own and I do not speak for my employer or my team. On this webpage, I'd like to share my study and my paper reading. Feel free to reach out to me if you have any comments/suggestions In my spare time, I like hiking a lot. I'm also very much into","title":"🙋🏻‍♂️About"},{"content":"Add the following into extend_head.html\n{{ if or .Params.math .Site.Params.math }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body, // The following is to parse inline math equation { delimiters: [ {left: \u0026#39;$$\u0026#39;, right: \u0026#39;$$\u0026#39;, display: true}, {left: \u0026#39;\\\\[\u0026#39;, right: \u0026#39;\\\\]\u0026#39;, display: true}, {left: \u0026#39;$\u0026#39;, right: \u0026#39;$\u0026#39;, display: false}, {left: \u0026#39;\\\\(\u0026#39;, right: \u0026#39;\\\\)\u0026#39;, display: false} ] } );\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} Then in the markdown file, in the header section we add math: true.\nMethods to Show Math The the above setup, you can use the following ways in the markdown writeup.\n\\\\(E=mc^2\\\\) $$E=mc^2$$ $E=mc^2$ ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/add_math/","summary":"Add the following into extend_head.html {{ if or .Params.math .Site.Params.math }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body, // The following is to parse inline math equation { delimiters: [ {left: \u0026#39;$$\u0026#39;, right: \u0026#39;$$\u0026#39;, display: true}, {left: \u0026#39;\\\\[\u0026#39;, right: \u0026#39;\\\\]\u0026#39;, display: true}, {left: \u0026#39;$\u0026#39;, right: \u0026#39;$\u0026#39;, display: false}, {left: \u0026#39;\\\\(\u0026#39;, right: \u0026#39;\\\\)\u0026#39;, display: false} ] } );\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} Then in the markdown file, in the header section we add math: true. Methods to Show Math The the above setup, you can use the following ways in the markdown writeup. \\\\(E=mc^2\\\\) $$E=mc^2$$ $E=mc^2$","title":"Add Math Equation in Blog"},{"content":"In the last blog, we talked about commonly used AWS commands. In this blog, I\u0026rsquo;ll document some commonly used docker commands to save some time when I need them.\nImage docker image ls # pull an image docker pull iamge_name Run docker container # rm is to clean constainer after exit # it is interactive tty # for normal docker image docker run --entrypoint /bin/bash -it \u0026lt;image_name\u0026gt; # for nvidia docker image nvidia-docker run --entrypoint /bin/bash --rm -it --name my_container_name image_name # mount a volume to docker nvidia-docker run --entrypoint /bin/bash -v $PWD/transforms_cache:/transforms_cache --rm -it image_name # add env to docker system nvidia-docker run --entrypoint /bin/bash -v $PWD/transforms_cache:/transforms_cache --rm --env SM_CHANNEL_TRAIN=/opt/ml/input/data/train -it image_name # docker run to use GPU, we can use another command docker run --entrypoint /bin/bash --gpus all -it xxxx_image_name Check all containers docker ps -a Clean space docker rmi -f $(docker images -a -q) sudo docker system prune Install package Install packages inside a running docker. Usually we\u0026rsquo;re able to install package based on distributeion of linux system running in the docker. For example, if it\u0026rsquo;s ubuntu, then the command is\napt-get -y update apt-get -y install tmux # package name Docker build We can use the following command to build docker image. Notice that the path is . (current directory). The path (a set of files) is called context and files inside can be used in COPY command in dockerfile. In building process, context will be packed into a tar file. So it\u0026rsquo;s good to put unnecessary files into .dockerignore file and select a reasonable path as context.\ndocker build -f Dockerfile_my_docker -t ${TAG} . --build-arg REGION=${region} ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/docker-commands/","summary":"In the last blog, we talked about commonly used AWS commands. In this blog, I\u0026rsquo;ll document some commonly used docker commands to save some time when I need them. Image docker image ls # pull an image docker pull iamge_name Run docker container # rm is to clean constainer after exit # it is interactive tty # for normal docker image docker run --entrypoint /bin/bash -it \u0026lt;image_name\u0026gt; # for nvidia docker image nvidia-docker run --entrypoint /bin/bash --rm -it --name my_container_name image_name # mount a volume to docker nvidia-docker run --entrypoint /bin/bash -v $PWD/transforms_cache:/transforms_cache --rm -it image_name # add env to docker system nvidia-docker run --entrypoint /bin/bash -v $PWD/transforms_cache:/transforms_cache --rm --env SM_CHANNEL_TRAIN=/opt/ml/input/data/train -it image_name # docker run to use GPU, we can use another command docker run --entrypoint /bin/bash --gpus all -it xxxx_image_name Check all containers docker ps -a Clean space","title":"Docker Commands"}]