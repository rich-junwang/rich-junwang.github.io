[{"content":"In this blog, I\u0026rsquo;ll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.\nBasics Monte Carlo Approximation Distributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as\n$$ \\mathbb{E_{x\\sim p(x)}}\\left(f(x)\\right) = \\int{f(x)p(x)} dx $$ when it\u0026rsquo;s a continuous random variable with a probability density function of $p$, or $$ \\mathbb{E}\\left(f(x)\\right) = \\sum_x{f(x)p(x)} $$ when it\u0026rsquo;s a discrete random variable with probability mass function of $p$. Then the Monte Carlo approximation says that the expectation is: $$ \\mathbb{E}\\left(f(x)\\right) \\approx \\frac{1}{N}\\sum_{i=1}^{N}{f(x_i)} $$\nassuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution.\nImportance Sampling In reality, it could be very challenging to sample data according to the distribution $p(x)$ as it is usually unknown to us. A workaround is to have another known distribution $q(x)$, and define the expectation as: $$ \\mathbb{E_{x\\sim p(x)}}[f] = \\int{q(x)\\frac{p(x)}{q(x)}f(x)} dx $$ This can be seen as the expectation of function $\\frac{p(x)}{q(x)}f(x)$ according to the distribution of $q(x)$. The distribution is sometimes called the proposal distribution. Then the expectation can be estimated as $$ \\mathbb{E_{x\\sim q(x)}}[f] \\approx \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{p(x_i)}{q(x_i)}f(x_i)} $$ Here the ratios $\\frac{p(x_i)}{q(x_i)}$ are referred sa the importance weights. The above derivation looks nice. However, we need to notice that the although the expectation is similar in both cases, the variance is different:\n$$ Var_{x\\sim p(x)}[f] = \\mathbb{E_{x\\sim p(x)}}[f(x)^2] - ({\\mathbb{E_{x\\sim p(x)}}[f(x)]})^2 $$\n$$ \\begin{aligned} Var_{x\\sim q(x)}[f] \u0026amp;= \\mathbb{E_{x\\sim q(x)}}[({\\frac{p(x_i)}{q(x_i)}f(x_i)})^2] - (\\mathbb{E_{x\\sim q(x)}}[{\\frac{p(x_i)}{q(x_i)}f(x_i)}])^2 \\\\ \u0026amp;= \\mathbb{E_{x\\sim p(x)}}[{\\frac{p(x_i)}{q(x_i)}f(x_i)^2}] - (\\mathbb{E_{x\\sim p(x)}}[f(x_i)])^2 \\end{aligned} $$ Notice that the second equation here, in the second step derivation, the expectation is relative to distribution of $p(x)$. From the above two equations, we can see that to make the sampling distribution as close as possible to the original distribution, the ratio $\\frac{p(x_i)}{q(x_i)}$ has to be close to 1.\nPolicy Gradient First, let\u0026rsquo;s remind ourselves some basics. The discounted return for a trajectory is defined as: $$ U_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + \u0026hellip; $$\nConsequently, the action-value function is defined as $$ Q_{\\pi}(s_t, a_t) = \\mathbb{E_t}[U_t|S_t=s_t, A_t=a_t] $$\nState-value function (or value function) can be calculated as: $$ V_{\\pi}(s_t) = \\mathbb{E_A}[Q_{\\pi}(s_t, A)] = \\sum_a \\pi(a|s_t) \\cdot Q_{\\pi}(s_t, a) $$\nIn policy gradient algorithm, the policy function $\\pi(a|s_t)$ is approximated by policy network $\\pi(a|s_t; \\theta)$. $\\theta$ here is the neural network model parameters. Then the policy-based learning is to maximize the objective function $$ \\begin{aligned} J(\\theta) \u0026amp;= \\mathbb{E_S}[V(S; \\theta)] \\\\ \u0026amp;= \\sum_{s\\in S} d_{\\pi}(s) V_{\\pi}(s_t; \\theta) \\\\ \u0026amp;= \\sum_{s\\in S} d_{\\pi}(s) \\sum_a \\pi(a|s_t; \\theta) \\cdot Q_{\\pi}(s_t, a) \\end{aligned} $$\nwhere $d_{\\pi}(s)$ is the stationary distribution of Markov chain for $\\pi_{\\theta}$, namely the state distribution under policy $\\pi$. Now we know the objective function of the policy-based algorithm, we can learn the parameters $\\theta$ through policy gradiet ascent.\nNow we can look at how to get the policy gradient. Since the first summation of the last step in the above equation has nothing to do with $\\theta$, so we can focus on getting the derivatives of the value function $V_{\\pi}(s; \\theta)$. Using chain rule, it\u0026rsquo;s easy to get: $$ \\begin{aligned} \\frac{\\partial{V(s; \\theta)}}{\\partial{\\theta}} \u0026amp;= \\sum_a \\frac{\\partial{\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\\\ \u0026amp;= \\sum_a \\pi(a|s_t; \\theta) \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\\\ \u0026amp;= \\mathbb{E_A}\\left[ \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a) \\right] \\end{aligned} $$ The last step assumes that $\\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot Q_{\\pi}(s, a)$ follows a distribution of $\\pi(a|s_t; \\theta)$ with respect to the random variable $A$.\nLet\u0026rsquo;s take another look at the policy gradient here. First, in practice, when we calculate the expectation we can use Monte Carlo Approximation. The gradient here becomes summations as below:\n$$ \\nabla_{\\theta}(J(\\theta)) = \\sum_{t} \\nabla_{\\theta}{\\log\\pi (a|s; \\theta)} \\cdot Q_{\\pi}(s, a) $$\nThis is also called Monte Carlo policy gradient. Since gradient is a direction, this formula shows that policy gradient estimation is the direction of the steepest increase in reward/return. When reward is larger, the policy gradient will be larger.\nTemporal Difference (TD) Learning Temporal Difference (TD) learning is one of the core concepts in Reinforcement Learning. Temporal difference algorithm always aims to bring the expected prediction and the new prediction together, thus matching expectations with reality and gradually increasing the accuracy of the entire chain of prediction.\nThe most basic version is TD(0) method. Specifically, if our agent is in a current state $s_t$, takes the action $a_t$ and receives the reward $r_t$, then we update our estimate of $V$ following\n$$ V(s_t) \\xleftarrow[]{} V(s_t) + \\alpha[r_{t+1} + \\gamma V(s_{t+1}) – V(s_t)] $$\nHere $r_{t+1} + \\gamma V(s_{t+1})$ is TD target and $r_{t+1} + \\gamma V(s_{t+1}) – V(s_t)$ is called TD error ($\\delta$).\nThere is SARSA (state-action-reward-state-action), where we replace the value function as the action-state value function.\n$$ Q(s_t, a_t) \\xleftarrow[]{} Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) – Q(s_t, a_t)] $$\nAnd TD with Q-learning $$ Q(s_t, a_t) \\xleftarrow[]{} Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) – Q(s_t, a_t)] $$\nREINFORCE Since $Q_{\\pi}(s, a)$ is the expectation of the return, we can once again use Monte Carlo approximation, $$ \\begin{aligned} Q_{\\pi}(s_t, a_t) \u0026amp;= u_t \\\\ \u0026amp;= \\sum_{i=t}^{N} {\\gamma^{i-t} \\cdot r_{i}} \\end{aligned} $$ The above MCPG actually gives us a practical algorithm to do policy gradient based RL. Let\u0026rsquo;s summarize it as follows:\nPlay one episode of game to get the trajectory: $s_1, a_1, r_1, s_2, a_2, r_2, \u0026hellip;$ Estimate all $q_t \\approx u_t$ using above equation Differentiate policy network to get $d_{\\theta, t}$ Compute policy gradient $g(a_t, \\theta_t) = q_t \\cdot d_{\\theta, t}$ Advantage Function and Generalized Advantage Estimation The above equation is the vanilla policy gradient method. More policy gradient algorithms are proposed later to reduce high variance of the vanilla version. John Schulman\u0026rsquo;s GAE paper summarized all the improvement methods. In the derivation, the policy gradient is represented as $$ \\frac{\\partial{V(s; \\theta)}}{\\partial{\\theta}} = \\mathbb{E_A}\\left[ \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] $$ where $\\hat{A_t}(s, a)$ is the advantage function. In implementation, we construct loss function in a way such that the policy gradient $g$ equals to the above result $$ L(\\theta) = \\mathbb{E_t}\\left[ \\log\\pi (a|s; \\theta) \\hat{A_t}(s, a) \\right] $$\nThe idea is that the Advantage function calculates how better taking that action at a state is compared to the average value of the state. It’s subtracting the mean value of the state from the state action pair. Mathematically, $A(s_t, a_t) = Q(s_t, a_t) − V (s_t)$, where $Q(s_t, a_t)$ is the action-value function, representing the expected return after taking action at at state $s$, and $V (s_t)$ is the value function, representing the average expected return at state $s_t$.\nBased on the above advantage definition, we have $$ \\begin{aligned} \\hat{A_t^{(1)}} \u0026amp;= r_t + \\gamma V(s_{t+1}) - V(s) \\\\ \\hat{A_t^{(2)}} \u0026amp;= r_t + \\gamma r_{t+1} +\\gamma^2 V(s_{t+2}) - V(s) \\\\ \u0026hellip;\\\\ \\hat{A_t^{(\\infty)}} \u0026amp;= r_t + \\gamma r_{t+1} +\\gamma^2 r_{t+2} + \u0026hellip; - V(s) \\end{aligned} $$\nNotice that $\\hat{A_t^{(1)}}$ has high bias, low variance, whilst $\\hat{A_t^{(\\infty)}}$ is unbiased, high variance. A weighted average of $\\hat{A_t^{(k)}}$ can be used to balance bias and variance. $$\\hat{A_t} = \\hat{A_t^{GAE}} = \\frac{\\sum_k w_k \\hat{A_t^{(k)}}}{\\sum_k w_k}$$ We set $w_k = \\lambda^{k-1}$, this gives clean calculation for $\\hat{A_t}$. Below we have the recursion equations. (Refer to [11] to learn how to derive the second equation here.)\n$$ \\begin{aligned} \\delta_t \u0026amp;= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\ \\hat{A_t} \u0026amp;= \\delta_t + \\gamma \\lambda \\delta_{t+1} + \u0026hellip; + (\\gamma \\lambda)^{T - t + 1} \\delta_{T - 1} \\\\ \u0026amp;= \\delta_t + \\gamma \\lambda \\hat{A_{t+1}} \\end{aligned} $$\nActor-Critic Algorithm There we give a recap of how actor-critic method works. In Actor-Critic algorithm, we use one neural network $\\pi(a|s; \\theta)$ to approximate policy function $\\pi(a|s)$ and use another neural network $q(s, a; w)$ to approximate value function $Q_{\\pi}(s, a)$.\nObserve state $s_t$, and randomly sample action from policy $a_t \\sim \\pi(\\cdot | s_t; \\Theta_t)$ Let agent perform action $a_t$, and get new state $s_{t+1}$ and reward $r_t$ from environment Randomly sample $\\tilde{a}_{t+1} \\sim \\pi(\\cdot | s_t; \\Theta_t)$ without performing the action Evaluate value network: $q_t = q(s_t, a_t; W_t)$ and $q_{t+1} = q(s_{t+1}, \\tilde{a}_{t+1}; W_t)$ Compute TD error: $\\delta_t = q_t - (r_t + \\gamma \\cdot q_{t+1})$ Differentiate value network: $d_{w,t} = \\frac{\\partial{q(s_t, a_t, w)}}{\\partial{w}}$ (autograd will do this for us) Update value network: $ w_{t+1} = w_t - \\alpha \\cdot \\delta_t \\cdot d_{w, t}$ Differentiate policy network: $ d_{\\theta, t} = \\frac{\\partial{\\log\\pi (a|s; \\theta)}}{\\partial{\\theta}} $ (again autograd will do this for us) Update policy network: $\\theta_{t+1} = \\theta_t + \\beta \\cdot q_t \\cdot d_{\\theta, t}$. We can also use: $\\theta_{t+1} = \\theta_t + \\beta \\cdot \\delta_t \\cdot d_{\\theta, t}$ to update policy network. This is called policy gradient with baseline. Essentially, the algorithm alternates between sampling and optimization. The expectation in the above equation indicates that we need to average over a finite batch of empirical samples. Proximal Policy Optimization Vanilla policy gradient method uses on-policy update. Concretely, the algorithm samples empirical data from a policy network $\\pi_{\\theta}$ parameterized with $\\theta$. After updating the network itself, the new policy network is $\\pi_{\\theta_{new}}$ and the old policy $\\pi_{\\theta}$ is out of use and future sampling will be from $\\pi_{\\theta_{new}}$. This whole process is not efficient enough. The solution to this is to reuse the old samples to achieve off-policy training. From above importance sampling section, we know that:\n$$ \\mathbb{E_{x\\sim p(x)}}\\left[f \\right] = \\mathbb{E_{x\\sim q(x)}} \\left[ \\frac{p(x_i)}{q(x_i)}f(x_i) \\right] $$\nSimilarly, we can make a change to the objective function of our policy gradient, and the resulting policy gradient will become $$ \\begin{aligned} g \u0026amp;= \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta}}}\\left[ \\frac{\\partial{\\log\\pi (a_t|s_t; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] \\\\ \u0026amp;= \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t; \\theta)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\frac{\\partial{\\log\\pi (a_t|s_t; \\theta)}}{\\partial{\\theta}} \\cdot \\hat{A_t}(s, a) \\right] \\end{aligned} $$ Consequently, the loss becomes\n$$ L(\\theta) = \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\hat{A_t}(s, a) \\right] $$ This is so-called surrogate objective function. In the above section, we mentioned how to use chain rule to get the expectation format of gradient, here we just to reverse the process to get the above loss function.\nIn the importance sampling section, we saw that the variance of new distribution could be large when the proposal distribution is not so close to the original distribution. Thus, to deal with this, people add KL diveragence to the loss function to limit the old and new policy difference. Using Largrangian dual method, we can add this constraint to the objective function:\n$$ L(\\theta) = \\mathbb{E_{{(s_t, a_t)} \\sim \\pi_{\\theta_{old}}}}\\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\hat{A_t}(s, a) - \\beta KL[\\pi_{\\theta_{old}}(a_t|s_t), \\pi_{\\theta}(a_t|s_t)]\\right] $$\nImplementation For language generation task, generating a token is an action. Agent is the target language model we want to train.\nHere we first look at the implementation from Deepspeed-chat model. The actor-critic algorithm requires to load four model in training: actor model, critic model, reference model and reward mdoel. Actor model is the poliy network and critice model is the value network. Reference model and reward model are frozen in training. Reference model is used to contrain the actor model predictions so that they won\u0026rsquo;t divege too much. Reward model gives the current step reward.\nReferences [1] High-Dimensional Continuous Control Using Generalized Advantage Estimation [2] Proximal Policy Optimization Algorithms [3] Policy Gradient Methods for Reinforcement Learning with Function Approximation [4] Dueling Network Architectures for Deep Reinforcement Learning [5] https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/schedule.html [6] https://github.com/wangshusen/DRL [7] https://www.davidsilver.uk/teaching/ [8] Fine-Tuning Language Models from Human Preferences [9] https://zhuanlan.zhihu.com/p/677607581 [10] DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [11] Secrets of RLHF in Large Language Models Part I: PPO [12] Secrets of RLHF in Large Language Models Part II: Reward Modeling [13] The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization [14] Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study [15] Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO [16] Advanced Tricks for Training Large Language Models with Proximal Policy Optimization\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/rl/ppo/","summary":"\u003cp\u003eIn this blog, I\u0026rsquo;ll go through the theory (simplified version) of PPO algorithm and try to code it from scratch.\u003c/p\u003e\n\u003ch3 id=\"basics\"\u003eBasics\u003c/h3\u003e\n\u003ch4 id=\"monte-carlo-approximation\"\u003eMonte Carlo Approximation\u003c/h4\u003e\n\u003cp\u003eDistributions of random variables in reality are mostly unknown. Sampling-based methods are extensively used in practice becaue of its ease of use and the generality where they can be applied. One of the fundamental problems is to calculate the expectation of a random variable, which can be expressed as\u003c/p\u003e\n\u003cp\u003e$$\n\\mathbb{E_{x\\sim p(x)}}\\left(f(x)\\right) = \\int{f(x)p(x)} dx\n$$\nwhen it\u0026rsquo;s a continuous random variable with a probability density function of $p$, or\n$$\n\\mathbb{E}\\left(f(x)\\right) = \\sum_x{f(x)p(x)}\n$$\nwhen it\u0026rsquo;s a discrete random variable with probability mass function of $p$.\nThen the Monte Carlo approximation says that the expectation is:\n$$\n\\mathbb{E}\\left(f(x)\\right) \\approx \\frac{1}{N}\\sum_{i=1}^{N}{f(x_i)}\n$$\u003c/p\u003e\n\u003cp\u003eassuming that the $x_i$ here is the i.i.d samples from the distribution $p(x)$. So we say that Monte Carlo Approximation is to use one or more samples to calculate the expectation of a distribution.\u003c/p\u003e","title":"PPO and Its Implementation"},{"content":"Diffusion is the process where we gradually convert a know distribution into a target distribution and the corresponding reverse process. Fortunately, people have proved that for gaussian distributions, we can convert data to noise and noise to data using the same functional form.\nFigure 1. diffusion process Forward Process As is shown in the figure above, there are two processes in diffusion:\nForward diffusion process: we slowly and iteratively add noise to the images Reverse diffusion process: we iteratively perform the denoising in small steps starting from a noisy image to convert it back to original form. In the forward process (diffusion process), in each step, gaussian noise is added according to a variance schedule $ \\beta_1, \\dotsb, \\beta_T $ $$ q_\\theta(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{(1-\\beta_t)}x_{t-1}, \\beta_t\\mathbf{I}) $$ And it\u0026rsquo;s a Markov chain process, so $$ q_\\theta(x_{1:T}|x_0) = \\prod_{t=1}^{T}q(x_t|x_{t-1}) $$ However, in implementation the above formulation has a problem because doing sequential sampling will result in inefficient forward process. The authors defined the following items: $$ \\alpha_t = 1 - \\beta_t \\\\ \\bar{\\alpha} = \\prod_{s=1}^t\\alpha_s $$ Then we have $$ q_\\theta(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}}x_0, (1 - \\bar{\\alpha_t})\\mathbf{I}) $$ This process is called reparameterization. This allows us to directly sample $x_t$ from $x_0$ using a single sample of Gaussian noise:\n$$ x_t = \\sqrt{\\bar{\\alpha_{t}}}x_0 + \\sqrt{1 - \\bar{\\alpha_{t}}} \\epsilon $$ Here $\\epsilon \\sim \\mathcal{N}(0, I)$. Even though we know the distribution, we don\u0026rsquo;t know the value the noise. In practice, we use neural network to approximate this noise.\nReparameterization Reparameterization is used on both VAE and diffusion. It\u0026rsquo;s needed because in diffusion, we have a lot of sampling operation and these operations are not differentiable. We use reparameterization to make it differentiable. Concretely, people introduce a random variable $\\epsilon$, then we can sample from any gussian $z \\sim \\mathcal{N}(z; \\mu_{\\theta}, \\sigma^2_{\\theta}\\mathbf{I}) $ as follows: $$ z = \\mu_{\\theta} + \\sigma_{\\theta} \\odot \\epsilon ; \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}) $$\nReverse Process Obviously, in reverse process we denoise the data and recover the images we have originally step by step. In the reverse process, we use the image at time $t$ to predict image at $t-1$, it follows the following distribution $$ p(x_{t-1}|x_{t}) = \\mathcal{N}(\\frac{1}{\\sqrt{\\alpha_{t}}}(x_t - \\frac{1- \\alpha_{t}}{\\sqrt{1 - \\bar{\\alpha_{t}}}} \\epsilon); \\frac{(1- \\alpha_{t})(1 - \\bar{\\alpha_{t-1}})}{\\sqrt{1 - \\bar{\\alpha_{t}}}}) $$\nEverything relates to $\\alpha$ is a constant.\nReferences Denoising Diffusion Probabilistic Models Understanding Diffusion Models: A Unified Perspective Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/diffusion/","summary":"\u003cp\u003eDiffusion is the process where we gradually convert a know distribution into a target distribution and the corresponding reverse process. Fortunately, people have proved that for gaussian distributions, we can convert data to noise and noise to data using the same functional form.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"gpu memory\" src=\"images/diffusion_process.png\" width=\"80%\" height=auto/\u003e \n    \u003cem\u003eFigure 1. diffusion process\u003c/em\u003e\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003ch3 id=\"forward-process\"\u003eForward Process\u003c/h3\u003e\n\u003cp\u003eAs is shown in the figure above, there are two processes in diffusion:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eForward diffusion process: we slowly and iteratively add noise to the images\u003c/li\u003e\n\u003cli\u003eReverse diffusion process: we iteratively perform the denoising in small steps starting from a noisy image to convert it back to original form.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the forward process (diffusion process), in each step, gaussian noise is added according to a variance schedule $ \\beta_1, \\dotsb, \\beta_T $\n$$\nq_\\theta(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{(1-\\beta_t)}x_{t-1}, \\beta_t\\mathbf{I})\n$$\nAnd it\u0026rsquo;s a Markov chain process, so\n$$\nq_\\theta(x_{1:T}|x_0) = \\prod_{t=1}^{T}q(x_t|x_{t-1})\n$$\nHowever, in implementation the above formulation has a problem because doing sequential sampling will result in inefficient forward process. The authors defined the following items:\n$$\n\\alpha_t = 1 - \\beta_t \\\\\n\\bar{\\alpha} = \\prod_{s=1}^t\\alpha_s\n$$\nThen we have\n$$\nq_\\theta(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}}x_0, (1 - \\bar{\\alpha_t})\\mathbf{I})\n$$\nThis process is called reparameterization. This allows us to directly sample $x_t$ from $x_0$ using a single sample of Gaussian noise:\u003c/p\u003e","title":"Diffusion Probabilistic Models"},{"content":"Promise and Future Before diving deep into Ray, I\u0026rsquo;ll first give a brief introduction to the async ops in programming in C++. An asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread\u0026rsquo;s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task\u0026rsquo;s result. The result of the delegated task is only required at some point in the future when it is needed.\nAn asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.\nPromise and future When an asynchronous call is created, an instance of std::future is returned to the creator of the asynchronous call (receiver). Meanwhile, the executor of the asynchronous call (sender) holds an instance of std::promise. The executor uses std::promise to fulfill its promise (a commitment to deliver the result at a future point in time after the operation is completed), while the creator uses std::future to obtain this future value (the result corresponding to the promise fulfilled in the future). The std::promise instance held by the executor and the std::future instance held by the creator are both connected to a shared object. This shared object establishes a communication channel for information synchronization between the creator and the executor of the asynchronous call, enabling both parties to exchange information about the execution status of the asynchronous operation through this channel.\nThe executor accesses this channel via its std::promise instance to write values into the channel. The creator uses its std::future instance to retrieve values from the channel. Once the executor completes the execution of the asynchronous operation, it writes the result of the operation into the channel via the std::promise instance. The creator then retrieves the result of the asynchronous operation through its std::future instance.\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;future\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;numeric\u0026gt; #include \u0026lt;chrono\u0026gt; int main() { // 创建一个promise对象实例 std::promise\u0026lt;int\u0026gt; _promise; // 从promise对象实例中获取对应的future对象实例 std::future\u0026lt;int\u0026gt; _future = _promise.get_future(); // 构建测试对象 std::vector\u0026lt;int\u0026gt; test_data = {1, 2, 3, 4, 5, 6}; // 创建一个任务，该任务对容器内的所有元素求和，求和完成之后通过promise来返回结果 auto sum_of_data = [](const std::vector\u0026lt;int\u0026gt;\u0026amp; data, std::promise\u0026lt;int\u0026gt; prom) { int sum = std::accumulate(data.begin(), data.end(), 0); std::this_thread::sleep_for(std::chrono::milliseconds(500)); // 休眠500ms prom.set_value(sum); // 完成承诺，将结果写入通道中 }; // 将这个任务交给一个线程，进行异步执行 std::thread work_thread(sum_of_data, test_data, std::move(_promise)); // 通过future的方法来获取异步调用的结果 std::cout \u0026lt;\u0026lt; \u0026#34;async result is \u0026#34; \u0026lt;\u0026lt; _future.get() \u0026lt;\u0026lt; std::endl; // 等待work_thread退出销毁完成 work_thread.join(); return 0; } // output // async result is 21 Ray Ray is a general-purpose framework for parallel programming on a cluster.\nRay Architecture Delay ray.get() With Ray, the invocation of every remote operation (e.g., task, actor method) is asynchronous. This means that the operation returns immediately a promise/future, which is essentially an identifier (ID) of the operation’s result. This is key to achieve parallelism, as it allows the driver program to launch multiple operations in parallel. To get the actual results, the programmer needs to call ray.get() on the IDs of the results. This call blocks until the results are available. As a side effect, this operation also blocks the driver program from invoking other operations, which can hurt parallelism.\nimport time def do_some_work(x): time.sleep(1) # Replace this with work you need to do. return x start = time.time() results = [do_some_work(x) for x in range(4)] print(\u0026#34;duration =\u0026#34;, time.time() - start, \u0026#34;\\nresults = \u0026#34;, results) # Above output, the program takes around 4 ses: # duration = 4.0149290561676025 # results = [0, 1, 2, 3] start = time.time() results = [do_some_work.remote(x) for x in range(4)] print(\u0026#34;duration =\u0026#34;, time.time() - start, \u0026#34;\\nresults = \u0026#34;, results) # Above output, # duration = 0.0003619194030761719 # results = [ObjectID(0100000000bdf683fc3e45db42685232b19d2a61), ObjectID(01000000da69c40e1c2f43b391443ce23de46cda), ObjectID(010000007fe0954ac2b3c0ab991538043e8f37e0), ObjectID(01000000cf47d5ecd1e26b42624454c795abe89b)] start = time.time() results = [ray.get(do_some_work.remote(x)) for x in range(4)] print(\u0026#34;duration =\u0026#34;, time.time() - start, \u0026#34;\\nresults = \u0026#34;, results) # Above output, # duration = 4.018050909042358 # results = [0, 1, 2, 3] results = ray.get([do_some_work.remote(x) for x in range(4)]) # output # duration = 1.0064549446105957 # results = [0, 1, 2, 3] Note that ray.get() is blocking, so calling it after each remote operation means that we wait for that operation to complete, which essentially means that we execute one operation at a time, hence no parallelism! To enable parallelism, we need to call ray.get() after invoking all tasks. We can easily do so in our example by replacing line “results = [do_some_work.remote(x) for x in range(4)]” with:\nAvoid passing same object repeatedly to remote tasks When we pass a large object as an argument to a remote function, Ray calls ray.put() under the hood to store that object in the local object store. This can significantly improve the performance of a remote task invocation when the remote task is executed locally, as all local tasks share the object store. However, there are cases when automatically calling ray.put() on a task invocation leads to performance issues. One example is passing the same large object as an argument repeatedly, as illustrated by the program below:\nimport time import numpy as np import ray ray.init(num_cpus = 4) @ray.remote def no_work(a): return start = time.time() a = np.zeros((10000, 2000)) result_ids = [no_work.remote(a) for x in range(10)] results = ray.get(result_ids) print(\u0026#34;duration =\u0026#34;, time.time() - start) # output # duration = 1.0699057579040527 The right way is to put the shared object in the object store as shown below.\nimport time import numpy as np import ray ray.init(num_cpus = 4) @ray.remote def no_work(a): return start = time.time() a_id = ray.put(np.zeros((10000, 2000))) result_ids = [no_work.remote(a_id) for x in range(10)] results = ray.get(result_ids) print(\u0026#34;duration =\u0026#34;, time.time() - start) # output # duration = 0.12425804138183594 Pipeline data processing If we use ray.get() on the results of multiple tasks we will have to wait until the last one of these tasks finishes. This can be an issue if tasks take widely different amounts of time. To illustrate this issue, consider the following example where we run four do_some_work() tasks in parallel, with each task taking a time uniformly distributed between 0 and 4 sec. Next, assume the results of these tasks are processed by process_results(), which takes 1 sec per result. The expected running time is then (1) the time it takes to execute the slowest of the do_some_work() tasks, plus (2) 4 sec which is the time it takes to execute process_results().\nimport time import random import ray ray.init(num_cpus = 4) @ray.remote def do_some_work(x): time.sleep(random.uniform(0, 4)) # Replace this with work you need to do. return x def process_results(results): sum = 0 for x in results: time.sleep(1) # Replace this with some processing code. sum += x return sum start = time.time() data_list = ray.get([do_some_work.remote(x) for x in range(4)]) sum = process_results(data_list) print(\u0026#34;duration =\u0026#34;, time.time() - start, \u0026#34;\\nresult = \u0026#34;, sum) # output # duration = 7.82636022567749 # result = 6 In order to process data as soon as it\u0026rsquo;s available, we can use the pipeline to remove the waiting time. Ray allows us to do exactly this by calling ray.wait() on a list of object IDs. Without specifying any other parameters, this function returns as soon as an object in its argument list is ready. This call has two returns: (1) the ID of the ready object, and (2) the list containing the IDs of the objects not ready yet. The modified program is below.\nimport time import random import ray ray.init(num_cpus = 4) @ray.remote def do_some_work(x): time.sleep(random.uniform(0, 4)) # Replace this with work you need to do. return x def process_incremental(sum, result): time.sleep(1) # Replace this with some processing code. return sum + result start = time.time() result_ids = [do_some_work.remote(x) for x in range(4)] sum = 0 while len(result_ids): done_id, result_ids = ray.wait(result_ids) sum = process_incremental(sum, ray.get(done_id[0])) print(\u0026#34;duration =\u0026#34;, time.time() - start, \u0026#34;\\nresult = \u0026#34;, sum) #output # duration = 4.852453231811523 # result = 6 Pipeline execution, image from 1 A full example on how to use Ray for distributed neural network training\nimport torch import torch.nn as nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor from ray import train from ray.train.torch import TorchTrainer from ray.train import ScalingConfig def get_dataset(): return datasets.FashionMNIST( root=\u0026#34;/tmp/data\u0026#34;, train=True, download=True, transform=ToTensor(), ) class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, inputs): inputs = self.flatten(inputs) logits = self.linear_relu_stack(inputs) return logits # without distributed training, pure pytorch def train_func(): num_epochs = 3 batch_size = 64 dataset = get_dataset() dataloader = DataLoader(dataset, batch_size=batch_size) model = NeuralNetwork() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(num_epochs): for inputs, labels in dataloader: optimizer.zero_grad() pred = model(inputs) loss = criterion(pred, labels) loss.backward() optimizer.step() print(f\u0026#34;epoch: {epoch}, loss: {loss.item()}\u0026#34;) # train_func() # distributed training def train_func_distributed(): num_epochs = 3 batch_size = 64 dataset = get_dataset() dataloader = DataLoader(dataset, batch_size=batch_size) dataloader = train.torch.prepare_data_loader(dataloader) model = NeuralNetwork() model = train.torch.prepare_model(model) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(num_epochs): for inputs, labels in dataloader: optimizer.zero_grad() pred = model(inputs) loss = criterion(pred, labels) loss.backward() optimizer.step() print(f\u0026#34;epoch: {epoch}, loss: {loss.item()}\u0026#34;) # For GPU Training, set `use_gpu` to True. use_gpu = False trainer = TorchTrainer( train_func_distributed, scaling_config=ScalingConfig(num_workers=4, use_gpu=use_gpu) ) results = trainer.fit() References https://rise.cs.berkeley.edu/blog/ray-tips-for-first-time-users/ Ray: A Distributed Framework for Emerging AI Applications https://github.com/dmatrix/ray-core-tutorial ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/rl/ray/","summary":"\u003ch3 id=\"promise-and-future\"\u003ePromise and Future\u003c/h3\u003e\n\u003cp\u003eBefore diving deep into Ray, I\u0026rsquo;ll first give a brief introduction to the async ops in programming in C++.\nAn asynchronous call delegates time-consuming or blocking tasks to other threads, thereby ensuring the current thread\u0026rsquo;s responsiveness. Concretely, it involves the current thread delegating a task to another thread for execution. The current thread continues executing its own tasks without waiting for the delegated task\u0026rsquo;s result. The result of the delegated task is only required at some point in the future when it is needed.\u003c/p\u003e\n\u003cp\u003eAn asynchronous operation is created, executed by another thread, and upon completion, returns a result. The creator of the asynchronous call retrieves this result when needed. To meet these requirements, C++ provides std::future and std::promise. The relation is shown in the figure below.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"Promise and future\" src=\"images/image.png\" width=\"90%\"/\u003e\n    \u003cem\u003ePromise and future\u003c/em\u003e\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003cp\u003eWhen an asynchronous call is created, an instance of std::future is returned to the creator of the asynchronous call (receiver). Meanwhile, the executor of the asynchronous call (sender) holds an instance of std::promise. The executor uses std::promise to fulfill its promise (a commitment to deliver the result at a future point in time after the operation is completed), while the creator uses std::future to obtain this future value (the result corresponding to the promise fulfilled in the future). The std::promise instance held by the executor and the std::future instance held by the creator are both connected to a shared object. This shared object establishes a communication channel for information synchronization between the creator and the executor of the asynchronous call, enabling both parties to exchange information about the execution status of the asynchronous operation through this channel.\u003c/p\u003e","title":"Ray"},{"content":"1. Multiprocess Communication in Python In multiprocessing, a pipe is a connection between two processes in Python. It is used to send data from one process which is received by another process.\nUnder the covers, a pipe is implemented using a pair of connection objects, provided by the multiprocessing.connection.Connection class.\nCreating a pipe will create two connection objects, one for sending data and one for receiving data. A pipe can also be configured to be duplex so that each connection object can both send and receive data.\nThe Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex (two-way).\n1.1 How Do Pipes Work? We always use a pipe with the system call fork() that creates a new process. As we guess, there is no point to use pipes when we have only one process. The figure below represents how we can have a two-way pipe between the parent and child process when we don’t close unnecessary file descriptors:\nWhen we close the unused file descriptors we will have a figure like the below. As we can see from the correct version in the figure below both the parent process and child process can read and write to pipes when we use pipe and fork. However, since the pipe is unidirectional we should be careful if we want the communicate unidirectionally. That means both parent and child can send data to each other. In that case, one pipe wouldn’t work and that’s why we would need two pipes. One pipe for data flow from parent to child, and one pipe from data flow from child to parent. We should also close the unneeded pipe descriptors:\nAs we’ve said pipes are a more suitable IPC method for related processes. Because communication should be simple enough to use raw binary bytes. Actually, pipes that we use in a shell script are the best application of pipes. What they do is that they basically execute binary programs. So, the limitation of pipe is obvious that we can apply it only to related processes and we can have one-to-one communication.\nFor more advanced IPC, there are of course some other ways like shared memory, message queue, and sockets.\nAs is shown below, there are three steps to create a pipe: Three steps to build a pipe.\nCreate a pipe Fork process and pipe connection Close not used connection Below is a small code snippet to show how pipe works.\nfrom multiprocessing import Pipe, Process def son_process(x, pipe): _out_pipe, _in_pipe = pipe # 关闭fork过来的输入端 _in_pipe.close() while True: try: msg = _out_pipe.recv() print(msg) except EOFError: # 当out_pipe接受不到输出的时候且输入被关闭的时候，会抛出EORFError，可以捕获并且退出子进程 break if __name__ == \u0026#39;__main__\u0026#39;: out_pipe, in_pipe = Pipe(True) son_p = Process(target=son_process, args=(100, (out_pipe, in_pipe))) son_p.start() # 等 pipe 被 fork 后，关闭主进程的输出端 # 这样，创建的Pipe一端连接着主进程的输入，一端连接着子进程的输出口 out_pipe.close() for x in range(1000): in_pipe.send(x) in_pipe.close() son_p.join() print(\u0026#34;Main process is done\u0026#34;) 2. Sockets Sockets have a significant role in today’s internet. The term socket is first coined in RFC 147 in 1971 when it was used in the ARPANET. It is a unique identification to or from which information is transmitted in the network. Today’s modern implementations of sockets come from the Berkeley sockets. Sockets are directly related to the operating systems and processes and we can understand this situation from the Berkeley sockets application programming interface (API) in the Berkeley Software Distribution (BSD) which originated from Unix OS.\nThe network protocol stack’s API establishes a connection for each socket generated by an application which is a socket descriptor. It is like a file descriptor in Unix-like operating systems. The process saves it for use with read and write operations on the channel.\n2.1. How Do Sockets Work? A network socket is bound to a combination of a kind of network protocol to be used for transmissions. This combination includes the host’s network address and a port number. Ports are numbered resources on the node that indicate a different sort of software structure. They identify the service types for processes and act as an externally accessible location component, allowing other hosts to connect to them. We can use network sockets to establish a permanent connection between two nodes or to engage connectionless and multicast communications.\nTo sum up, with sockets we can establish a connection between processes that runs even on different machines. The socket API supports send and recv operations that allow processes to share message buffers in and out of the kernel-level communication buffer.\nThe socket call allows us to create a kernel-level socket buffer. Also, it associates any kernel-level processing that needs to be associated with the socket along with the actual message movement. As we’ve mentioned, when we use sockets to establish communication, it can happen between processes on different machines.\n3. Differences Between Pipes and Sockets We’ve explained pipes and sockets and tried to give intuition about how they work. As we’ve seen, they have quite different roles when we try to establish a connection between processes. It really depends on the situation and the problem which one is more suitable to use. However, we can underline some of the differences between them:\nWhile communication in pipes is uni-directional, in sockets communication, it is bidirectional. In order to establish communication between processes with pipes, processes should be related to each other. They should have a relationship like a parent and a child process. However, we don’t have such a restriction for sockets. The other important difference is that we can use pipes to connect processes on the same physical machine. On the other side, we use sockets to establish connections between processes on different physical machines. That’s why they are one of the fundamental concepts in network systems. There isn’t any concept of packaging in pipes. Sockets can have packages through communication using IPv4 or IPv6. While sockets can divide the big size of data into smaller chunks and send it in that way, pipes aren’t able to do that. References https://realpython.com/python-data-classes/ https://www.baeldung.com/cs/pipes-vs-sockets ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/programming_language/python/pipe/","summary":"\u003ch2 id=\"1-multiprocess-communication-in-python\"\u003e1. Multiprocess Communication in Python\u003c/h2\u003e\n\u003cp\u003eIn multiprocessing, a pipe is a connection between two processes in Python. It is used to send data from one process which is received by another process.\u003c/p\u003e\n\u003cp\u003eUnder the covers, a pipe is implemented using a pair of connection objects, provided by the multiprocessing.connection.Connection class.\u003c/p\u003e\n\u003cp\u003eCreating a pipe will create two connection objects, one for sending data and one for receiving data. A pipe can also be configured to be duplex so that each connection object can both send and receive data.\u003c/p\u003e\n\u003cp\u003eThe Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex (two-way).\u003c/p\u003e\n\u003ch3 id=\"11-how-do-pipes-work\"\u003e1.1 How Do Pipes Work?\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eWe always use a pipe with the system call \u003cem\u003efork()\u003c/em\u003e that creates a new process.\u003c/strong\u003e As we guess, there is no point to use pipes when we have only one process. The figure below represents how we can have a two-way pipe between the parent and child process when we don’t close unnecessary file descriptors:\u003c/p\u003e","title":"Python Pipe in Multiprocessing"},{"content":"In this blog, we talk about Monte Carlo Tree Search, the algorithm behind very popular AlphaZero.\nDuel Process Human cognition has a duel process model which suggests that human reasoning has two modes: System 1 is a fast, unconscious and automatic mode of thought,like intuition. System 2 is a slow, conscious, explicit and rule-based mode of reasoning.\nComparing with how LLM works, we can think about that token-level generation (greedy decoding) is like System 1 mode, and agent planning, lookahead and backtracks is the System 2 mode. MCTS is the algorithm we can leverage to achieve the system 2 deliberation through exploration, proposing solution and evaluation.\nMCTS The main concept of MCTS is a search. Search is tree traversals of the game tree. Single traversal is a path from a root node (current game state) to a node that is not fully expanded. Node being not-fully expanded means at least one of its children is unvisited, not explored. Once not fully expanded node is encountered, one of its unvisited children is chosen to become a root node for a single playout/simulation. The result of the simulation is then propagated back up to the current tree root updating game tree nodes statistics. Once the search (constrained by time or computational power) terminates, the move is chosen based on the gathered statistics. Thus, the algorithm of the tree traversal in MCTS follows the following steps:\nSelection: select an unvisited node based on tree policy Expansion: whether to expand a node or skip it if it\u0026rsquo;s visited Simulation/Evaluation: a full play starts in current node (representing game state) and ends in a terminal node where game result can be computed. One simulation expansion is shown below. Backpropagation: Backpropagate result to all nodes in the traversal chain up to the current game tree root node. Figure 1. Simulation in MCTS, image from Ref 3 Modeling From the above process, we can see that there are two key problems that needs to be addressed in MCTS.\nFirst is the node selection. How do we select which node to explore. Basically this defines how we explore all action space. Second simulation. How do we get the current traversal evaluated. Before answering these question, we first have to keep record of the game tree traversal simulation results. We\u0026rsquo;ll need to maintain a few values for each node:\n– $Q(v)$ is total simulation reward is an attribute of a node $v$ and in a simplest form is a sum of simulation results that passed through considered node.\n– $N(v)$ is total number of visits. It\u0026rsquo;s another atribute of a node $v$ representing a counter of how many times a node has been on the backpropagation path (and so how many times it contributed to the total simulation reward)\nThese node statistics reflects the exploitation and exploration in the algorithm. Nodes with high reward are good candidates to follow (exploitation) but those with low amount of visits may be interesting too as they are not explored well.\nSelection UCT Upper Confidence Bound applied to trees (UCT) is the function used for node selection when traverse the tree. For node $v$ with child node $v_i$, we define the UCT function as follows: $$ UCT(v_i, v) = \\frac{Q(v_i)}{N(v_i)} + c \\sqrt{\\frac{log(N(v))}{N(v_i)}} $$\nwhere $c$ is a hyperparameter which is used to balance exploitation and exploration. The first part is exploitation component which qualifies the winning rate of a particular child. However, only exploitation component is not enough because it will lead to greedily exploring only those nodes that bring a single winning playout very early at the beginning of the search.\nThe second component of UCT called exploration component which favors nodes that have not been explored.\nPUCT Predictor UCT is from Ref 5, it adds a prior to the preference of a particular node. $$ PUCT(v_i, v) = \\frac{Q(v_i)}{N(v_i)} + c P(v, v_i) \\frac{\\sqrt{N(v)}}{1 + N(v_i)} $$\nAlphaZero Up to now, we only talked about the skeleton of the MCTS algorithm. The following part, we briefly talk about the plug-in part which is the value estimation and policy estimation.\nIn AlphaGo, the $Q(v_i)$ function is estimated using the fusion of RL value network and customized fast rollout of supervised training network. In AlphaZero, the customized fast rollout is removed replaced with a single 19-layer CNN Residual neural network.\nThe prior probability of the move (transition from $v$ to $v_i$ ) is estimated through a policy network.\nIn AlphaZero, one network is served as both the policy network and value network. The CNN residual neural network has two heads one is used to predict action and the other is to predict value.\nThe training of AlphaZero involves three steps:\nLeveraging MCTS to collect self-play game data Use the data from step 1 to train policy and value networks. The goal of the training is to make policy and value network estimation as close to MCTS result as possible. Use the new models to generate data Figure 2. AlphaZero References Thinking Fast and Slow with Deep Learning and Tree Search Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm https://int8.io/monte-carlo-tree-search-beginners-guide Bandit based Monte-Carlo Planning Multi-armed bandits with episode context https://github.com/opendilab/LightZero AlphaGo: Mastering the game of Go with deep neural networks and tree search AlphaZero: Mastering the game of go without human knowledge A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play Monte Carlo Tree Search: A Review of Recent Modifications and Applications ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/mcts/","summary":"\u003cp\u003eIn this blog, we talk about Monte Carlo Tree Search, the algorithm behind very popular AlphaZero.\u003c/p\u003e\n\u003ch3 id=\"duel-process\"\u003eDuel Process\u003c/h3\u003e\n\u003cp\u003eHuman cognition has a duel process model which suggests that human reasoning has two modes: System 1 is a fast, unconscious and automatic mode of thought,like intuition. System 2 is a slow, conscious, explicit and rule-based mode of reasoning.\u003c/p\u003e\n\u003cp\u003eComparing with how LLM works, we can think about that token-level generation (greedy decoding) is like System 1 mode, and agent planning, lookahead and backtracks is the System 2 mode. MCTS is the algorithm we can leverage to achieve the system 2 deliberation through exploration, proposing solution and evaluation.\u003c/p\u003e\n\u003ch3 id=\"mcts\"\u003eMCTS\u003c/h3\u003e\n\u003cp\u003eThe main concept of MCTS is a search. Search is tree traversals of the game tree. Single traversal is a path from a root node (current game state) to a node that is not fully expanded. Node being not-fully expanded means at least one of its children is unvisited, not explored. Once not fully expanded node is encountered, one of its unvisited children is chosen to become a root node for a single playout/simulation. The result of the simulation is then propagated back up to the current tree root updating game tree nodes statistics. Once the search (constrained by time or computational power) terminates, the move is chosen based on the gathered statistics. Thus, the algorithm of the tree traversal in MCTS follows the following steps:\u003c/p\u003e","title":"Monte Carlo Tree Search"},{"content":"In the code pre-training, it is often necessary to generate corresponding inserted content based on both the left context and right context. Thus, in code pretraining we have an additional task called fill-in-the-middle.\nTraining Data Format With a certain probability $p$ called the FIM rate, documents are cut into three parts: prefix, middle, and suffix. For PSM format, it arrange the text as the following format\n\u0026lt;PRE\u0026gt; ◦ Enc(prefix) ◦ \u0026lt;SUF\u0026gt; ◦ Enc(suffix) ◦ \u0026lt;MID\u0026gt; ◦ Enc(middle) \u0026lt;PRE\u0026gt;, \u0026lt;SUF\u0026gt; and \u0026lt;MID\u0026gt; are special sentinel tokens.\nAccordingly, SPM format we swap the order of prefix and suffix. At training time, we can jointly do both PSM and SPM training. At inference time, we can choose either as the inference format.\nFIM style training gives us the opportunity to explore more training paradigms with code data. For instance, constructing training data, we can parse the code into an abstract syntax tree (AST) and randomly select a complete node to construct a FIM task. The rationale is simple. Through this training, model could learn to predict from top-down or bottom-up. This is exactly what is needed for reasoning.\nAnother benefit is that model\u0026rsquo;s predictions are more complete, with the generated code having a full hierarchical structure.\nBuild Training Data Given the fact that code data is mostly from github which already comes with some meta information. We could leverage these meta info and static analysis tools to build high quality training dataset. Here are the 7 steps to build AIxcoder training data.\nRaw Data Selection Exclude projects under copyleft licenses. Deduplicate projects gathered from various code hosting platforms and open-source datasets Project-Level Comprehensive Ranking Calculate project metrics, including the number of Stars, Git Commit counts, and the quantity of Test files. Exclude the lowest 10% of data based on a comprehensive score. Code File-Level Filtering Remove automatically generated code. Employ near-deduplication for redundancy removal. Sensitive Information Removal Use named entity recognition models to identify and delete sensitive information such as names, IP addresses, account passwords, and URLs. Commented Code Randomly deleting large sections of commented code Syntax Analysis Delete code with syntax parsing errors or syntactical errors in the top fifty languages. Static Analysis Utilize static analysis tools to scan for and locate 161 types of Bugs affecting code reliability and maintainability, as well as 197 types of vulnerabilities impacting code security. References Efficient Training of Language Models to Fill in the Middle DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence https://github.com/aixcoder-plugin/aiXcoder-7B ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/coder_training/","summary":"\u003cp\u003eIn the code pre-training, it is often necessary to generate corresponding inserted content based on both the left context and right context. Thus, in code pretraining we have an additional task called fill-in-the-middle.\u003c/p\u003e\n\u003ch3 id=\"training-data-format\"\u003eTraining Data Format\u003c/h3\u003e\n\u003cp\u003eWith a certain probability $p$ called the FIM rate, documents are cut into three parts: prefix, middle, and suffix. For PSM format,\nit arrange the text as the following format\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;PRE\u0026gt; ◦ Enc(prefix) ◦ \u0026lt;SUF\u0026gt; ◦ Enc(suffix) ◦ \u0026lt;MID\u0026gt; ◦ Enc(middle)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003e\u0026lt;PRE\u0026gt;\u003c/code\u003e, \u003ccode\u003e\u0026lt;SUF\u0026gt;\u003c/code\u003e and \u003ccode\u003e\u0026lt;MID\u0026gt;\u003c/code\u003e are special sentinel tokens.\u003c/p\u003e\n\u003cp\u003eAccordingly, SPM format we swap the order of prefix and suffix. At training time, we can jointly do both PSM and SPM training. At inference time, we can choose either as the inference format.\u003c/p\u003e\n\u003cp\u003eFIM style training gives us the opportunity to explore more training paradigms with code data. For instance, constructing training data, we can parse the code into an abstract syntax tree (AST) and randomly select a complete node to construct a FIM task. The rationale is simple. Through this training, model could learn to predict from top-down or bottom-up. This is exactly what is needed for reasoning.\u003c/p\u003e","title":"Coder Training"},{"content":"In PyTorch, autograd keeps a record of data (tensors) \u0026amp; all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, we can automatically compute the gradients using the chain rule.\nIn a forward pass, autograd does two things simultaneously: run the requested operation to compute a resulting tensor, and maintain the operation’s gradient function in the DAG. The backward pass kicks off when .backward() is called on the DAG root. autograd then: computes the gradients from each grad_fn, accumulates them in the respective tensor’s .grad attribute, and using the chain rule, propagates all the way to the leaf tensors. From this, we can know that when we call functions like torch.distributed.all_gather, the resulting tensors do not propagate back gradients. This can be verified with the following code snippet.\nimport os import torch from torch import nn batch_size = 16 rank = int(os.environ.get(\u0026#39;OMPI_COMM_WORLD_RANK\u0026#39;, \u0026#39;0\u0026#39;)) world_size = int(os.environ.get(\u0026#39;OMPI_COMM_WORLD_SIZE\u0026#39;, \u0026#39;1\u0026#39;)) bs_each = batch_size // world_size device_id = int(os.environ.get(\u0026#39;OMPI_COMM_WORLD_LOCAL_RANK\u0026#39;, \u0026#39;0\u0026#39;)) torch.cuda.set_device(device_id) torch.distributed.init_process_group( backend=\u0026#39;nccl\u0026#39;, init_method=\u0026#39;tcp://localhost:12345\u0026#39;, rank=rank, world_size=world_size, ) model = nn.Linear(1, 1, bias=False) model.weight.data[:] = 1. model = model.cuda() x = torch.ones((bs_each, 1), requires_grad=True).cuda() y = model(x) ys = [torch.zeros_like(y) for i in range(world_size)] torch.distributed.all_gather(ys, y) print(y.grad_fn) #\u0026lt;MmBackward object at 0x7ff10dfea500\u0026gt; for x in ys: print(x.grad_fn) # None print(x.requires_grad) # False Here we talk about how to use all_gather function in the pytorch so that we could still leverage auto_grad to help us for backpropagation.\nSolution One We can wrap the all_gather function and pass the context information to the gathered tensor.\nimport torch import torch.distributed as dist class GatherLayer(torch.autograd.Function): \u0026#34;\u0026#34;\u0026#34;Gather tensors from all process, supporting backward propagation.\u0026#34;\u0026#34;\u0026#34; @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = [torch.zeros_like(input) for _ in range(dist.get_world_size())] dist.all_gather(output, input) return tuple(output) @staticmethod def backward(ctx, *grads): (input,) = ctx.saved_tensors grad_out = torch.zeros_like(input) grad_out[:] = grads[dist.get_rank()] return grad_out Solution Two As shown below, we put the auto_grad captured tensor back to the gather tensor. In this way, this specific element on current rank will have gradient.\nall_x = [torch.zeros_like(x) for _ in range(world_size)] torch.distributed.all_gather(all_x, x) all_x[rank] = x References https://github.com/Spijkervet/SimCLR https://github.com/princeton-nlp/SimCSE ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/pytorch/all_gather_and_gradient_backpropagation/","summary":"\u003cp\u003eIn PyTorch, autograd keeps a record of data (tensors) \u0026amp; all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, we can automatically compute the gradients using the chain rule.\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\nIn a forward pass, autograd does two things simultaneously:\n\u003cul\u003e\n\u003cli\u003erun the requested operation to compute a resulting tensor, and\u003c/li\u003e\n\u003cli\u003emaintain the operation’s gradient function in the DAG.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!--   # to block the following text block to generate p tag and creating extra space with listing items.  --\u003e\n\u003cp\u003e\u003c/p\u003e\nThe backward pass kicks off when .backward() is called on the DAG root. autograd then:\n\u003cul\u003e\n\u003cli\u003ecomputes the gradients from each \u003ca href=\"https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00\"\u003egrad_fn\u003c/a\u003e,\u003c/li\u003e\n\u003cli\u003eaccumulates them in the respective tensor’s .grad attribute, and\u003c/li\u003e\n\u003cli\u003eusing the chain rule, propagates all the way to the leaf tensors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFrom this, we can know that when we call functions like \u003ccode\u003e torch.distributed.all_gather\u003c/code\u003e, the resulting tensors do not propagate back gradients. This can be verified with the following code snippet.\u003c/p\u003e","title":"All Gather and Gradients"},{"content":"Model Evaluation Robustness LLMs performance is sensitive to evaluation details. One of my previous co-workers\u0026rsquo;s work shows that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions.\nPAL for Math Reasoning In PAL paper, the authors found that solving mathematical problems using external tools (Python interpreter) could greatly boost math reasoning performance.\nReferences When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/model_eval/","summary":"\u003ch2 id=\"model-evaluation-robustness\"\u003eModel Evaluation Robustness\u003c/h2\u003e\n\u003cp\u003eLLMs performance is sensitive to evaluation details. One of my previous co-workers\u0026rsquo;s work shows that for popular multiple choice question\nbenchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions.\u003c/p\u003e\n\u003ch2 id=\"pal-for-math-reasoning\"\u003ePAL for Math Reasoning\u003c/h2\u003e\n\u003cp\u003eIn \u003ca href=\"https://arxiv.org/pdf/2211.10435.pdf\"\u003ePAL\u003c/a\u003e paper, the authors found that solving mathematical problems using external tools (Python interpreter) could greatly boost math reasoning performance.\u003c/p\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2402.01781\"\u003eWhen Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2305.14596.pdf\"\u003eIncreasing Probability Mass on Answer Choices Does Not Always Improve Accuracy\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- https://github.com/deepseek-ai/DeepSeek-Coder/tree/main/Evaluation/PAL-Math --\u003e","title":"Model Evaluation"},{"content":"The biggest lesson we\u0026rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we\u0026rsquo;ll closely examine the Mixtral model to study MoE models.\nIntroduction Most of today\u0026rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:\nFigure 1. Switch MoE Model In these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.\nDynamic Routing There are a couple of ways to design router to route tokens to each expert. Ideally, we want to design a router that could make each expert specialize one of domains/tasks. Obviously there is no straightforward way to achieve this. In Mixtral, softmax-topk based gating mechanism is used to select experts.\nFor any input $x$ of dimension $[\\text{sequence\\_len}, \\text{dim}]$, it multiplies with a gate matrix $W$ of shape $[\\text{dim}, 8]$, then we get a router representation of shape $[\\text{sequence\\_len}, 8]$. It selects top k (num of experts per token) logits which then go through softmax op to normalize to get k experts weights. In Mixtral, the k is equal to 2.\nMoE training is prone to instability issues because it has extra exponential functions. To deal with mixed precision roundoff errors, people apply z-loss to logits before sending them to router.\n$$ L_z = \\frac{1}{B} \\sum_{i=1}^{B} (log\\sum_{j=1}^{N}e^{x_j^{x_i}})^2 $$\nIn python,\nz_loss = torch.mean(torch.square(torch.logsumexp(logits, dim=-1))) * z_loss_coeff Ref [7] uses the similar kind of approach to stabilize the training. $$ L_{max_z} = 2 e^{-4} * z^2 $$ where $z$ is the max logit value.\nLoad Balancing For dynamic routing, which token is routed to which expert is unknown upfront, so there exists the load balancing issue. Common solution is to add an load balancing loss.\nTraining Directly training MoE could be challenging due to low efficiency. One popular approach is called sparse upcycling to use pretrained dense model to initialize the sparse model and continue to train for certain steps.\nFigure 2. Training MoE Model (To be continued)\nPublic Implementations https://github.com/XueFuzhao/OpenMoE https://github.com/pjlab-sys4nlp/llama-moe https://github.com/NVIDIA/NeMo https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe https://github.com/stanford-futuredata/megablocks References Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity BASE Layers: Simplifying Training of Large, Sparse Models Mixtral of Experts Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference Baichuan 2: Open Large-scale Language Models ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/moe_models/","summary":"\u003cp\u003eThe biggest lesson we\u0026rsquo;ve learnt in the past few years is that scaling is the key for model performance. Scaling without sacrificing inference speed makes Mixture of Export (MoE) models very appealing for large language model training. Today we\u0026rsquo;ll closely examine the Mixtral model to study MoE models.\u003c/p\u003e\n\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eMost of today\u0026rsquo;s MoE model are following an architecture that is similar to the Switch Transformer [1] which is shown below:\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"MoE model\" src=\"images/moe.png\" width=\"80%\" height=auto/\u003e \n    Figure 1. Switch MoE Model\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003cp\u003eIn these models, the sparsity lies in the feed-forward layer for the Transformer block. Switch is using one expert out of 4 experts for each token. For models like Mixtral/Grok both are using two experts out of 8 experts. Router dynamically chooses experts for each token. Can we route different samples to different experts? The answer is yes, however, coarse-grained design (giving less flexibility for model to learn the pattern) usually leads to worse performance.\u003c/p\u003e","title":"MoE Models"},{"content":"LLMs has made remarkable progress these days, however, they still exhibit notable limitations. Among these, hallucination is one of the most seen issues. In other words, the generations from LLMs are not grounded. To this end, people are turning to retrieval augmented generation to tackle the issue. In this blog, let’s roll up our sleeves and dive deep into the retrieval augmented system.\nRAG system contains: chunking, indexing, querying and generation. Chunking and indexing both are the offline processes which is the crucial data modeling phase. Querying and generation are online processes.\nRAG system for QA, image from [1] Indexing Retrieval Retrieval is the online process where the system converts user query into vector representation and retrieve relevant documents.\nRetrieval Evaluation Metric Like recommender system, retrieval system commonly use the following evaluation metrics.\nHit ratio (hit@k), Normalized Discounted Cumulative Gain (NDCG), Precision@k, Recall@k Mean Reciprocal Rank (MRR) Mean Average Precision (MAP) Hit@k Hit@k sometimes is also called Top-k accuracy. It is the percentage of search queries for each of which at least one item from the ground truth is returned within the top-k results. Simply put, it means % of queries get answer hit at top k retrieved passages. (Answer hit means user clicked on the doc). This number is meaningful when there are multiple test cases.\nNDCG Normalized Discounted Cumulative Gain (NDCG) is popular method for measuring the quality of a set of search results. It asserts the following:\nVery relevant results are more useful than somewhat relevant results which are more useful than irrelevant results (cumulative gain) Relevant results are more useful when they appear earlier in the set of results (discounting). The result of the ranking should be irrelevant to the query performed (normalization). Cumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. Suppose you were presented with a set of search results for a query and asked to rank each result. Given a true relevance score $R_i$ (real value) for every item, there exist several ways to define a gain. One of the most popular is: $$ G_i = 2^{R_i} - 1 $$\nwhere $i$ is the rank position/index of the item. The relevance score can be defined by ourselves, for instance, we can say: 0 =\u0026gt; Not relevant 1 =\u0026gt; Near relevant 2 =\u0026gt; Relevant. In practical use case, it\u0026rsquo;s defined as a binary value 0 and 1 where 0 is irrelevant and 1 is there is relevance.\nThe cumulative gain is then defined as: $$ CG@k = \\sum_{i=1}^k G_i $$\nTo achieve the Discounted cumulative gain (DCG) we must discount results that appear lower. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. Thus $DCG$ is: $$ DCG@k = \\sum_{i=1}^k\\frac{2^{R_i} - 1}{log_2{(i + 1)}} $$\nAs DCG either goes up with $k$ or it stays the same, the queries that return larger result sets will always have higher DCG scores than queries that return small result sets. To make comparison across queries fairer is we want to normalize the DCG score by the maximum possible DCG for different $k$. $$ NDCG@k = \\frac{DCG@k}{IDCG@k} $$ where $IDCG@k$ is the best $DCG$ we can get at position $k$.\nPrecision@k and Recall@k Precision@k measures the percentage of relevant results among top k results. At the same time, recall@k evaluates the ratio of relevant results among top k to the total number of relevant items in the whole dataset. $$ Precision@k = \\frac{\\text{number of recommended relevant items among top k}}{\\text{number of recommended items k}} $$ $$ Recall@k = \\frac{\\text{number of recommended relevant items among top k}}{\\text{number of all relevant items in the system}} $$\nMRR MRR measures “Where is the first relevant item?”. Given a query and a list of returned items, we find the rank (position $p_i$) of the first relevant items. We take the inverse of the rank to get the so-called reciprocal rank. For mean reciprocal rank, we just take average of reciprocal rank for all queries.\nMRR MRR Pros\nThis method is simple to compute and is easy to interpret. This method puts a high focus on the first relevant element of the list. It is best suited for targeted searches such as users asking for the “best item for me”. Good for known-item search such as navigational queries or looking for a fact. MRR Cons\nThe MRR metric does not evaluate the rest of the list of recommended items. It focuses on a single item from the list. It gives a list with a single relevant item just a much weight as a list with many relevant items. It is fine if that is the target of the evaluation. This might not be a good evaluation metric for users that want a list of related items to browse. The goal of the users might be to compare multiple related items. MAP The P@N decision support metric calculates the fraction of n recommendations that are good. The drawback of this metric is that it does not consider the recommended list as an ordered list. Precision@k considers the whole list as a set of items, and treats all the errors in the recommended list equally. The goal is to cut the error in the first few elements rather than much later in the list. For this, we need a metric that weights the errors accordingly. The goal is to weight heavily the errors at the top of the list. Then gradually decrease the significance of the errors as we go down the lower items in a list. The Average Prediction (AP) metric tries to approximate this weighting sliding scale.\nAverage Precision (AP) is a metric about how a single sorted prediction compares with the ground truth. i.e., AP tells how correct a single ranking of documents is, with respect to a single query. Thus, MAP is meaningful when there are multiple test cases.\nAP is calculated as the average of precision@k over the list.\nMAP TREC_EVAL TREC_EVAL is an evaluation tool which is used to evaluate an Information Retrieval system. TREC_EVAL requires two files: one is qrels (query relevance) and the other is retrieval output that needs to be evaluated.\ntrec_eval [-q] [-a] trec_rel_file trec_top_file Where trec_eval is the executable name for the code, -q is a parameter specifying detail for all queries, -a is a parameter specifying summary output only (-a and –q are mutually exclusive), trec_rel_file is the qrels, trec_top_file is the results file.\nThe qrels file\nThis file contains a list of documents considered relevants for each query. This relevance judgement is made by human beings who manually select documents that should be retrieved when a particular query is executed. This file can be considered as the \u0026ldquo;correct answer\u0026rdquo; and the documents retrieved by your IR system should approximate the maximum to it. It has the following format:\nquery-id 0 document-id relevance\nThe field query-id is a alphanumeric sequence to identify the query, document-id is a alphanumeric sequence to identify the judged document and the field relevance is a number to indicate the relevance degree between the document and query (0 for non relevant and 1 for relevant). The second field \u0026ldquo;0\u0026rdquo; is not currently used, just put it in the file. The fields can be separated by a blank space or tabulation.\nThe results file\nThe results file contains a ranking of documents for each query automaticaly generated by your application. This is the file that will evaluated by trec_eval based in the \u0026ldquo;correct answer\u0026rdquo; provided by the first file. This file has the following format:\nquery-id Q0 document-id rank score STANDARD\nThe field query-id is a alphanumeric sequence to identify the query. The second field, with \u0026ldquo;Q0\u0026rdquo; value, is currently ignored by trec_eval, just put it in the file. The field document-id is a alphanumeric sequence to identify the retrieved document. The field rank is an integer value which represents the document position in the ranking, but this field is also ignored by trec_eval. The field score can be an integer or float value to indicate the similarity degree between document and query, so the most relevants docs will have higher scores. The last field, with \u0026ldquo;STANDARD\u0026rdquo; value, is used only to identify this run (this name is also showed in the output), you can use any alphanumeric sequence.\nRetrieval Fusion The straightforward idea in the era is dense retrieval is that we could combine sparse retrieval and dense retrieval together. A common approach is to use reciprocal rank fusion. The RRF score of document $d$ is: $$ RRF_d = \\sum_r^R{\\frac{1}{c + r(d)}} $$ assuming there are $R$ ranking items and $r(d)$ is the rank of document $d$. Here $c$ is a constant.\nRRF, image from [4] Generation References [1] Retrieval-Augmented Generation for Large Language Models: A Survey [2] Evaluation Metrics for Ranking problems: Introduction and Examples [3] Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods [4] Advanced RAG Techniques: an Illustrated Overview\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/rag/","summary":"\u003cp\u003eLLMs has made remarkable progress these days, however, they still exhibit notable limitations. Among these, hallucination is one of the most seen issues. In other words, the generations from LLMs are not grounded. To this end, people are turning to retrieval augmented generation to tackle the issue. In this blog, let’s roll up our sleeves and dive deep into the retrieval augmented system.\u003c/p\u003e\n\u003cp\u003eRAG system contains: chunking, indexing, querying and generation. Chunking and indexing both are the offline processes which is the crucial data modeling phase. Querying and generation are online processes.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"mrr\" src=\"images/rag.png\" width=\"80%\"/\u003e\n    RAG system for QA, image from [1]\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003ch3 id=\"indexing\"\u003eIndexing\u003c/h3\u003e\n\u003ch3 id=\"retrieval\"\u003eRetrieval\u003c/h3\u003e\n\u003cp\u003eRetrieval is the online process where the system converts user query into vector representation and retrieve relevant documents.\u003c/p\u003e\n\u003ch4 id=\"retrieval-evaluation-metric\"\u003eRetrieval Evaluation Metric\u003c/h4\u003e\n\u003cp\u003eLike recommender system, retrieval system commonly use the following evaluation metrics.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHit ratio (hit@k),\u003c/li\u003e\n\u003cli\u003eNormalized Discounted Cumulative Gain (NDCG),\u003c/li\u003e\n\u003cli\u003ePrecision@k,\u003c/li\u003e\n\u003cli\u003eRecall@k\u003c/li\u003e\n\u003cli\u003eMean Reciprocal Rank (MRR)\u003c/li\u003e\n\u003cli\u003eMean Average Precision (MAP)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"hitk\"\u003eHit@k\u003c/h5\u003e\n\u003cp\u003eHit@k sometimes is also called Top-k accuracy. It is the percentage of search queries for each of which at least one item from the ground truth is returned within the top-k results. Simply put, it means % of queries get answer hit at top k retrieved passages. (Answer hit means user clicked on the doc). This number is meaningful when there are multiple test cases.\u003c/p\u003e","title":"Retrieval Augmented Generation"},{"content":"In order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.\nGPU Compute Model and Memory Hierarchy The Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.\nFigure 1. GPU memory Figure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit.\nFigure 2. GPU memory hierarchy IO-aware Computation First let\u0026rsquo;s take a look at the vallina attention computation which is shown below\nFigure 3. Vallina attention computation Essentially, each of the operation follows the three steps of operation below.\nRead op — Move tensor from HBM to SRAM Compute op - Perform compute intensive task on SRAM write op - move tensor back from SRAM to HBM The breakdown of these computation is as follows. Apparently, all these green ops in the vallina attention can be saved.\nFigure 4. Vallina attention computation break down However, it\u0026rsquo;s hard to put giant attention matrix of size [N x N] in the cache. The idea to solve this challenge is to use tiling. Concretely, we slice the matrices into smaller blocks and in each of Q K computation, we do it in a small block scale. The output of the small block thus can be saved on the cache. This sounds perfectly except that softmax op is not possible with small block computation. Lucklily there are already some studies dealing with this [1-2]. Before talking about this, let\u0026rsquo;s first revisit stable softmax computation.\nBlockwise Softmax Underflow in numerical computation can cause precision issue. Overflow can be more problematic because it usually leads to divergence of training job (some may argue silent error is more detrimental :)). Softmax operation involves exponential computation which without careful handling can easily lead to overflow (such as exp(2000)).\n$$ \\text{softmax}(x)_i = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}} $$\nSimilary, the cross entropy can be computed as\n$$ \\begin{aligned} H(p, q) \u0026amp;= -\\sum_i p_i\\log(q_i) \\\\ \u0026amp;= -1\\cdot\\log(q_y) -\\sum_{i \\neq y} 0\\cdot\\log(q_i) \\\\ \u0026amp;= -\\log(q_y) \\\\ \u0026amp;= -\\log(\\text{softmax}(\\hat{y})_y) \\\\ \\end{aligned} $$\nWhen $max(x)$ is very large, the numerator could become $0$, and $log$ computation could overflow. To prevent this, we can do one more step: $$ \\begin{aligned} \\log(\\text{softmax}(x)_i) \u0026amp;= \\log(\\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}) \\\\ \u0026amp;= x_i - \\max(x) - \\log(\\sum_j e^{x_j - \\max(x)}) \\end{aligned} $$\nBy simply extracting the max value, we limit the exponential values to be in [0, 1]. In Flashattention paper, the softmax is represented as follows:\nFigure 5. Softmax Then blockwise softmax can be computed as follows:\nFigure 6. Blockwise Softmax With saving some summary (i.e. max) statistics, the softmax op can be decomposed into blocks.\nRecomputation in Backpropagation With the fused kernel, we effectively do the computation outside Pytorch computation graph. Thus, we can\u0026rsquo;t use the AutoGrad for gradient computation in backpropagation. Consequently, we have to define the backpropagation by ourselves. The way to solve this is very simple as well. We just define our own backpropagation ops for fused kernel like gradient checkpointing.\nReferences [1] SELF-ATTENTION DOES NOT NEED O(n^2) MEMORY [2] Online normalizer calculation for softmax [3] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/flash_attn/","summary":"\u003cp\u003eIn order to understand how flash attention and its variants help improve compute efficiency of modern LLMs training, we first have to dive deep into GPU compute model and its memory hierarchy.\u003c/p\u003e\n\u003ch3 id=\"gpu-compute-model-and-memory-hierarchy\"\u003eGPU Compute Model and Memory Hierarchy\u003c/h3\u003e\n\u003cp\u003eThe Figure 1 here shows the high level compute model and memory in GPU. We can see that there are three types of memory affect GPU computation. CPU memory (data loading etc), GPU high bandwidth memory (the gpu memory we usually mentioned), and GPU caches (SRAM). These memories are of different size and bandwidth (read speed). The idea of flash attention is to design IO-aware fused computation kernel to save memory access to speed up training job.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"gpu memory\" src=\"images/gpu_mem.png\" width=\"80%\" height=auto/\u003e \n    \u003cem\u003eFigure 1. GPU memory\u003c/em\u003e\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003cp\u003eFigure 2 shows a more detailed hierarchy of GPU memory in A100. Notice that cache is specific to each compute unit.\u003c/p\u003e","title":"Flash Attention"},{"content":"A memory mapping is a region of the process’s virtual memory space that is mapped in a one-to-one correspondence with another entity. In this section, we will focus exclusively on memory-mapped files, where the memory of region corresponds to a traditional file on disk. For example, assume that the address 0xf77b5000 is mapped to the first byte of a file. Then 0xf77b5001 maps to the second byte, 0xf77b5002 to the third, and so on.\nWhen we say that the file is mapped to a particular region in memory, we mean that the process sets up a pointer to the beginning of that region. The process can the dereference that pointer for direct access to the contents of the file. Specifically, there is no need to use standard file access functions, such as read(), write(), or fseek(). Rather, the file can be accessed as if it has already been read into memory as an array of bytes. Memory-mapped files have several uses and advantages over traditional file access functions:\nMemory-mapped files allow for multiple processes to share read-only access to a common file. As a straightforward example, the C standard library (glibc.so) is mapped into all processes running C programs. As such, only one copy of the file needs to be loaded into physical memory, even if there are thousands of programs running. In some cases, memory-mapped files simplify the logic of a program by using memory-mapped I/O. Rather than using fseek() multiple times to jump to random file locations, the data can be accessed directly by using an index into an array. Memory-mapped files provide more efficient access for initial reads. When read() is used to access a file, the file contents are first copied from disk into the kernel’s buffer cache. Then, the data must be copied again into the process’s user-mode memory for access. Memory-mapped files bypass the buffer cache, and the data is copied directly into the user-mode portion of memory. If the region is set up to be writable, memory-mapped files provide extremely fast IPC data exchange. That is, when one process writes to the region, that data is immediately accessible by the other process without having to invoke a system call. Note that setting up the regions in both processes is an expensive operation in terms of execution time; however, once the region is set up, data is exchanged immediately. In contrast to message-passing forms of IPC (such as pipes), memory-mapped files create persistent IPC. Once the data is written to the shared region, it can be repeatedly accessed by other processes. Moreover, the data will eventually be written back to the file on disk for long-term storage. mmap file ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/distributed_training/data_processing/","summary":"\u003cp\u003eA memory mapping is a region of the process’s virtual memory space that is mapped in a one-to-one correspondence with another entity. In this section, we will focus exclusively on memory-mapped files, where the memory of region corresponds to a traditional file on disk. For example, assume that the address 0xf77b5000 is mapped to the first byte of a file. Then 0xf77b5001 maps to the second byte, 0xf77b5002 to the third, and so on.\u003c/p\u003e\n\u003cp\u003eWhen we say that the file is mapped to a particular region in memory, we mean that the process sets up a pointer to the beginning of that region. The process can the dereference that pointer for direct access to the contents of the file. Specifically, there is no need to use standard file access functions, such as read(), write(), or fseek(). Rather, the file can be accessed as if it has already been read into memory as an array of bytes. Memory-mapped files have several uses and advantages over traditional file access functions:\u003c/p\u003e","title":"Data Processing in Distributed Training"},{"content":"In this blog, we talk about distributed optimizer implementation. The discussion here is mostly based on Megatron-LM.\nAdam Optimizer Adaptive moment estimation is an algorithm to compute the adaptive learning rate for each parameters. It consists two parts: first-order momentum which is exponentially decaying average (moving average) of gradient and second-order momentum (variance, which controls adaptive learning rate) which is exponentially decaying average of squared gradient. $$ m_t = \\frac{\\beta_1}{1 - \\beta_{1}^{t}} m_{t-1} + \\frac{1 - \\beta_1}{1 - \\beta_{1}^{t}} g_{t} \\\\[5pt] v_t = \\frac{\\beta_2}{1 - \\beta_{2}^{t}} v_{t-1} + \\frac{1 - \\beta_2}{1 - \\beta_{2}^{t}} g_{t}^2 \\\\[5pt] u_t = \\frac{m_t}{\\sqrt{v_t} + \\epsilon} \\\\[5pt] \\theta_{t+1} = \\theta_t - \\eta_t u_t $$\nWe can think about the adaptive learning rate part monitors the historical update frequency for each parameter. For frequently updated parameters, we don\u0026rsquo;t want them to be updated very often with a single sample, thus, we would like to have a smaller learning rate. The updating frequency is measured by $v = \\sum g^2$.\nNote that adaptive learning sometimes can be problematic when training data is huge. The reason is that when $v$ monotonically increases, it could yield very small learning rate. Essentially, model won\u0026rsquo;t be able to learn anything.\nMemory Footprint in Training The full spectrum of memory consumption of training system can be categorized into three parts:\nModel weights Optimizer states Activations, temporary buffers and fragmented memory As most of modern training is done in mixed precision training (such as bf16 and fp32), so here our analysis will be based on these scenarios. When use the above Adam optimizer and assuming the model parameter is $M$, then the memory footprint could include:\n2M (model parameter in bf16) 2M (gradient in bf16) 4M (fp32 model parameter in optimizer state) 4M (fp32 gradient in optimizer state) 4M (fp32 grad moving avg) 4M (fp32 grad sq moving avg) In total, we need 20M memory to per replica in model training. For instance, for 7B model training, the above parameters and optimizer state will consume 140G memory. Refer to [1] for how to compute the activation part. Keeping a fp32 copy of model parameters for model update in optimizer state is important for performance as is shown in Gopher paper.\nDistributed Optimizer Distributed optimizer is to save memory by distributing the optimizer state evenly across data parallel ranks, versus the current method of replicating the optimizer state across data parallel ranks.\nTheoretical memory savings vary depending on the combination of the model\u0026rsquo;s param dtype and grad dtype. In the Megatron-LM implementation, the theoretical number of bytes per parameter is (where \u0026rsquo;d\u0026rsquo; is the data parallel size):\nNon-distributed optim Distributed optim float16 param, float16 grads 20 4 + 16/d float16 param, fp32 grads 18 6 + 12/d fp32 param, fp32 grads 16 8 + 8/d Data flow The following image shows the data flow of the optimizer. Here I\u0026rsquo;ll give a thorough walk-through about how this works.\nFigure 1. Distributed optimizer data flow Reference Reducing Activation Recomputation in Large Transformer Models ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/distributed_training/distributed_optimizer/","summary":"\u003cp\u003eIn this blog, we talk about distributed optimizer implementation. The discussion here is mostly based on Megatron-LM.\u003c/p\u003e\n\u003ch2 id=\"adam-optimizer\"\u003eAdam Optimizer\u003c/h2\u003e\n\u003cp\u003eAdaptive moment estimation is an algorithm to compute the adaptive learning rate for each parameters. It consists two parts: first-order momentum which is exponentially decaying average (moving average) of gradient and second-order momentum (variance, which controls adaptive learning rate) which is exponentially decaying average of squared gradient.\n$$\nm_t = \\frac{\\beta_1}{1 - \\beta_{1}^{t}} m_{t-1} + \\frac{1 - \\beta_1}{1 - \\beta_{1}^{t}} g_{t} \\\\[5pt]\nv_t = \\frac{\\beta_2}{1 - \\beta_{2}^{t}} v_{t-1} + \\frac{1 - \\beta_2}{1 - \\beta_{2}^{t}} g_{t}^2 \\\\[5pt]\nu_t = \\frac{m_t}{\\sqrt{v_t} + \\epsilon} \\\\[5pt]\n\\theta_{t+1} = \\theta_t - \\eta_t u_t\n$$\u003c/p\u003e\n\u003cp\u003eWe can think about the adaptive learning rate part monitors the historical update frequency for each parameter. For frequently updated parameters, we don\u0026rsquo;t want them to be updated very often with a single sample, thus, we would like to have a smaller learning rate. The updating frequency is measured by $v = \\sum g^2$.\u003c/p\u003e","title":"Distributed Optimizer"},{"content":"Light-weight LoRA fine-tuning seems to be the go-to option for many application where compute resources are limited.\nLoRA The way lora works is illustrated in the figure below. For some matrices in the transformer model, we add a parallel weighted matrix. The branch matrix can be decomposed into two smaller matrix. At training time, we only train these small matrices with original weights frozen.\nAt inference time, we don\u0026rsquo;t need to maintain separate parameters, thus we merge the LoRA weights into the original model weights.\nFigure 1. Lora Fine-tuning and Inference LoRA Fine-tuning with Deepspeed Huggingface PEFT package already provides easy-to-use APIs for LoRA fine-tuning. However, when we combine these with Deepspeed, we need to be careful when we merge model weights.\nBelow we assume we have Deepspeed checkpoints and we want to have inference model weights with LoRA parameters merged. We have to follow the following steps:\nConvert Zero checkpoint into a single shard fp32 checkpoint Load the original model before fine-tuning. Get peft config and get peft model using base model and peft config. This kind be done with PEFT API get_peft_model Load single shard zero ckpt from step 1 into step3 model definition do merge_and_unload and save pretrained model. Reference https://huggingface.co/docs/peft/main/en/conceptual_guides/lora LoRA: Low-Rank Adaptation of Large Language Models ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/distributed_training/lora_finetuning/","summary":"\u003cp\u003eLight-weight LoRA fine-tuning seems to be the go-to option for many application where compute resources are limited.\u003c/p\u003e\n\u003ch3 id=\"lora\"\u003eLoRA\u003c/h3\u003e\n\u003cp\u003eThe way lora works is illustrated in the figure below. For some matrices in the transformer model, we add a parallel weighted matrix.\nThe branch matrix can be decomposed into two smaller matrix. At training time, we only train these small matrices with original weights frozen.\u003c/p\u003e\n\u003cp\u003eAt inference time, we don\u0026rsquo;t need to maintain separate parameters, thus we merge the LoRA weights into the original model weights.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"lora\" src=\"images/lora.png\" width=\"80%\" height=auto/\u003e \n    Figure 1. Lora Fine-tuning and Inference\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003ch3 id=\"lora-fine-tuning-with-deepspeed\"\u003eLoRA Fine-tuning with Deepspeed\u003c/h3\u003e\n\u003cp\u003eHuggingface PEFT package already provides easy-to-use APIs for LoRA fine-tuning. However, when we combine these with Deepspeed, we need to be careful when we merge model weights.\u003c/p\u003e\n\u003cp\u003eBelow we assume we have Deepspeed checkpoints and we want to have inference model weights with LoRA parameters merged. We have to follow the following steps:\u003c/p\u003e","title":"LoRA Model Fine-tuning"},{"content":"Sama has a very good blog talking about how to manage one\u0026rsquo;s career. I copied it here.\nI’ve observed thousands of founders and thought a lot about what it takes to make a huge amount of money or to create something important. Usually, people start off wanting the former and end up wanting the latter.\nHere are 13 thoughts about how to achieve such outlier success. Everything here is easier to do once you’ve already reached a baseline degree of success (through privilege or effort) and want to put in the work to turn that into outlier success. [1] But much of it applies to anyone.\n1. Compound yourself\nCompounding is magic. Look for it everywhere. Exponential curves are the key to wealth generation.\nA medium-sized business that grows 50% in value every year becomes huge in a very short amount of time. Few businesses in the world have true network effects and extreme scalability. But with technology, more and more will. It’s worth a lot of effort to find them and create them.\nYou also want to be an exponential curve yourself—you should aim for your life to follow an ever-increasing up-and-to-the-right trajectory. It’s important to move towards a career that has a compounding effect—most careers progress fairly linearly.\nYou don\u0026rsquo;t want to be in a career where people who have been doing it for two years can be as effective as people who have been doing it for twenty—your rate of learning should always be high. As your career progresses, each unit of work you do should generate more and more results. There are many ways to get this leverage, such as capital, technology, brand, network effects, and managing people.\nIt’s useful to focus on adding another zero to whatever you define as your success metric—money, status, impact on the world, or whatever. I am willing to take as much time as needed between projects to find my next thing. But I always want it to be a project that, if successful, will make the rest of my career look like a footnote.\nMost people get bogged down in linear opportunities. Be willing to let small opportunities go to focus on potential step changes.\nI think the biggest competitive advantage in business—either for a company or for an individual’s career—is long-term thinking with a broad view of how different systems in the world are going to come together. One of the notable aspects of compound growth is that the furthest out years are the most important. In a world where almost no one takes a truly long-term view, the market richly rewards those who do.\nTrust the exponential, be patient, and be pleasantly surprised.\n2. Have almost too much self-belief\nSelf-belief is immensely powerful. The most successful people I know believe in themselves almost to the point of delusion.\nCultivate this early. As you get more data points that your judgment is good and you can consistently deliver results, trust yourself more.\nIf you don’t believe in yourself, it’s hard to let yourself have contrarian ideas about the future. But this is where most value gets created.\nI remember when Elon Musk took me on a tour of the SpaceX factory many years ago. He talked in detail about manufacturing every part of the rocket, but the thing that sticks in memory was the look of absolute certainty on his face when he talked about sending large rockets to Mars. I left thinking “huh, so that’s the benchmark for what conviction looks like.”\nManaging your own morale—and your team’s morale—is one of the greatest challenges of most endeavors. It’s almost impossible without a lot of self-belief. And unfortunately, the more ambitious you are, the more the world will try to tear you down. Most highly successful people have been really right about the future at least once at a time when people thought they were wrong. If not, they would have faced much more competition.\nSelf-belief must be balanced with self-awareness. I used to hate criticism of any sort and actively avoided it. Now I try to always listen to it with the assumption that it’s true, and then decide if I want to act on it or not. Truth-seeking is hard and often painful, but it is what separates self-belief from self-delusion.\nThis balance also helps you avoid coming across as entitled and out of touch.\n3. Learn to think independently\nEntrepreneurship is very difficult to teach because original thinking is very difficult to teach. School is not set up to teach this—in fact, it generally rewards the opposite. So you have to cultivate it on your own.\nThinking from first principles and trying to generate new ideas is fun, and finding people to exchange them with is a great way to get better at this. The next step is to find easy, fast ways to test these ideas in the real world.\n“I will fail many times, and I will be really right once” is the entrepreneurs’ way. You have to give yourself a lot of chances to get lucky.\nOne of the most powerful lessons to learn is that you can figure out what to do in situations that seem to have no solution. The more times you do this, the more you will believe it. Grit comes from learning you can get back up after you get knocked down.\n4. Get good at “sales”\nSelf-belief alone is not sufficient—you also have to be able to convince other people of what you believe.\nAll great careers, to some degree, become sales jobs. You have to evangelize your plans to customers, prospective employees, the press, investors, etc. This requires an inspiring vision, strong communication skills, some degree of charisma, and evidence of execution ability.\nGetting good at communication—particularly written communication—is an investment worth making. My best advice for communicating clearly is to first make sure your thinking is clear and then use plain, concise language.\nThe best way to be good at sales is to genuinely believe in what you’re selling. Selling what you truly believe in feels great, and trying to sell snake oil feels awful.\nGetting good at sales is like improving at any other skill—anyone can get better at it with deliberate practice. But for some reason, perhaps because it feels distasteful, many people treat it as something unlearnable.\nMy other big sales tip is to show up in person whenever it’s important. When I was first starting out, I was always willing to get on a plane. It was frequently unnecessary, but three times it led to career-making turning points for me that otherwise would have gone the other way.\n5. Make it easy to take risks\nMost people overestimate risk and underestimate reward. Taking risks is important because it’s impossible to be right all the time—you have to try many things and adapt quickly as you learn more.\nIt’s often easier to take risks early in your career; you don’t have much to lose, and you potentially have a lot to gain. Once you’ve gotten yourself to a point where you have your basic obligations covered you should try to make it easy to take risks. Look for small bets you can make where you lose 1x if you’re wrong but make 100x if it works. Then make a bigger bet in that direction.\nDon’t save up for too long, though. At YC, we’ve often noticed a problem with founders that have spent a lot of time working at Google or Facebook. When people get used to a comfortable life, a predictable job, and a reputation of succeeding at whatever they do, it gets very hard to leave that behind (and people have an incredible ability to always match their lifestyle to next year’s salary). Even if they do leave, the temptation to return is great. It’s easy—and human nature—to prioritize short-term gain and convenience over long-term fulfillment. But when you aren’t on the treadmill, you can follow your hunches and spend time on things that might turn out to be really interesting. Keeping your life cheap and flexible for as long as you can is a powerful way to do this, but obviously comes with tradeoffs.\n6. Focus\nFocus is a force multiplier on work.\nAlmost everyone I’ve ever met would be well-served by spending more time thinking about what to focus on. It is much more important to work on the right thing than it is to work many hours. Most people waste most of their time on stuff that doesn’t matter.\nOnce you have figured out what to do, be unstoppable about getting your small handful of priorities accomplished quickly. I have yet to meet a slow-moving person who is very successful.\n7. Work hard\nYou can get to about the 90th percentile in your field by working either smart or hard, which is still a great accomplishment. But getting to the 99th percentile requires both—you will be competing with other very talented people who will have great ideas and be willing to work a lot.\nExtreme people get extreme results. Working a lot comes with huge life trade-offs, and it’s perfectly rational to decide not to do it. But it has a lot of advantages. As in most cases, momentum compounds, and success begets success.\nAnd it’s often really fun. One of the great joys in life is finding your purpose, excelling at it, and discovering that your impact matters to something larger than yourself. A YC founder recently expressed great surprise about how much happier and more fulfilled he was after leaving his job at a big company and working towards his maximum possible impact. Working hard at that should be celebrated. It’s not entirely clear to me why working hard has become a Bad Thing in certain parts of the US, but this is certainly not the case in other parts of the world—the amount of energy and drive exhibited by entrepreneurs outside of the US is quickly becoming the new benchmark.\nYou have to figure out how to work hard without burning out. People find their own strategies for this, but one that almost always works is to find work you like doing with people you enjoy spending a lot of time with.\nI think people who pretend you can be super successful professionally without working most of the time (for some period of your life) are doing a disservice. In fact, work stamina seems to be one of the biggest predictors of long-term success.\nOne more thought about working hard: do it at the beginning of your career. Hard work compounds like interest, and the earlier you do it, the more time you have for the benefits to pay off. It’s also easier to work hard when you have fewer other responsibilities, which is frequently but not always the case when you’re young.\n8. Be bold\nI believe that it’s easier to do a hard startup than an easy startup. People want to be part of something exciting and feel that their work matters.\nIf you are making progress on an important problem, you will have a constant tailwind of people wanting to help you. Let yourself grow more ambitious, and don’t be afraid to work on what you really want to work on.\nIf everyone else is starting meme companies, and you want to start a gene-editing company, then do that and don’t second guess it.\nFollow your curiosity. Things that seem exciting to you will often seem exciting to other people too.\n9. Be willful\nA big secret is that you can bend the world to your will a surprising percentage of the time—most people don’t even try, and just accept that things are the way that they are.\nPeople have an enormous capacity to make things happen. A combination of self-doubt, giving up too early, and not pushing hard enough prevents most people from ever reaching anywhere near their potential.\nAsk for what you want. You usually won’t get it, and often the rejection will be painful. But when this works, it works surprisingly well.\nAlmost always, the people who say “I am going to keep going until this works, and no matter what the challenges are I’m going to figure them out”, and mean it, go on to succeed. They are persistent long enough to give themselves a chance for luck to go their way.\nAirbnb is my benchmark for this. There are so many stories they tell that I wouldn’t recommend trying to reproduce (keeping maxed-out credit cards in those nine-slot three-ring binder pages kids use for baseball cards, eating dollar store cereal for every meal, battle after battle with powerful entrenched interest, and on and on) but they managed to survive long enough for luck to go their way.\nTo be willful, you have to be optimistic—hopefully this is a personality trait that can be improved with practice. I have never met a very successful pessimistic person.\n10. Be hard to compete with\nMost people understand that companies are more valuable if they are difficult to compete with. This is important, and obviously true.\nBut this holds true for you as an individual as well. If what you do can be done by someone else, it eventually will be, and for less money.\nThe best way to become difficult to compete with is to build up leverage. For example, you can do it with personal relationships, by building a strong personal brand, or by getting good at the intersection of multiple different fields. There are many other strategies, but you have to figure out some way to do it.\nMost people do whatever most people they hang out with do. This mimetic behavior is usually a mistake—if you’re doing the same thing everyone else is doing, you will not be hard to compete with.\n11. Build a network\nGreat work requires teams. Developing a network of talented people to work with—sometimes closely, sometimes loosely—is an essential part of a great career. The size of the network of really talented people you know often becomes the limiter for what you can accomplish.\nAn effective way to build a network is to help people as much as you can. Doing this, over a long period of time, is what lead to most of my best career opportunities and three of my four best investments. I’m continually surprised how often something good happens to me because of something I did to help a founder ten years ago.\nOne of the best ways to build a network is to develop a reputation for really taking care of the people who work with you. Be overly generous with sharing the upside; it will come back to you 10x. Also, learn how to evaluate what people are great at, and put them in those roles. (This is the most important thing I have learned about management, and I haven’t read much about it.) You want to have a reputation for pushing people hard enough that they accomplish more than they thought they could, but not so hard they burn out.\nEveryone is better at some things than others. Define yourself by your strengths, not your weaknesses. Acknowledge your weaknesses and figure out how to work around them, but don’t let them stop you from doing what you want to do. “I can’t do X because I’m not good at Y” is something I hear from entrepreneurs surprisingly often, and almost always reflects a lack of creativity. The best way to make up for your weaknesses is to hire complementary team members instead of just hiring people who are good at the same things you are.\nA particularly valuable part of building a network is to get good at discovering undiscovered talent. Quickly spotting intelligence, drive, and creativity gets much easier with practice. The easiest way to learn is just to meet a lot of people, and keep track of who goes on to impress you and who doesn’t. Remember that you are mostly looking for rate of improvement, and don’t overvalue experience or current accomplishment.\nI try to always ask myself when I meet someone new “is this person a force of nature?” It’s a pretty good heuristic for finding people who are likely to accomplish great things.\nA special case of developing a network is finding someone eminent to take a bet on you, ideally early in your career. The best way to do this, no surprise, is to go out of your way to be helpful. (And remember that you have to pay this forward at some point later!)\nFinally, remember to spend your time with positive people who support your ambitions.\n12. You get rich by owning things\nThe biggest economic misunderstanding of my childhood was that people got rich from high salaries. Though there are some exceptions—entertainers for example —almost no one in the history of the Forbes list has gotten there with a salary.\nYou get truly rich by owning things that increase rapidly in value.\nThis can be a piece of a business, real estate, natural resource, intellectual property, or other similar things. But somehow or other, you need to own equity in something, instead of just selling your time. Time only scales linearly.\nThe best way to make things that increase rapidly in value is by making things people want at scale.\n13. Be internally driven\nMost people are primarily externally driven; they do what they do because they want to impress other people. This is bad for many reasons, but here are two important ones.\nFirst, you will work on consensus ideas and on consensus career tracks. You will care a lot—much more than you realize—if other people think you’re doing the right thing. This will probably prevent you from doing truly interesting work, and even if you do, someone else would have done it anyway.\nSecond, you will usually get risk calculations wrong. You’ll be very focused on keeping up with other people and not falling behind in competitive games, even in the short term.\nSmart people seem to be especially at risk of such externally-driven behavior. Being aware of it helps, but only a little—you will likely have to work super-hard to not fall in the mimetic trap.\nThe most successful people I know are primarily internally driven; they do what they do to impress themselves and because they feel compelled to make something happen in the world. After you’ve made enough money to buy whatever you want and gotten enough social status that it stops being fun to get more, this is the only force I know of that will continue to drive you to higher levels of performance.\nThis is why the question of a person’s motivation is so important. It’s the first thing I try to understand about someone. The right motivations are hard to define a set of rules for, but you know it when you see it.\nJessica Livingston and Paul Graham are my benchmarks for this. YC was widely mocked for the first few years, and almost no one thought it would be a big success when they first started. But they thought it would be great for the world if it worked, and they love helping people, and they were convinced their new model was better than the existing model.\nEventually, you will define your success by performing excellent work in areas that are important to you. The sooner you can start off in that direction, the further you will be able to go. It is hard to be wildly successful at anything you aren’t obsessed with.\n[1] A comment response I wrote on HN:\nOne of the biggest reasons I\u0026rsquo;m excited about basic income is the amount of human potential it will unleash by freeing more people to take risks.\nUntil then, if you aren\u0026rsquo;t born lucky, you have to claw your way up for awhile before you can take big swings. If you are born in extreme poverty, then this is super difficult :(\nIt is obviously an incredible shame and waste that opportunity is so unevenly distributed. But I\u0026rsquo;ve witnessed enough people be born with the deck stacked badly against them and go on to incredible success to know it\u0026rsquo;s possible.\nI am deeply aware of the fact that I personally would not be where I am if I weren\u0026rsquo;t born incredibly lucky.\nThanks to Brian Armstrong, Greg Brockman, Dalton Caldwell, Diane von Furstenberg, Maddie Hall, Drew Houston, Vinod Khosla, Jessica Livingston, Jon Levy, Luke Miles (6 drafts!), Michael Moritz, Ali Rowghani, Michael Seibel, Peter Thiel, Tracy Young and Shivon Zilis for reviewing drafts of this, and thanks especially to Lachy Groom for help writing it.\nGood Books to Read 2013 Silicon Valley Career Guide ","permalink":"https://rich-junwang.github.io/en-us/posts/read/career/","summary":"\u003cp\u003eSama has a very good blog talking about how to manage one\u0026rsquo;s career. I copied it here.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eI’ve observed thousands of founders and thought a lot about what it takes to make a huge amount of money or to create something important. Usually, people start off wanting the former and end up wanting the latter.\u003c/p\u003e\n\u003cp\u003eHere are 13 thoughts about how to achieve such outlier success. Everything here is easier to do once you’ve already reached a baseline degree of success (through privilege or effort) and want to put in the work to turn that into outlier success. [1] But much of it applies to anyone.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Compound yourself\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCompounding is magic. Look for it everywhere. Exponential curves are the key to wealth generation.\u003c/p\u003e\n\u003cp\u003eA medium-sized business that grows 50% in value every year becomes huge in a very short amount of time. Few businesses in the world have true network effects and extreme scalability. But with technology, more and more will.  It’s worth a lot of effort to find them and create them.\u003c/p\u003e","title":"How to Manager Your Career"},{"content":"我身边很多人都非常讨厌川普，但是不明白川普为什么成功。一个简单的观察是，他们不明白川普看上去一个纨绔子弟，但是他无数的演讲，对自己的政策侃侃而谈，如数家珍，毫不含糊。\n原因很简单。川普的思想体系是美英广为传承的保守主义。什么是保守主义？ 保守主义是尊重历史和传统的价值。他的核心理念是：谨慎（Prudence），温和和不冒进（对立面是激进），倚重经验和常识（对立面是唯理性主义、抽象理念）、传统和习俗（对立面是标新立异，彻底推倒重来）、折衷和妥协（对立面是偏激）、改良和演进（对立面是彻底变革或革命）， 私人产权和自由市场，有限政府。保守主义认为人是不完美的，因此需要一个社会结构去规范人的行为。\n保守主义反对激进变革、主张循序渐进, 但并非任何时候都主张温和变革, 在捍卫自由时、激进不是恶, 在面对邪恶时、中庸不是善. 这就是常说的common sense.\n要理解美国秩序（American order），就要理解它的起源。他根植于犹太-基督教的社会伦理和传统，传承雅典的理性主义和哲学思想，罗马的法律和共和秩序，建立于英国的大宪章（Magna Carta）和议会政治的基础之上，最后形成美国的独立宣言。\n中国人常讲，我们有五千年的源远流长的历史。美国的思想价值的传承也是非常久远。保守主义思想的很多部分都是基督的价值观。在基督徒的认知中，人是不完美的，因此需要救赎。承认人的渺小，接受只有神才是完美的存在，才能让人收起自己的狂妄，尊重历史自然的规律。在基督的世界里，信徒是自由的，所有的法律和信条是指引而不是枷锁。\n只有理解了这些，才能真正理解美国政治。\n","permalink":"https://rich-junwang.github.io/en-us/posts/read/us_politics/","summary":"\u003cp\u003e我身边很多人都非常讨厌川普，但是不明白川普为什么成功。一个简单的观察是，他们不明白川普看上去一个纨绔子弟，但是他无数的演讲，对自己的政策侃侃而谈，如数家珍，毫不含糊。\u003c/p\u003e\n\u003cp\u003e原因很简单。川普的思想体系是美英广为传承的保守主义。什么是保守主义？\n保守主义是尊重历史和传统的价值。他的核心理念是：谨慎（Prudence），温和和不冒进（对立面是激进），倚重经验和常识（对立面是唯理性主义、抽象理念）、传统和习俗（对立面是标新立异，彻底推倒重来）、折衷和妥协（对立面是偏激）、改良和演进（对立面是彻底变革或革命）， 私人产权和自由市场，有限政府。保守主义认为人是不完美的，因此需要一个社会结构去规范人的行为。\u003c/p\u003e","title":"保守主义"},{"content":"It\u0026rsquo;s a long way to go, but let\u0026rsquo;s just start with baby steps.\nCommon Libs Cuda: Library to use GPUs. cudnn: Library to do Neural Net stuff on GPUs (probably uses cuda to talk to the GPUs) CUTLASS: CUDA GEMM lib. cuBLAS: cuda basic linear algebra lib. ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/ml_system/","summary":"\u003cp\u003eIt\u0026rsquo;s a long way to go, but let\u0026rsquo;s just start with baby steps.\u003c/p\u003e\n\u003ch3 id=\"common-libs\"\u003eCommon Libs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCuda: Library to use GPUs.\u003c/li\u003e\n\u003cli\u003ecudnn: Library to do Neural Net stuff on GPUs (probably uses cuda to talk to the GPUs)\u003c/li\u003e\n\u003cli\u003eCUTLASS: CUDA GEMM lib.\u003c/li\u003e\n\u003cli\u003ecuBLAS: cuda basic linear algebra lib.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ## References\n[1] [Characterization of Large Language Model Development in the Datacenter](https://arxiv.org/pdf/2403.07648.pdf%EF%BC%8C)\n[2] https://github.com/intelligent-machine-learning/dlrover\n[3] [MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs](https://arxiv.org/abs/2402.15627) \n[4] [The Falcon Series of Open Language Models](https://arxiv.org/pdf/2311.16867.pdf) --\u003e","title":"ML System"},{"content":"Recently ChatGPT model has demonstrated remarkable success of large pretrained language model being able to generate coherent, logical and meaningful conversations. While as of this writing, the corresponding paper is still not available yet. In this blog, I\u0026rsquo;ll dive deep into InstructGPT model to see what\u0026rsquo;s under the hood of this model.\nIssues with Traditional LM Language modeling objective is trying to predict next token given all previous tokens. However, when we\u0026rsquo;re prompting LM in inference time, we hope LM can generate things based on our instructions/intent instead of merely predicting the most likely tokens. This is the so-called misalignment between training and inference.\nSolution Using reinforcement learning to learn human feedback. For this purpose, they have to collect a dataset. The way to collect the dataset is as follows:\nselect some contract labeler collect human written prompt-answer pairs. Prompts are either from GPT3 API or from human annotation. collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. The following diagram from the paper demonstrated how these steps unfold during the training.\nIn summary, there are three steps:\nUse labeled data to fine-tune GPT3 model Train a reward model Use RL to optimize GPT3 parameters In the first step, we got data from annotators and use this data to fine-tune GPT3 model. In the second step, they prepare some questions and GPT3 model gives multiple predictions for each question and annotators are asked to rank the generated predictions. This data is used to train reward model. The reward model is used for prediction and predict which one is most close to human choice. Reward model gives a score and the closer to human choice, the higher of the score.\nFinally, use policy-based RM algorithm to do further optimization. The whole process is shown in the diagram below. It uses reward mechanism to train model. The reward can be seen as the loss function in traditional ML training. Reward function is much more versatile than loss function (Think about DotaRL and AlphaGo). The consequence is that reward function may not be differentiable, thus can\u0026rsquo;t be used for back-propagation. People can sample rewards to proximate this loss function.\nRL algorithm. Image from [4] PPO From the repo in [4], the three steps of PPO are as follows:\nRollout: The language model generates a response or continuation based on query which could be the start of a sentence. Evaluation: The query and response are evaluated with a function, model, human feedback or some combination of them. The important thing is that this process should yield a scalar value for each query/response pair. Optimization: In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and and a reference model, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses don\u0026rsquo;t deviate to far from the reference language model. The active language model is then trained with PPO. (To be continued)\nReferences [1] Learning to summarize from human feedback [2] InstructGPT: Training language models to follow instructions with human feedback [3] Fine-Tuning Language Models from Human Preferences [4] https://github.com/lvwerra/trl [5] https://zhuanlan.zhihu.com/p/590311003 [6] Super-natural instructions: generalization via declarative instructions on 1600+ NLP tasks [7] Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/instructgpt/","summary":"\u003cp\u003eRecently ChatGPT model has demonstrated remarkable success of large pretrained language model being able to generate coherent, logical and meaningful conversations. While as of this writing, the corresponding paper is still not available yet. In this blog, I\u0026rsquo;ll dive deep into InstructGPT model to see what\u0026rsquo;s under the hood of this model.\u003c/p\u003e\n\u003ch3 id=\"issues-with-traditional-lm\"\u003eIssues with Traditional LM\u003c/h3\u003e\n\u003cp\u003eLanguage modeling objective is trying to predict next token given all previous tokens. However, when we\u0026rsquo;re prompting LM in inference time, we hope LM can generate things based on our instructions/intent instead of merely predicting the most likely tokens. This is the so-called \u003ccode\u003emisalignment\u003c/code\u003e between training and inference.\u003c/p\u003e\n\u003ch3 id=\"solution\"\u003eSolution\u003c/h3\u003e\n\u003cp\u003eUsing reinforcement learning to learn human feedback. For this purpose, they have to collect a dataset. The way to collect the dataset is as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eselect some contract labeler\u003c/li\u003e\n\u003cli\u003ecollect human written prompt-answer pairs. Prompts are either from GPT3 API or from human annotation.\u003c/li\u003e\n\u003cli\u003ecollect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe following diagram from the paper demonstrated how these steps unfold during the training.\u003c/p\u003e","title":"InstructGPT and ChatGPT"},{"content":"Large language model pretraining is a very challenging task which requires very strong engineering and science skills. People tend to underestimate efforts needed to train a good large model like GPT3 etc. Most people imagine that they can get decent language models given enough computation resources. The fact is even today only OpenAI is providing LM APIs where people can freely play with and get good performances. In this blog, we\u0026rsquo;ll talk about pretraining from the whole pipeline: data sourcing, collection and processing, tokenization, architecture engineering and evaluation. Hopefully, it would be helpful for foundational model training practitioners.\nData Data is crucial in any ML system. This is true to pretraining as well. As is shown in Gopher paper, a large, diverse and high-quality dataset is needed to train a good model. In the following table, it shows the datasets used in Gopher model training. Now we\u0026rsquo;re looking at terabytes scale of training data.\nDatasets used in Gopher [2] An ensuing problem with large amount of data is that data quality is hard to control. In practice, we have to at least make sure the content should be intelligible. We might want to give more training on high-quality datasets such as books and wikipedia [31]. Diversified datasets are necessary but can't guarantee training success as can be seen from `Gopher` paper, model performs well on QA related tasks but suffers on reasoning task. What else is needed? We'll come back to this later. Toxicity Filter There is experiment shows that toxicity filter could have a big impact on model performance. In [37], the authors proposed that instead of using a toxicity filter, inverse toxicity filtering is more helpful. Inverse toxicity filter removes the LEAST toxicity data from training data.\nTokenizer Language models compute probability of any string sequence. How to represent the string sequence is determined by tokenizer. Popular options are byte pair encoding (BPE) or wordpiece. As the majority of models are using BPE today, here we focus on BPE based tokenizer. Tokenizer can impact several things in LLM training: (1) a high compression rate (tokenized token number vs raw token number, the lower the better). Compression rate affects input context length and inference speed. (2) Vocab size. An appropriately sized vocabulary to ensure adequate training of each word embedding.\nAs mentioned in GPT2 paper, BPE effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences. Directly using greedy method to build BPE merging rules can be problematic. For example, word cat can be used in a lot of places like cat?, cat!, cat.. One way to solve this issue is to prevent BPE from generating rules across different character categories (letters, digits, puncts etc).\nAs people are pivoting in-context learning/instruction learning with large models, tokenization efficiency becomes more important. The following tables from Jurassic-1 paper shows the efficiency of tokenizer on several public dataset.\nTokenizer efficiency comparison from [16] Tokenizer determines the size of vocab. Usually when we support multilinguality and code data, the vocab size will be much larger. However, this is not always the case. CodeLLaMa shows very good performance (onpar with GPT4) with a vocab size of 32k. When vocab is too large, some of the tokens may not be trained enough. When vocab size is too small, the compression rate might be limited.\nCompression rate determines the input sequence length to the model. With high compression rate, the input length is shorter. Short sequence length might be able to mitigate exposure bias to some extent.\nOpen tokenizer implementations are: tiktoken.\nModel Architecture All pretrained models are variant of original transformer model. The differences are mainly about it\u0026rsquo;s encoder-decoder architecture or decoder-only architecture. First of all, let\u0026rsquo;s take a look at the choices of available large models.\nModels Model Size Token Size Architecture GPT3 175B 300B Decoder OPT 175B 300B Decoder PaLM 540B 780B Decoder Gopher 280B 300B Decoder Chinchilla 70B 1400B Decoder Jurassic-1 178B - Decoder Megatron-Turing NLG 530B 270B Decoder LaMDA 137B 2810B Decoder Although all models listed here are auto-regressive decoder only model, they actually differ a bit inside the decoder. For instance, to speed up inference time, PaLM is using multi-query attention. Normally, in multi-head attention, there will be h heads each with a linear project layer for Q, K, V. With multiquery attention, instead of using h different linear project layers for K and V, we can share a single smaller linear project layer for K and a single linear projection layer for V for each head. Then, for different head layers, K and V will be the same. In this way, we can save memory IO and get better latency performance in incremental inference. To speed up training, people also proposed parallel layer architecture as shown below.\nModel Architecture A systematic study of transformer architecture is done in Ref [29]. Most of recent LLM architecture are following design from this paper. People usually call the embedding dim as the width of transformer and number of layers as the depth. There is a optimal depth-to-width allocation for a given self-attention network size as is shown in [34].\nTraining Design Most of today\u0026rsquo;s pretraining follow suits of a multi-stage and multi-task training. As is shown by Yao in [1], GPT series model is pretrained in such way as well.\nGPT Model Lineage. Image from [1] From the lineage diagram, we can see that ChatGPT model comes from Codex model which can be seen as a different stage of training. The way of scheduling tasks and data during training can have great impact on the final model performance.\nBatch Size Research [5] shows that there is a critical batch size in pretraining. When training batch size exceeds critical batch size, model performance starts to degrade. Critical batch size is independent of model size and is related to loss.\nGenerally small batch size leads to better validation loss when training with the same number of tokens as more random movement of gradient explores more of loss landscape. Often times, small batch size gives better generalization performance as well as pointed out in [27]. The reason given from the paper is that smaller batch size usually converges to flat minimum as oppose to sharp minimum. Intuitively, this is related to graident update in each step is small for large batch size training.\nFlat and Sharp Minima [27] Learning Rate Scheduling Usually as pointed out in [20], when we scale up batch size, we increase learning rate propotionally. However, when we increase model size (usually followed with batch size increase), the training tends to be more unstable. Thus, in reality, we decrease maximum learning rate when we increase model size (batch size).\nLearning rate scheduling usually involves a (linear) warm-up step to maximum learning rate and followed by a decaying step to 0 or a minimum learning rate. Currently, there are several methods in literature for the decaying step:\nLinear scheduler Plateau-linear schedule Cosine scheduler Regularization One of the most used regularization method is L2 regularization, aka, weight decay [28]. For instance, GPT 3 training uses a weight decay of 0.1. Note that comparing with traditional neural network tuning weight decay number (such as 0.01) GPT3 weight decay is pretty large.\nLength Extrapolation As in-context learning becomes popular, people are asking a question, Can an LLM maintain equally good, if not better, perplexities when longer sequences are used during inference time? This is the so-called length extrapolation [25].\nOptimizer When we select an optimizer, we have to take consideration of memory footprint and stability issues etc. Options are Adafactor, Adam etc. According to Gopher paper, adafactor optimizer has smaller memory footprint, and on smaller scale model (\u0026lt;7B) adafactor works well. However, when model size goes larger, performance suffers because of stability issue.\nEvaluation A lot of large models come out every year and many claims that they could beat GPT3 model in a wide range of benchmarks like SuperGlue, CLUE, MMLU etc. However, when you do benchmark these models in zero-shot setting or some less common tasks (but still very reasonable ones), these models tend to perform really bad. I personally tested GPT3 model (175b) and UL2 model (20b) on text2sql and sql2text task, GPT3 gives way better performance to the extent that you\u0026rsquo;ll believe UL2 is like garbage. The similar thing happened in evaluation in [24]. You may argue that the model size differs a lot. However, we can think the other way around: the results they claim better than GPT3 is also got from a smaller model and maybe their model training is not easy/efficient to scale to such level. Essentially, what I want to say is that good performance on popular benchmark datasets doesn\u0026rsquo;t mean much for large LM pretraining as this is highly related to source of training data, whether or not doing fine-tuning, proper prompting etc. Human evaluation is what really matters.\nStability During the model training, the most commonly seen issue is gradient exploding, aka, gradient becomes NaN. As layers go deeper, this problem happens more often because the way backpropagation works. Over the years, people have proposed many different ways to solve the challenge. As is shown in paper [21], the post-LN shows stability issue without carefully designed warming-up stage. As a result, they are proposing pre-LN to alleviate the problem.\nThe objective function for highly nonlinear deep neural networks often contains sharp nonlinearities in parameter space resulting from the multiplication of several parameters. These nonlinearities give rise to very high derivatives in some places. When the parameters get close to such a cliff region, a gradient descent update can catapult the parameters very far, possibly losing most of the optimization work that had been done [33].\nIt\u0026rsquo;s important to monitor stability during training. Common practice is to plot activation norm and gradient norm for each step. When these values spike, we know there is something wrong. It\u0026rsquo;s better than looking at loss curve only as loss explosion generally lags behind these two indicators. For instance, when there is bad data, we could have better gauge of when that happens and restart training from that point.\nAdept AI has a lengthy blog post talking about hardware error induced stability issue. The blog mentioned two ways to identify erroneous node(s):\nGrid search: partition nodes into groups and train model on each group in a deterministic way. Find the one that has different training loss curve. Parameter checksum check: for each data parallel run, check parameter checksum to see if they are the same to determine which stage might be wrong. PaLM paper proposed adding z-loss to increase the stability of training. The method is to encourage the logits to stay close to zero. For example, we could add a max-z loss to normalize the logits: $$ L_{max\\_z} = 2e^{-4} * z^2 $$ where $z$ is the maximum logit value [39].\nEfficient Inference Inference speed determines product cost. Over the years, people have proposed various ways to improve inference speed. The multiquery attention mentioned above is one of these approaches. References [1] How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources [2] Gopher: Scaling Language Models: Methods, Analysis \u0026amp; Insights from Training Gopher [3] UL2: Unifying Language Learning Paradigms [4] Bloom: Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model [5] Scaling Laws for Neural Language Models [6] GPT: Improving Language Understanding by Generative Pre-Training [7] GPT2: Language Models are Unsupervised Multitask Learners [8] GPT3: Language Models are Few-Shot Learners [9] InstructGPT: Training language models to follow instructions with human feedback [10] WebGPT: Browser-assisted question-answering with human feedback [11] OPT: Open Pre-trained Transformer Language Models [12] OPT2: OPT-IML Scaling Language Model Instruction Meta Learning through the Lens of Generalization [13] PaLM: Scaling Language Modeling with Pathways [14] Flan-PaLM: Scaling Instruction-Finetuned Language Models [15] Chinchilla: Training Compute-Optimal Large Language Models [16] Jurassic-1: Technical details and evaluation. [17] Megatron-NLG: Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [18] LaMDA: Language Models for Dialog Applications [19] Codex: Evaluating Large Language Models Trained on Code [20] Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [21] On Layer Normalization in the Transformer Architecture [22] GLM-130B: An Open Bilingual Pre-trained Model [23] T0: Multitask Prompted Training Enables Zero-Shot Task Generalization [24] https://zhuanlan.zhihu.com/p/590240010 [25] RoFormer: Enhanced Transformer with Rotary Position Embedding [26] Receptive Field Alignment Enables Transformer Length Extrapolation [27] On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima [28] Decoupled Weight Decay Regularization [29] Do Transformer Modifications Transfer Across Implementations and Applications? [30] xFormers: A modular and hackable Transformer modelling library [31] LLaMA: Open and Efficient Foundation Language Models [32] What Language Model to Train if You Have One Million GPU Hours? [33] On the difficulty of training Recurrent Neural Networks [34] Limits to Depth-Efficiencies of Self-Attention [35] Baichuan LLM [36] Qwen LLM [37] A Pretrainer\u0026rsquo;s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, \u0026amp; Toxicity [38] GLaM: Efficient Scaling of Language Models with Mixture-of-Experts [39] Baichuan 2: Open Large-scale Language Models [40] The Falcon Series of Open Language Models [41] Simplifying Transformer Blocks [42] What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/large_scale_pretraining/pretrain/","summary":"\u003cp\u003eLarge language model pretraining is a very challenging task which requires very strong engineering and science skills. People tend to underestimate efforts needed to train a good large model like GPT3 etc. Most people imagine that they can get decent language models given enough computation resources. The fact is even today only OpenAI is providing LM APIs where people can freely play with and get good performances. In this blog, we\u0026rsquo;ll talk about pretraining from the whole pipeline: data sourcing, collection and processing, tokenization, architecture engineering and evaluation. Hopefully, it would be helpful for foundational model training practitioners.\u003c/p\u003e\n\u003ch3 id=\"data\"\u003eData\u003c/h3\u003e\n\u003cp\u003eData is crucial in any ML system. This is true to pretraining as well. As is shown in Gopher paper,  a large, diverse and high-quality dataset is needed to train a good model. In the following table, it shows the datasets used in \u003ca href=\"https://arxiv.org/pdf/2112.11446.pdf\"\u003e\u003ccode\u003eGopher\u003c/code\u003e model\u003c/a\u003e training. Now we\u0026rsquo;re looking at terabytes scale of training data.\u003c/p\u003e","title":"Large Scale Pretraining"},{"content":"Basics Kubernetes, also known as “k8s”, is an open source platform solution provided by Google for scheduling and automating the deployment, management, and scaling of containerized applications. Kubernetes has the ability of scheduling and running application containers on a cluster of physical or virtual machines. In managing the applications, the concepts of \u0026rsquo;labels\u0026rsquo; and \u0026lsquo;pods\u0026rsquo; are used to group the containers which make up an application. Currently, it supports Docker for containers.\nkubernetes architecture, image from [1] Pod is a type of abstraction on top of container. Deployment defines the pod. Deployment is used for stateless apps and StatefulSet is used for stateful apps or database. KubeCTL talks with API server to create components or delete components, in other words configuring the cluster. More about this basics is here\nBasic Operations To find out all the pods, using the following command\nkubectl get pods kubectl get pods | grep username kubectl get pods -n my_namespace_name # get pod understand a # get all the nodes kubectl get nodes # get services kubectl get services # create deployment. (blueprint of pod) kubectl create deployment my_pod_name --image=my_image kubectl get deployment kubectl delete deployment deployment_name # Log into the pod kubectl exec -it my_pod_name -- /bin/bash # describe a pod kubectl describe pods kubectl describe Pod my_pod # delete a pod kubectl delete Pod my_pod_name To get all the containers running the pod, using the following command\nkubectl get pods my_pod_name -o custom-columns=\u0026#39;NAME:.metadata.name,CONTAINERS:.spec.containers[*].name\u0026#39; kubectl describe pod my_pod_name -n my_namespace_name # To get ip. The ip can be used to launch distributed runs. kubectl get pod my_pod_name --template \u0026#39;{{.status.podIP}}\u0026#39; View logs of job running in the pod\nkubectl logs my_pod_name kubectl logs -f my_pod_name # similar to attach kubectl attach my_pod_name # works with tqdm Deploy a k8s job\n# deploy kubectl apply -f ./job.yaml # delete kubectl delete -f ./job.yaml # describe kubectl describe -f ./job.yaml When we want to delete kubernetes created resources, it\u0026rsquo;s important to remember the dependencies of the resources. For instance, to delete a fsx app, we have to do the following in order\nkubectl delete pod --all kubectl delete pvc --all kubectl delete pv --all We can use kubectl to copy files to/from the pod. Be careful that your container may not support ~ this kind of path expansion.\nkubectl cp src_file_path pod:dest_file_path To use rsync is not that straightforward, I\u0026rsquo;m using the tool from here.\n# save the file as krsync, and put it to /usr/bin, and chmod +x to the file #!/bin/bash if [ -z \u0026#34;$KRSYNC_STARTED\u0026#34; ]; then export KRSYNC_STARTED=true exec rsync --blocking-io --rsh \u0026#34;$0\u0026#34; $@ fi # Running as --rsh namespace=\u0026#39;\u0026#39; pod=$1 shift # If use uses pod@namespace rsync passes as: {us} -l pod namespace ... if [ \u0026#34;X$pod\u0026#34; = \u0026#34;X-l\u0026#34; ]; then pod=$1 shift namespace=\u0026#34;-n $1\u0026#34; shift fi exec kubectl $namespace exec -i $pod -- \u0026#34;$@\u0026#34; Then use the following command to sync files. Note that you have to install rsync on the pod.\nkrsync -av --progress --stats src-dir/ pod:/dest-dir # with namespace krsync -av --progress --stats src-dir/ pod@namespace:/dest-dir To make it easier to use, we can add the following to the .zshrc file\nfunction krsync_watch_and_sync_to { fswatch -o . | xargs -n1 -I{} krsync -av --progress --stats *(D) $1 } Sometimes we have to change file ownership. Check out more here\nchown -R 33:33 /data/uploads When we want to log into multiple pods, we can use the following command:\nxpanes -c \u0026#34;kubectl exec -it {} -- /bin/bash \u0026#34; $(kubectl get pods | grep my_pod_prefixes | cut -d\u0026#34; \u0026#34; -f1 | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;) EKS AWS has managed kubenetes service called EKS.\n# create a basic cluster eksctl create cluster eksctl create cluster -f ./my_eks.yaml # get details of a cluster eksctl get cluster --name=\u0026#34;my_cluster_1\u0026#34; --region=\u0026#34;us-west-2\u0026#34; References [1] Setting up a Kubernetes cluster using Docker in Docker [2] https://kubernetes.io/docs/reference/kubectl/cheatsheet/ ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/kubenetes/","summary":"\u003ch3 id=\"basics\"\u003eBasics\u003c/h3\u003e\n\u003cp\u003eKubernetes, also known as “k8s”, is an open source platform solution provided by Google for scheduling and automating the deployment, management, and scaling of containerized applications. Kubernetes has the ability of scheduling and running application containers on a cluster of physical or virtual machines. In managing the applications, the concepts of \u0026rsquo;labels\u0026rsquo; and \u0026lsquo;pods\u0026rsquo; are used to group the containers which make up an application. Currently, it supports Docker for containers.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"gopher dataset\" src=\"images/kubernetes.png\" width=\"80%\"/\u003e\n    \u003cbr\u003e\n    \u003cem\u003ekubernetes architecture, image from [1]\u003c/em\u003e\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003cp\u003ePod is a type of abstraction on top of container. Deployment defines the pod. Deployment is used for stateless apps and StatefulSet is used for stateful apps or database. KubeCTL talks with API server to create components or delete components, in other words configuring the cluster. More about this basics is \u003ca href=\"https://www.youtube.com/watch?v=X48VuDVv0do\u0026amp;t=2749s\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"basic-operations\"\u003eBasic Operations\u003c/h3\u003e\n\u003cp\u003eTo find out all the pods, using the following command\u003c/p\u003e","title":"Kubernetes"},{"content":"Parsing as the very first step of compiling is important for language analysis. ANTLR is a powerful tool to generate parsers. In this blog, we\u0026rsquo;re trying to understand more about ANTLR and its usage.\nANTLR Grammar (1) ANTLR has two kinds of labels: alternative labels and rule elements labels; both can be useful. We assume you are familiar with these two kinds of labels, but here it is an example.\nexpression : left=expression \u0026#39;*\u0026#39; right=expression #multiplication ​ | expression \u0026#39;+\u0026#39; expression #addition ​ | NUMBER #atom ​ ; Alternative labels are the one that follows an #, the rule element labels are the one preceding the = sign. They serve two different purposes. The first ones facilitate act differently for each alternative while the second ones facilitate accessing the different parts of the rules.\nFor alternative labels, if you label one alternative, you have to label all alternatives, because there will be no base node. Rule element labels instead provides an alternative way to access the content parsed by the sub-rule to which the label is assigned.\n(2) For the following grammar, both item nad clause will be parsed as a list in the parsing tree. They can be visited using ctx.items() and ctx.clause()\nexpression : items* ​ | clause+ (3) Parsing nested rule sometimes can be very challenging, the solution is we move the nested on into a new rule, or using labels mentioned above.\nANTLR Lexer (1) In a lexer rule, the characters inside square brackets define a character set. So [\u0026quot;] is the set with the single character \u0026quot;. Being a set, every character is either in the set or not, so defining a character twice, as in [\u0026quot;\u0026quot;] makes no difference, it\u0026rsquo;s the same as [\u0026quot;].\n~ negates the set, so ~[\u0026quot;] means any character except \u0026quot;.\n(2) In lexer or grammar, literals are marked out by quote. In the following example, namedChars will be single-quote quoted char list and ended with X or x\nnamedChars : \u0026#39;\\\u0026#39;\u0026#39; Chars \u0026#39;\\\u0026#39;\u0026#39;[Xx] Note that the grammar doesn\u0026rsquo;t count any spaces in the char.\nANTLR Parser Visitor and Listener Mode ANTLR parser provides two kinds of mechanisms to access the parsing nodes. First is listener mode: we can enter a node to perform actions based on our needs. Second is visitor mode: we can visit all parsing tree nodes top-down, left-right sequentially. This repo provides simple but useful tutorials about how this works.\nReference [1] This blog is very useful for to me when I wrote this summary doc.\n[2] StackOverflow\n[3] The definitive ANTLR Guide book\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/antlr/","summary":"\u003cp\u003eParsing as the very first step of compiling is important for language analysis. ANTLR is a powerful tool to generate parsers. In this blog, we\u0026rsquo;re trying to understand more about ANTLR and its usage.\u003c/p\u003e\n\u003ch2 id=\"antlr-grammar\"\u003eANTLR Grammar\u003c/h2\u003e\n\u003cp\u003e(1) ANTLR has two kinds of labels: \u003cem\u003ealternative labels\u003c/em\u003e and \u003cem\u003erule elements labels\u003c/em\u003e; both can be useful. We assume you are familiar with these two kinds of labels, but here it is an example.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eexpression  : left=expression \u0026#39;*\u0026#39; right=expression #multiplication\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e​           | expression \u0026#39;+\u0026#39; expression            #addition      \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e​           | NUMBER                               #atom\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e​           ;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAlternative labels are the one that follows an \u003ccode\u003e#\u003c/code\u003e, the rule element labels are the one preceding the = sign. They serve two different purposes. The first ones facilitate act differently for each alternative while the second ones facilitate accessing the different parts of the rules.\u003c/p\u003e\n\u003cp\u003eFor alternative labels, if you label one alternative, you have to label all alternatives, because there will be no base node. Rule element labels instead provides an alternative way to access the content parsed by the sub-rule to which the label is assigned.\u003c/p\u003e","title":"ANTLR Parser Generator"},{"content":"Modern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we\u0026rsquo;ll dive deep into parallel training in recent distributed training paradigms.\nA lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We\u0026rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency.\nData Parallelism Data parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is fed with one shard of data. Forward and backward computation is in parallel (simutaneously) and then there is a synchronization step where gradients are averaged across workers to update parameters. The DP computation can be summarized as the following three steps:\nEach machine computes local gradients given local inputs and a consistent global view of the parameters. LocalGrad_i = f(Inputs_i, Targets_i, Params) Sum up all the local gradients and distribute that sum to each machine, so there is a consistent global view of the gradients. GlobalGrad = all_reduce(LocalGrad_i) Each machine can now locally update the parameters and optimizer state under the assumption that the exact same calculation will happen on all machines. NewParams, NewOptimState = g(Params, OldOptimState, GlobalGrad) Pipeline Parallelism Pipeline parallelism (PP) is from model parallelism. Model parallelism is initially proposed to solve that challenge that one model can\u0026rsquo;t fit into one GPU. The idea is we can vertically slice model into different layers (e.g. one or more layers in transformer models) and put different layers in different GPUs. The issue with this method is that because sequential computation order of layers, if we feed single large batch data into one of the workers, all other workers are idle. This is the so-called bubble waiting time.\nTo solve the problem, we can reuse the data parallelism idea. Instead of feeding a single large batch into a model shard, we can partition data into small chunks. Each chunk of data goes through different model shards (workers) in a pipeline way. The following figure illustrates how this works.\nPipeline parallelism. image from [4] Tensor Parallelism The bottleneck of neural network training is compute. Among all the computation parts, the general matrix multiplication (GEMM) consumes the most of time. One way to parallize the matrix multiplication is to use matrix decomposition. Specifically, we can split a matrix into two or multiple parts based on row or column. Then we can aggregate results after the computation of each parts in the end. This is the core idea of tensor parallelism (TP).\nIn it\u0026rsquo;s essence, tensor parallelism is block matrix mutiplication. Based on how we partition the parameter matrix, there is row parallel partition and column parallel partition. For row parallel partition, there is\n$$ Y = XW = \\begin{bmatrix} X_1, \u0026amp; X_2\\end{bmatrix} \\begin{bmatrix} W_1 \\\\ W_2\\end{bmatrix} = X_1W_1 + X_2W_2 $$\nFor column parallel partition, there is $$ Y = XW = X\\begin{bmatrix} W_1, \u0026amp; W_2\\end{bmatrix} = \\begin{bmatrix} XW_1, \u0026amp; XW_2\\end{bmatrix} $$ Note that for row parallel, we need to partition the input into two parts as well. In original transformer MLP layer, there are two projection steps: hidden.size -\u0026gt; 4 * hidden.size -\u0026gt; hidden.size. In this case, in Megatron-LM MLP implementation, it first does column parallel partition, generating two matrices, then a row parallel partition. This is shown in the following figure:\nTensor Parallelism in Megatron-LM As these three parallelism is orthogonal to each other, it\u0026rsquo;s easy to combine them together. The following diagram shows how to combine pipeline parallelism with data parallelism.\nCombination of pipeline parallelism and data parallelism. Image from Deepspeed tutorial ZeRO DP Zero Redundancy Optimizer (ZeRO) is an optimizied data parallelism proposed by Deepspeed team. The idea is instead of replicating the whole model, optimizer on each of workers, we can only store needed part.\nZero DP. Image from Deepspeed Parallelism in Megatron Megatron-LM and NeMo are the open source libraries from Nvidia for the distributed training. In these two libs, there are two constraints for the parallelism settings.\n- world_size = TP * PP * DP - global_batch_size % (PP * DP) == 0 Sequence Parallel For operations such as layer normation, the operation can be paralleized on the sequence dimension. Remember that layernorm is normalization over the feature dimenstion, ie. a token representation of 2048 will be normalized over 2048 numbers. In light of this, sequence parallel is proposed to reduce GPU memory consumption.\nSequence parallelism Implementation A few key points in 3D parallelism implementation.\nTP is communication heavy, thus TP blocks should be put on different GPUs within the same node to leverage fast NVLink communication. On the contrary, PP communication is light, and it is usually put across nodes. Within a data parallel group, all GPUs hold the same model parameters. After each update, there will be gradient all-reduce operation. How to achieve this, in Megatron-LM, this is achieved by first partition all GPUs by pipeline parallelism. Then withnin the same pipeline block, partition GPUs based on tensor parallelism. After that, the number of copies within the pipeline block will be the data parallelism number.\nTraining Efficiency Metric A simpler metric for evaluation of training efficiency is model FLOPs utilization (MFU) which is defined as the ratio of the observed throughput to the theoretical maximum throughput with peak FLOPs. As can be seen in the definition, it\u0026rsquo;s very hardware dependent metric. For A100 GPUs, the metric can be calculated as $$ MFU_{V100} = \\frac{{num\\_of\\_parameters} * 6 * {total\\_training\\_token\\_count}}{{num\\_of\\_gpus} * 312e^{12} * {training\\_days} * 24 * 3600 } $$\nGenerally a good MFU should be above 40%.\nReferences [1] https://huggingface.co/blog/bloom-megatron-deepspeed [2] https://github.com/NVIDIA/NeMo [3] https://openai.com/blog/techniques-for-training-large-neural-networks/ [4] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism [5] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism [6] https://www.deepspeed.ai/tutorials/pipeline/ [7] MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs [8] AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training [9] Reducing Activation Recomputation in Large Transformer Models\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/parallelism/","summary":"\u003cp\u003eModern large language model usually is trained with billions number of parameters and trillions number of tokens. With model size and training data at such scale, computation resource and memory footprint requirement is huge. How to effectively leverage GPU resources to speed up training is an important topic in language model pretraining. In this blog, we\u0026rsquo;ll dive deep into parallel training in recent distributed training paradigms.\u003c/p\u003e\n\u003cp\u003eA lot of contents of here are from OpenAI, Nvidia, Deepspeed and bigscience blogs. We\u0026rsquo;ll first go through different parallelism techniques and then talk about how to combine them to maximize training efficiency.\u003c/p\u003e\n\u003ch3 id=\"data-parallelism\"\u003eData Parallelism\u003c/h3\u003e\n\u003cp\u003eData parallelism (DP) is the most straightforward way of parallel training. With data parallelism, model parameters and optimzer states are replicated across different workers. Data is partitioned into the same number of shards and each replicate of model is fed with one shard of data. Forward and backward computation is in parallel (simutaneously) and then there is a synchronization step where gradients are averaged across workers to update parameters. The DP computation can be summarized as the following \u003ca href=\"https://www.adept.ai/blog/sherlock-sdc\"\u003ethree steps\u003c/a\u003e:\u003c/p\u003e","title":"Parallelism in LLM Training"},{"content":"Using PyTorch for NN model training on single GPU is simple and easy. However, when it comes to multiple GPU training, there could be various of issues. In this blog, I\u0026rsquo;ll summarize all kinds of issues I ran into during model training/evaluation.\nGradient Accumulation Gradient accumulation is a way to virtually increase the batch size during training. In gradient accumulation, N batches go through the forward path and backward path, and each time, the gradient is computed and accumulated (usually summed or averaged), but model parameters are not updated. Model parameters are updated after iterate through all N batches. The logic is as follows:\nfor step, oneBatch in enumerate(dataloader): ... ypred = model(oneBatch) loss = loss_func(ytrue, ypred) loss.backward() # release all activations memory if step % accumulation_step == 0: # update weights every accumulation_step steps loss.step() loss.zero_grad() In order to backpropagate, all the hidden activations must be stored until we call loss.backward(). In contrast, if we only add losses together (accumulating losses), all the activation memory won\u0026rsquo;t be released, so we can\u0026rsquo;t save memory.\nPyTorch Inference At inference time, we call model.eval() so that model wouldn\u0026rsquo;t calculate the gradient. It would still be beneficial to wrap the code block with with torch.no_grad(). The reason is it seems PyTorch creates grad buffer for the input tensors created in the computation graph. With this no_grad wrapper, it could free more spaces.\nPyTorch DataParallel DataParallel is very easy to use, we just wrap the model with DataParallel() wrapper. The input should be splittable on dim 0. Caveat here normally when we directly feed output of tokenizer into model, e.g. using tokenizer(input) as model input, sthis will lead to unsplittable tensors.\nThe issue with DataParallel is unbalanced GPU usage. The input is splitted to each GPU, and gathered on default GPU (usually cuda:0). Thus, the default GPU has much larger memory load.\nA way to solve this issue is to wrap your model and make sure most ops are done on each GPU. Model only return a small tensor.\nimport torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader input_size = 5 output_size = 2 batch_size = 30 data_size = 100000 device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.len rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True) class Model(nn.Module): def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\u0026#34;\\tIn Model: input size\u0026#34;, input.size(), \u0026#34;output size\u0026#34;, output.size()) return output model = Model(input_size, output_size) if torch.cuda.device_count() \u0026gt; 1: print(\u0026#34;Let\u0026#39;s use\u0026#34;, torch.cuda.device_count(), \u0026#34;GPUs!\u0026#34;) # dim = 0 [30, xxx] -\u0026gt; [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model) model.to(device) for data in rand_loader: input = data.to(device) output = model(input) print(\u0026#34;Outside: input size\u0026#34;, input.size(), \u0026#34;output_size\u0026#34;, output.size()) Always make sure that the batch size is divisible by 8. If not, we can do this simple trick. This is helpful when we don\u0026rsquo;t use dataloadder and sampler.\n# get smaller number that is greater than x and is multiples of 8 def roundup(x): return (x + 7) \u0026amp; (-8) if len(input_batch) \u0026lt; batch_size: new_batch_size = roundup(len(input_batch)) input_batch += [input_batch[-1]] * (new_batch_size - len(input_batch)) Install Apex Sometimes to use the latest distributed training feature, we have to install Apex. As Apex is closely coupled with Cuda, we need to follow the next few steps to correctlly install apex.\nFind out the Cuda version used in the system. python -c \u0026#34;import torch; print(torch.version.cuda)\u0026#34; Install from source git clone https://github.com/NVIDIA/apex cd apex CUDA_HOME=/usr/local/cuda-{your-version-here}/ pip install -v --disable-pip-version-check --no-cache-dir --global-option=\u0026#34;--cpp_ext\u0026#34; --global-option=\u0026#34;--cuda_ext\u0026#34; ./ Commonly Used Pytorch Tricks Distributed training is error-prone, so effective ways of debugging is needed. Here I document some of these commands\n# print the whole tensor torch.set_printoptions(profile=\u0026#34;full\u0026#34;) torch.set_printoptions(linewidth=16000) Dataloader sometimes can be buggy, when there are errors related to dataloader, a good practice is to disable the worker number and disable prefetching.\nLaunch Distributed Run python3 -m torch.distributed.run --nnodes=2 --nproc_per_node 8 --node_rank=${NODE_RANK} --master_port=1234 --master_addr=xxx train.py args.. MASTER_ADDR=${MASTER_ADDR:-\u0026#34;localhost\u0026#34;} MASTER_PORT=${MASTER_PORT:-2345} NODE_RANK=${RANK} NNODES=${NUM_NODES} torchrun --nproc-per-node=$GPUS_PER_NODE --nnodes=$NUM_NODES --node_rank $NODE_RANK --rdzv-endpoint=${MASTER_ADDR}:${MASTER_PORT} --rdzv-backend=c10d train.py args.. Pytorch and Numpy Advanced Indexing When selection object is sequence object, ndarray/tensor, it will trigger advanced indexing. To understand how it works, we start from simple.\nx = np.arange(12).reshape(4,3) print(x) #output [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]] (1) Specify integer arrays in each dimension where every element in the array represents a number of indices into that dimension. In the example below, we select (0, 0), (1, 1), (2, 0) elements from the above array. x has two dimensions so we have two arrays to specify the indices in each dimension.\ny = x[[0,1,2], [0,1,0]] print(y) #[0 4 6] (2) The above way of indexing only renders single dimension result. We can use multi-dimension array to get multi-dimension output. Below is one of these examples. This is to select [(0, 0), (0, 2)], [(3, 0), (3, 2)] elements. Note that in each dimension we still only select one index, like 0 from row-dim, and 0 from col-dim.\nrows = np.array([[0,0],[3,3]]) cols = np.array([[0,2],[0,2]]) y = x[rows,cols] print (y) # output [[ 0 2] [ 9 11]] Loading a pretrained checkpoint A lot of times when we save a checkpoint of a pretrained model, we also save the trainer (or model state) information. This means when we load model checkpoint again, model will already have a preallocated device. When we use the same number of GPU to continue training, it will work as expected. However, the issue will arise when we have different number of GPUs for two runs. Let\u0026rsquo;s say, we first trained model on a single GPU, then we want to use multiple GPU to continue the training. When we move model to multiple GPU, there will be something weird. For instance, on GPU 0, you might see multiple process (normally one process per GPU). Or in other cases, you can see GPU 0 has much higher memory usage than other GPUs.\nSolution: when we load model, we only load parameters and strip all state information. This might be tricky sometimes. The simplest way to solve this issue is to wrap the command with with PyTorch distributed data parallel.\npython3 -m torch.distributed.launch --nnodes=1 --nproc_per_node=8 my_script.py my_config_file ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/pytorch/pytorch/","summary":"\u003cp\u003eUsing PyTorch for NN model training on single GPU is simple and easy. However, when it comes to multiple GPU training, there could be various of issues. In this blog, I\u0026rsquo;ll summarize all kinds of issues I ran into during model training/evaluation.\u003c/p\u003e\n\u003ch3 id=\"gradient-accumulation\"\u003eGradient Accumulation\u003c/h3\u003e\n\u003cp\u003eGradient accumulation is a way to virtually increase the batch size during training. In gradient accumulation, \u003ccode\u003eN\u003c/code\u003e batches go through the forward path and backward path, and each time, the gradient is computed and accumulated (usually summed or averaged), but model parameters are not updated. Model parameters are updated after iterate through all \u003ccode\u003eN\u003c/code\u003e batches. The logic is as follows:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e step, oneBatch \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e enumerate(dataloader):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   ypred \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(oneBatch)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   loss \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e loss_func(ytrue, ypred)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   loss\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebackward() \u003cspan style=\"color:#75715e\"\u003e# release all activations memory\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e step \u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e accumulation_step \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e: \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#75715e\"\u003e# update weights every accumulation_step steps\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      loss\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estep() \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      loss\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ezero_grad()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn order to backpropagate, all the hidden activations must be stored until we call loss.backward(). In contrast, if we only add losses together (accumulating losses), all the activation memory won\u0026rsquo;t be released, so we can\u0026rsquo;t save memory.\u003c/p\u003e","title":"Pytorch Multiple-GPU Training"},{"content":"Distributed infrastructure is a big and interesting topic. I don\u0026rsquo;t work on infrastructure side, but I run into the concepts a lot, so I create this blog to help me understand more about infrastructure.\nMost of today\u0026rsquo;s distributed framework involves three parts, collective communication, data loading and preprocessing and distributed scheduler. We\u0026rsquo;ll look into these three parts respectively.\nDistributed System Overview In the diagram below, I\u0026rsquo;m showing the modern distributed network communication implementation stack, from the bottom hardware to top level application.\nTraining system architecture Collective Communication We can start with point to point communication. Normally point to point communication refers to two processes communication and it\u0026rsquo;s one to one communication. Accordingly, collective communication refers to 1 to many or many to many communication. In distributed system, there are large amount of communications among the nodes.\nThere are some common communication ops, such as Broadcast, Reduce, Allreduce, Scatter, Gather, Allgather etc.\nBroadcast and Scatter Broadcast is to distribute data from one node to other nodes. Scatter is to distribute a portion of data to different nodes.\nMPI broadcast and scatter Gather Gather is an inverse operation of scatter.\nMPI gather Reduce and Allreduce Reduce is a collections of ops. Specifically, the operator will process an array from each process and get reduced number of elements.\nMPI reduce MPI reduce Allreduce means that the reduce operation will be conducted throughout all nodes. An all_reduce takes in a local array on each machine and returns the sum of all the arrays on every machine. Here we show flat all reduce operation below. However, the most common algorithm for doing this is a variant of the “ring allreduce”,\nMPI Allreduce The ReduceScatter operation performs the same operation as Reduce, except that the result is scattered in equal-sized blocks between ranks, each rank getting a chunk of data based on its rank index. In the figure below, each rank provides an array in of N (also called N element buffer, 4 here) values,\nMPI ReduceScatter Note: Executing ReduceScatter, followed by AllGather, is equivalent to the AllReduce operation.\nDifference between All2All and All_gather The following figure shows the difference between MPI all2all and all_gather.\nMPI all2all and all_gather Ring-AllReduce Implementation An all_reduce takes in a local array on each machine and returns the sum of all the arrays on every machine. The most common algorithm for doing this is a variant of the “ring allreduce”, which we’ll show how it works below. In practice, it\u0026rsquo;s usually optimized for better performance. It has two steps:\nReduce-scatter All-gather Mixed Precision Training Normally, during training we use single precision (32-bit floats). However, for LLM pretraining, this requires high-bandwidth computing platform. To address this challenge, people proposed mixed precision training. As the name suggested, mixed precision training is to leverage mixed different data type during training process, e.g. fp32 and fp16 or fp32 and bf16. We train model mostly in half precision and leave some critical ops in fp32. ss\nMixed precision training (image from fastai) Since it has same range as FP32, BF16 mixed precision training skips the scaling steps. All other Mixed Precision steps remain the same as FP16 Mixed Precision. We leave the batchnorm layers in single precision (they don’t have many weights so it’s not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss). The training loop is as follows:\ncompute the output with the FP16 model, then the loss back-propagate the gradients in half-precision. copy the gradients in FP32 precision do the update on the master model (in FP32 precision) copy the master model in the FP16 model. References https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html Andrew Gibiansky. Bringing HPC techniques to deep learning. http://research.baidu.com/bringing-hpc-techniques-deep-learning, 2017. [Online; accessed 6-December2017]. ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/distributed_training/infra/","summary":"\u003cp\u003eDistributed infrastructure is a big and interesting topic. I don\u0026rsquo;t work on infrastructure side, but I run into the concepts a lot, so I create this blog to help me understand more about infrastructure.\u003c/p\u003e\n\u003cp\u003eMost of today\u0026rsquo;s distributed framework involves three parts, collective communication, data loading and preprocessing and distributed scheduler. We\u0026rsquo;ll look into these three parts respectively.\u003c/p\u003e\n\u003ch2 id=\"distributed-system-overview\"\u003eDistributed System Overview\u003c/h2\u003e\n\u003cp\u003eIn the diagram below, I\u0026rsquo;m showing the modern distributed network communication implementation stack, from the bottom hardware to top level application.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"flat sharp minimum\" src=\"images/system.png\" width=\"100%\" height=auto/\u003e \n    \u003cem\u003eTraining system architecture\u003c/em\u003e\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003ch2 id=\"collective-communication\"\u003eCollective Communication\u003c/h2\u003e\n\u003cp\u003eWe can start with point to point communication. Normally point to point communication refers to two processes communication and it\u0026rsquo;s one to one communication. Accordingly, collective communication refers to 1 to many or many to many communication. In distributed system, there are large amount of communications among the nodes.\u003c/p\u003e\n\u003cp\u003eThere are some common communication ops, such as Broadcast, Reduce, Allreduce, Scatter, Gather, Allgather etc.\u003c/p\u003e","title":"Distributed Training Infra"},{"content":"在美国看新闻，经常可以看到讲某个政治人物是其他人的Ally。中文对应的应该就是朋党。说起朋党，中文世界基本都是贬义。通常与呼朋引伴，结党营私，朋比为奸用在一起。然而美国的政治却对ally直言不讳，甚至职场上都经常有人说allies。\n我们的文化常讲君子之交淡如水，小人之交甘若醴。认为与人结为朋党，皆是有所企图。其实这更多是皇权社会给人们灌输的信条。在皇权社会，天子一人为大，余者各自为政是最有利于统治的了。西方国家不论是内阁制还是总统制，都是一朝天子一朝臣。总统要推行自己的政策方案，当然要选择跟自己有相同信念和理想的人。\n其实不论中西方还是政治，职场，想要做成事，总是要有allies。通常因为制度的惯性，要做成一件事阻力是很大的。单枪匹马的战斗，会被折腾的遍体鳞伤，最后改革者或挑战者自己的信念都丧失了，因此也常常导致失败。\n我们看中国古代成功的政治变革，都是通过新的联盟不断对抗旧的团体。商鞅变法扶植了军功集团来打击旧贵族，隋唐科举扶植寒门对抗士族，雍正利用汉人集团打击旗人势力，借助田文静等监生官员打击士林清流。\n改革者, 挑战者通常都是少数派，想要做成事情，更是要找到惺惺相惜的ally。有相同的信念和理想，并愿意一起为之奋斗付出才能把事情做成。\n","permalink":"https://rich-junwang.github.io/en-us/posts/read/ally/","summary":"\u003cp\u003e在美国看新闻，经常可以看到讲某个政治人物是其他人的Ally。中文对应的应该就是朋党。说起朋党，中文世界基本都是贬义。通常与呼朋引伴，结党营私，朋比为奸用在一起。然而美国的政治却对ally直言不讳，甚至职场上都经常有人说allies。\u003c/p\u003e\n\u003cp\u003e我们的文化常讲君子之交淡如水，小人之交甘若醴。认为与人结为朋党，皆是有所企图。其实这更多是皇权社会给人们灌输的信条。在皇权社会，天子一人为大，余者各自为政是最有利于统治的了。西方国家不论是内阁制还是总统制，都是一朝天子一朝臣。总统要推行自己的政策方案，当然要选择跟自己有相同信念和理想的人。\u003c/p\u003e","title":"Ally -- 论朋党"},{"content":"最近吃饭的时候，会听一些讲历史的视频。最近主要听了清朝康雍乾三朝的事儿。后世常说康乾盛世（盛名之下其实难副），为这盛世做出重大贡献的雍正却很少有人提及。流传下来的很多故事，篡改遗照，血滴子等等都是负面消息。其实他做了很多事儿，很多改革都有推动社会进步。可能做的很多，也触及很多人的利益, 招致更多诋毁。这些人多数是官僚集团，既得利益集团（通常是文化人）。所以留下很多文字诽谤他。实际的受益人\u0026ndash;寻常百姓（当然他的政策也是为稳固大清帝国之目的）因为多数目不识丁，所以很少有称赞留下来。\n历史上另外一个例子是宋仁宗.仁宗一朝赋税比前朝重很多,但因其善待文人士子,所以得谥仁.\n梁启超在李鸿章传中写到誉满天下，未必不是乡愿，谤满天下未必不是伟人。可能历史就是这样吊诡，立功立名难以兼得！\n","permalink":"https://rich-junwang.github.io/en-us/posts/read/history/","summary":"\u003cp\u003e最近吃饭的时候，会听一些讲历史的视频。最近主要听了清朝康雍乾三朝的事儿。后世常说康乾盛世（盛名之下其实难副），为这盛世做出重大贡献的雍正却很少有人提及。流传下来的很多故事，篡改遗照，血滴子等等都是负面消息。其实他做了很多事儿，很多改革都有推动社会进步。可能做的很多，也触及很多人的利益, 招致更多诋毁。这些人多数是官僚集团，既得利益集团（通常是文化人）。所以留下很多文字诽谤他。实际的受益人\u0026ndash;寻常百姓（当然他的政策也是为稳固大清帝国之目的）因为多数目不识丁，所以很少有称赞留下来。\u003c/p\u003e","title":"History and Reputation"},{"content":"Frequently, I would turn to these quotes as a source of inspiration, using them to encourage personal growth for both my mind and soul.\nPart I The world is your oyster. It\u0026rsquo;s up to you to find the pearls. Still a dreamer, yet more of a realist than ever before, I knew this was my time to sail. On the horizon I saw the shining future, as before. The difference now was that I felt the wind at my back. I was ready. The future was uncertain, absolutely, and there were many hurdles, twists, and turns to come, but as long as I kept moving forward, one foot in front of the other, the voices of fear and shame, the messages from those who wanted me to believe that I wasn\u0026rsquo;t good enough, would be stilled. Others may question your credentials, your papers, your degrees. Others may look for all kinds of ways to diminish your worth. But what is inside you no one can take from you or tarnish. This is your worth, who you really are, your degree that can go with you wherever you go, that you bring with you the moment you come into a room, that can\u0026rsquo;t be manipulated or shaken. Without that sense of self, no amount of paper, no pedigree, and no credentials can make you legit. No matter what, you have to feel legit inside first. Walk that walk and go forward all the time. Don\u0026rsquo;t just talk that talk, walk it and go forward. Also, the walk didn\u0026rsquo;t have to be long strides; baby steps counted too. Go forward. You got a dream\u0026hellip; You gotta protect it. People can\u0026rsquo;t do somethin' themselves, they wanna tell you you can\u0026rsquo;t do it. If you want somethin\u0026rsquo;, go get it. Don\u0026rsquo;t worry if your tasks are small and rewards are few, remember that the mighty oak was once a nut like you. When you know this world as it is and you still love it, then your life will be beautiful. What is a success: To laugh often and much, to win the respect of intelligent people and the affection of children, to earn the appreciation of honest critics and endure the betrayal of false friends, to appreciate beauty, to find the best in others, to leave the world a bit better\u0026hellip; A journey of a thousand miles begins with a single step! Adversity is a catalyst for greatness! Winners worry about winning; losers worry about winners Slow is smooth and smooth is fast Success is not final, failure is not fatal, it is the courage to continue that counts. Doublethink, to deliberately believe in lies, while knowing they\u0026rsquo;re false. Examples of this in everyday life: \u0026ldquo;Oh, I need to be pretty to be happy. I need surgery to be pretty. I need to be thin, famous, fashionable.\u0026rdquo; Our young men today are being told that women are whores, bitches, things to be screwed, beaten, shit on, and shamed. This is a marketing holocaust. Twenty-four hours a day for the rest of our lives, the powers that be are hard at work dumbing us to death. So to defend ourselves, and fight against assimilating this dullness into our thought processes, we must learn to read. To stimulate our own imagination, to cultivate our own consciousness, our own belief systems. We all need skills to defend, to preserve our own minds. (Detachment) Part II 不迁怒，不贰过: 回年二十九，发尽白，蚤死。孔子哭之恸，曰：“自吾有回，门人益亲。” 鲁哀公问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不迁怒，不贰过。不幸短命死矣，今也则亡，未闻好学者 也。”\n爱好和付出是一个正反馈的过程\n人生惟有常是第一美德 （曾国藩）\n兄自问近年得力惟一悔字诀。兄昔年自负本领甚大，可屈可伸，可行可藏，又每见得人家不是。自从丁已、戊午大悔大悟之后，乃知自己全无本领，凡事都见得人家有几分是处。故自成午 至今九载，与四十岁以前迥不相同，大约以能立能达为体，以不怨不尤为用。立者，发奋自强，站得住也； 达者，办事圆融，行得通也。（曾国藩）\n孔子曰：君子有九思：视思明，听思聪，色思温，貌思恭，言思忠，事思敬，疑思问，忿思难，见得思义。孔子说：君子有九种考虑：看的时候，考虑看明白了没有；听的时候，考虑听清楚了没有；考虑自己的表情温和么？态度庄重么？说话诚恳老实么？工作严肃认真么？遇到疑难，考虑怎样去 向人家请教；要发怒了，考虑有没有后患；在可以得到利益的时候，考虑是不是该得。\n凡遇牢骚欲发之时，则反躬自思，吾果有何不足，而蓄此不平之气，猛然内省，决然去之。（曾国藩）\n怕什么真理无穷，进一寸有一寸的欢喜.\n他强由他强，清风拂山岗，他横任他横，明月照大江。\n颜渊问仁。子曰：克己复礼，为仁。一日克己复礼，天下归仁焉。为仁由己，而由人乎哉？颜渊曰：请问其目。子曰：非礼勿视，非礼勿听，非礼勿言，非礼勿动。颜渊曰：回虽不敏，请 事斯语矣。\n君子不忧不惧。\n士不可以不弘毅，任重而道远，仁以为己任，不亦重乎，死而后已，不亦远乎。\n为天地立心，为生民立命，为往圣继绝学，为万世开太平。\n修身齐家治国平天下。\n一切众生皆具如来智慧德相，唯以妄想执着不能证得.\n成功不是终结，失败不是终结，唯有勇气才是永恒 (Winston Churchill)\n","permalink":"https://rich-junwang.github.io/en-us/posts/read/quotes/","summary":"\u003cp\u003eFrequently, I would turn to these quotes as a source of inspiration, using them to encourage personal growth for both my mind and soul.\u003c/p\u003e\n\u003ch3 id=\"part-i\"\u003ePart I\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eThe world is your oyster. It\u0026rsquo;s up to you to find the pearls.\u003c/li\u003e\n\u003cli\u003eStill a dreamer, yet more of a realist than ever before, I knew this\nwas my time to sail. On the horizon I saw the shining future, as before. The difference now was that I felt\nthe wind at my back. I was ready.\u003c/li\u003e\n\u003cli\u003eThe future was uncertain, absolutely, and there were many hurdles,\ntwists, and turns to come, but as long as I kept moving forward, one foot in front of the other, the voices\nof fear and shame, the messages from those who wanted me to believe that I wasn\u0026rsquo;t good enough, would be\nstilled.\u003c/li\u003e\n\u003cli\u003eOthers may question your credentials, your papers, your degrees.\nOthers may look for all kinds of ways to diminish your worth. But what is inside you no one can take from\nyou or tarnish. This is your worth, who you really are, your degree that can go with you wherever you go,\nthat you bring with you the moment you come into a room, that can\u0026rsquo;t be manipulated or shaken. Without that\nsense of self, no amount of paper, no pedigree, and no credentials can make you legit. No matter what, you\nhave to feel legit inside first.\u003c/li\u003e\n\u003cli\u003eWalk that walk and go forward all the time. Don\u0026rsquo;t just talk that\ntalk, walk it and go forward. Also, the walk didn\u0026rsquo;t have to be long strides; baby steps counted too. Go\nforward.\u003c/li\u003e\n\u003cli\u003eYou got a dream\u0026hellip; You gotta protect it. People can\u0026rsquo;t do somethin'\nthemselves, they wanna tell you you can\u0026rsquo;t do it. If you want somethin\u0026rsquo;, go get it.\u003c/li\u003e\n\u003cli\u003eDon\u0026rsquo;t worry if your tasks are small and rewards are few, remember\nthat the mighty oak was once a nut like you.\u003c/li\u003e\n\u003cli\u003eWhen you know this world as it is and you still love it, then your\nlife will be beautiful.\u003c/li\u003e\n\u003cli\u003eWhat is a success: To laugh often and much, to win the respect of\nintelligent people and the affection of children, to earn the appreciation of honest critics and endure the\nbetrayal of false friends, to appreciate beauty, to find the best in others, to leave the world a bit\nbetter\u0026hellip;\u003c/li\u003e\n\u003cli\u003eA journey of a thousand miles begins with a single step!\u003c/li\u003e\n\u003cli\u003eAdversity is a catalyst for greatness!\u003c/li\u003e\n\u003cli\u003eWinners worry about winning; losers worry about winners\u003c/li\u003e\n\u003cli\u003eSlow is smooth and smooth is fast\u003c/li\u003e\n\u003cli\u003eSuccess is not final, failure is not fatal, it is the courage to continue that counts.\u003c/li\u003e\n\u003cli\u003eDoublethink, to deliberately believe in lies, while knowing they\u0026rsquo;re false. Examples of this in everyday life: \u0026ldquo;Oh, I need to be pretty to be happy. I need surgery to be pretty. I need to be thin, famous, fashionable.\u0026rdquo; Our young men today are being told that women are whores, bitches, things to be screwed, beaten, shit on, and shamed. This is a marketing holocaust. Twenty-four hours a day for the rest of our lives, the powers that be are hard at work dumbing us to death. So to defend ourselves, and fight against assimilating this dullness into our thought processes, we must learn to read. To stimulate our own imagination, to cultivate our own consciousness, our own belief systems. We all need skills to defend, to preserve our own minds. (Detachment)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"part-ii\"\u003ePart II\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e不迁怒，不贰过: 回年二十九，发尽白，蚤死。孔子哭之恸，曰：“自吾有回，门人益亲。” 鲁哀公问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不迁怒，不贰过。不幸短命死矣，今也则亡，未闻好学者 也。”\u003c/p\u003e","title":"Quotes"},{"content":"人生而自由，却无往不在枷锁之中。\n每当有事难以释怀的时候，我总是会去看看胡适之先生的容忍与自由。先生娓娓道来的文笔，总是让人如沐春风，豁然开朗。胡适知行合一，有容人的雅量， 当真是儒雅君子。金庸写，刚而易折，强极则辱，谦谦君子，温润如玉，大抵也是相同意思。\n自由是每个人的追求，是和个人利益一致的。 容忍却是要有一番修为，不是每个人都能够做到，在今天互联网时代，当每个人可以自由表达自己的时候，容忍显得 尤为难能可贵。\n容忍是谦逊，是爱。\n","permalink":"https://rich-junwang.github.io/en-us/posts/read/tolerence_and_freedom/","summary":"\u003cp\u003e人生而自由，却无往不在枷锁之中。\u003c/p\u003e\n\u003cp\u003e每当有事难以释怀的时候，我总是会去看看胡适之先生的容忍与自由。先生娓娓道来的文笔，总是让人如沐春风，豁然开朗。胡适知行合一，有容人的雅量，\n当真是儒雅君子。金庸写，刚而易折，强极则辱，谦谦君子，温润如玉，大抵也是相同意思。\u003c/p\u003e\n\u003cp\u003e自由是每个人的追求，是和个人利益一致的。 容忍却是要有一番修为，不是每个人都能够做到，在今天互联网时代，当每个人可以自由表达自己的时候，容忍显得\n尤为难能可贵。\u003c/p\u003e","title":"Tolerence and Freedom"},{"content":"In this doc, I keep record of some commonly used aws related commands for my quick reference. I\u0026rsquo;ll be very glad if this could be somewhat helpful to you.\nECR ECR login\nFor aws-cli 2.7 or above version, use the command below:\n# check all images aws ecr describe-repositories # login the docker aws ecr get-login-password --region \u0026lt;region\u0026gt; | docker login --username AWS --password-stdin \u0026lt;aws_account_id\u0026gt;.dkr.ecr.\u0026lt;region\u0026gt;.amazonaws.com # we can get the aws account id using the following command aws ecr get-login-password --region \u0026lt;region\u0026gt; | docker login --username AWS --password-stdin \u0026#34;$(aws sts get-caller-identity --query Account --output text).dkr.ecr.\u0026lt;region\u0026gt;.amazonaws.com\u0026#34; # pull the image docker pull \u0026lt;image_name\u0026gt; # If we pushed the image using sudo, then pull also add sudo sudo docker pull \u0026lt;image_name\u0026gt; Sometimes we need one image in one region, but it\u0026rsquo;s pushed to another region. We can do the dollowing steps to push the image to target region.\n# login to the region where the image current is. Here assume it\u0026#39;s in us-east-1 REGION=us-east-1 ; aws ecr get-login-password --region ${REGION} | docker login --username AWS --password-stdin \u0026lt;aws_account_id\u0026gt;.dkr.ecr.${REGION}.amazonaws.com # Then pull the image from ECR docker pull image_name # Find out the image id docker image ls | grep image_name | cut -f3 # tag docker tag image_id new_image_tag_with_new_region # login to the new region REGION=us-west-2 ; aws ecr get-login-password --region ${REGION} | docker login --username AWS --password-stdin \u0026lt;aws_account_id\u0026gt;.dkr.ecr.${REGION}.amazonaws.com # push docker push new_image_tag_with_new_region GitLab and Github Gitlab changes its authentication methods and the way it works is almost identical to Github. The easiest way to use it is through personal token.\n# For gitlab usage # clone a repo using personal token git clone https://oauth2:personal_token@gitlab.com/username/project.git git remote set-url origin https://oauth2:personal_token@gitlab.com/username/project.git git push https://personal_token@gitlab.com/username/project.git ## For github usage git clone https://username:personal_token@github.com/username/project.git . git remote set-url origin https://username:personal_token@github.com/username/project.git git push https://personal_token@github.com/username/project.git Another simpler way to clone personal repo on a new machine (1) Add ssh public key into github/gitlab webpage. settings -\u0026gt; ssh and GPG keys -\u0026gt; add ssh key. (2) To clone a repo, use ssh link. git@github.com:xxx\nWe can also use https to clone a repo, but need to add personal access token from github.\nCommon AWS CLI To get the current region,\naws configure get region # if using the profile aws configure get region --profile $PROFILE_NAME # aws sync with exclude aws s3 sync s3://my-first-bucket s3://my-second-bucket --exclude \u0026#39;datasets/*\u0026#39; # get the identity aws sts get-caller-identity # export credentials export $(ada credentials print --account xxx --role myrole --provider=myprovider --profile my_profile --format env | xargs -L 1) CloudWatch To use cloudwatch insight, we can use the following query\nfields @timestamp, @message, @logStream | filter @logStream like /xxxxx/ | sort @timestamp desc | limit 10000 VPC and Security Group Security group controls how we login the instance (like through ssh etc) VPC determines what kind of resource we can visit from the instance. For instance if we are able to access specific EFS and FSx. Private VPC subnet will require a bastion to connect to instance.\nkubernetes architecture, image from [1] An Internet Gateway is a logical connection between a VPC and the Internet. If there is no Internet Gateway, then the VPC has no direct access to the Internet. (However, Internet access might be provided via a Transit Gateway, which itself would need an Internet Gateway.)\nThink of the Internet Gateway as the wire that you use to connect your home router to the Internet. Pull out that wire and your home network won\u0026rsquo;t be connected to the Internet.\nA subnet is a \u0026lsquo;public subnet\u0026rsquo; if it has a Route Table that references an Internet Gateway.\nA NAT Gateway receives traffic from a VPC, forwards it to the Internet and then returns the response that was received. It must live in a public subnet because it needs to communicate with the Internet (and therefore needs a route to the Internet Gateway).\nResources in a private subnet (which, by definition, cannot route to the Internet Gateway) will have their Internet-bound requests sent to the NAT Gateway (due to a Route Table configuration). The NAT Gateway will then forward that request to the Internet and return the response that was received from the Internet.\nNAT Gateways exist because organizations want the additional security offered by private subnets, which guarantee that there is no inbound access from the Internet. Similar security can be provided with a Security Group, so private subnets aren\u0026rsquo;t actually required. However, people who are familiar with traditional (non-cloud) networking are familiar with the concept of public and private subnets, so they want to replicate that architecture in the cloud. Physical network routers only apply rules at the boundary of subnets, whereas Security Groups can be applied individually to each Resource. It\u0026rsquo;s a bit like giving each resource its own router.\nYou are right that all of the above is implemented as a virtual network. There is no physical device called an Internet Gateway or a NAT Gateway. Much of it is logical routing, although the NAT Gateway does involve launching infrastructure behind-the-scenes (probably on the same infrastructure that runs EC2 instances). The NAT Gateway only connects to one VPC \u0026ndash; it is not a \u0026lsquo;shared service\u0026rsquo; like Amazon S3, which is available to many AWS users simultaneously.\nYou also mention performing work \u0026lsquo;manually\u0026rsquo;. An entire VPC (including subnets, route tables, Internet Gateway, NAT Gateway, Security Groups) can be deployed automatically using an AWS CloudFormation template, or via the VPC Wizard in the VPC management console.\nEFS # install efs-utils, depends on platform, it could be different. # Then mkdir $HOME/efs sudo mount -t efs efs_id $HOME/efs EKS # create an eks without node group eksctl create cluster --region us-east-1 --without-nodegroup --vpc-public-subnets ${subnets} # dry run eksctl create cluster -f ./eks.yaml --dry-run When use cloudformation to create eks node group, the hardware could fail. We can use the following command to check the failure. The first failure in the stack (the resource provision is in a certain order) is usually what we\u0026rsquo;re looking for.\naws cloudformation describe-stack-events --stack-name my-cluster-stack-name To delete a failed stack\naws cloudformation delete-stack --stack-name my-cluster-stack-name FSx for Lustre Lustre (linux + cluster) is a high performance file system. FSx for lustre is an aws managed service to launch and run for lustre file system.\nFSx and S3 s3 is long-term durable storage. FSx for lustre is used when processing data. Usually FSx is used as a high performance file system linked into s3 bucket.\nStore your data on s3 Create an FSx file system and link it to your s3 bucket At any time, use a lustre command to write changes back to s3 Delete FSx file system when you\u0026rsquo;re done processing ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/cloud/aws/","summary":"\u003cp\u003eIn this doc, I keep record of some commonly used aws related commands for my quick reference. I\u0026rsquo;ll be very glad if this could be somewhat helpful to you.\u003c/p\u003e\n\u003ch3 id=\"ecr\"\u003eECR\u003c/h3\u003e\n\u003cp\u003eECR login\u003c/p\u003e\n\u003cp\u003eFor aws-cli 2.7 or above version, use the command below:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# check all images\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eaws ecr describe-repositories\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# login the docker\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eaws ecr get-login-password --region \u0026lt;region\u0026gt; | docker login --username AWS --password-stdin \u0026lt;aws_account_id\u0026gt;.dkr.ecr.\u0026lt;region\u0026gt;.amazonaws.com\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# we can get the aws account id using the following command\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eaws ecr get-login-password --region \u0026lt;region\u0026gt; | docker login --username AWS --password-stdin \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e$(\u003c/span\u003eaws sts get-caller-identity --query Account --output text\u003cspan style=\"color:#66d9ef\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.dkr.ecr.\u0026lt;region\u0026gt;.amazonaws.com\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# pull the image\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker pull \u0026lt;image_name\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# If we pushed the image using sudo, then pull also add sudo\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo docker pull \u0026lt;image_name\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSometimes we need one image in one region, but it\u0026rsquo;s pushed to another region. We can do the dollowing steps to push the image to target region.\u003c/p\u003e","title":"A Walk in the Cloud"},{"content":"A lot of people are talking about leetcode problem solving these days. If you go to the popular Chinese overseas BBS website, you can find literally under every post there are people talking about solving leetcode problems. In this blog, I want to share my two cents on this issue.\nI personally don\u0026rsquo;t like solving leetcode problems which I guess most people share with my feelings. I don\u0026rsquo;t take any pride in being ranked as the top K problem solver. My opinion is that it\u0026rsquo;s huge waste of time. There are definitely some good parts in doing this. If you\u0026rsquo;re not familiar with a programming langauge, you can learn a bit from good solutions in the disucssion section. Through solving the problem, you\u0026rsquo;ll get familiar with the specific programming language you use. Through the thinking process, you\u0026rsquo;ll learn how to convert logics into codes.\nHowever, focusing on these Fake problems will cost a person gigantic amount of time. There are more important things to learn. In my opinion, a pragmatic engineer should focus on the following four quadrants to improve himself:\nTechnical skills. A good understanding of a wide range of topics such as ML, system design etc. Real problem solving skills. When tasked with a real problem, which is the best route to solve the challenge. Communication skills. How to use concise and precise words to convey your ideas and onboard others with your thoughts. Business acumen. How customers can get benefits from our product, what\u0026rsquo;s our moat, and is our solution going to bring revenue to company. At the end of the day, we want to ask ourselves what kind of innovations/changes we have brought to this world. In my view, that\u0026rsquo;s what defines our value. In the meanwhile, I feel it\u0026rsquo;s a lot more fun in solving the real world problem and tackle the real issues.\nAll big techs today are relying on leetcode to select best talents which, in my opinion, is quite unfortunate.(To be continued)\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/tech/","summary":"\u003cp\u003eA lot of people are talking about leetcode problem solving these days. If you go to the popular Chinese overseas BBS website, you can find literally under every post there are people talking about solving leetcode problems. In this blog, I want to share my two cents on this issue.\u003c/p\u003e\n\u003cp\u003eI personally don\u0026rsquo;t like solving leetcode problems which I guess most people share with my feelings. I don\u0026rsquo;t take any pride in being ranked as the top K problem solver. My opinion is that it\u0026rsquo;s huge waste of time. There are definitely some good parts in doing this. If you\u0026rsquo;re not familiar with a programming langauge, you can learn a bit from good solutions in the disucssion section. Through solving the problem, you\u0026rsquo;ll get familiar with the specific programming language you use. Through the thinking process, you\u0026rsquo;ll learn how to convert logics into codes.\u003c/p\u003e","title":"Tech \u0026 Talents"},{"content":"LLM Serving Working on LLMs often entails us to conduct a demo for real-time test. Sometimes we have to set things up so that co-worker can play with our model to find out the issues there. An eassy way is to use Flask.\nimport flask app = flask.Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def index(): return \u0026#34;\u0026lt;h3\u0026gt;My LLM Playground\u0026lt;/h3\u0026gt;\u0026#34; Start the Server Start the server, we can run\nApiServicePort=xxxx python3 serve.py Front-End If we use flask render_template to provide the front end, then we can use the following to ways to launch the app,\n# method 1 flask run # method 2 python3 app.py Another way is to use streamlit. Streamlit is an open-source Python library that allows developers to create web applications for data science and machine learning projects with minimal effort. It is designed to simplify the process of turning data scripts into shareable web apps, enabling users to interact with data and models through a web browser. If we use streamlit, we can run with\nstreamlit run app.py Usually we first star the serve and specific the port to listening on. Then pull up the front end page.\nThe page will be like the following, simple and easy!!\nLLM Playground References [1] Openplayground ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/serve_model/","summary":"\u003ch3 id=\"llm-serving\"\u003eLLM Serving\u003c/h3\u003e\n\u003cp\u003eWorking on LLMs often entails us to conduct a demo for real-time test. Sometimes we have to set things up so that co-worker can play with our model to find out the issues there.\nAn eassy way is to use Flask.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e flask\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eapp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e flask\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eFlask(__name__)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003e@app.route\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;/\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eindex\u003c/span\u003e():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026lt;h3\u0026gt;My LLM Playground\u0026lt;/h3\u0026gt;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"start-the-server\"\u003eStart the Server\u003c/h3\u003e\n\u003cp\u003eStart the server, we can run\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eApiServicePort\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003exxxx python3 serve.py\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"front-end\"\u003eFront-End\u003c/h3\u003e\n\u003cp\u003eIf we use flask \u003ccode\u003erender_template\u003c/code\u003e to provide the front end, then we can use the following to ways to launch the app,\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# method 1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eflask run\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# method 2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython3 app.py\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAnother way is to use \u003ccode\u003estreamlit\u003c/code\u003e. Streamlit is an open-source Python library that allows developers to create web applications for data science and machine learning projects with minimal effort. It is designed to simplify the process of turning data scripts into shareable web apps, enabling users to interact with data and models through a web browser.\nIf we use \u003ccode\u003estreamlit\u003c/code\u003e, we can run with\u003c/p\u003e","title":"LLM Playground"},{"content":"Keep SSH Connected There is always one issue that bothers me when using SSH to access server (e.g. EC2) which is that the ssh connection can disconnect very soon. I tried to make changes in the local ssh config: ~/.ssh/config\nHost remotehost HostName remotehost.com ServerAliveInterval 50 Then do a permission change\nchmod 600 ~/.ssh/config However, this doesn\u0026rsquo;t work for me on Mac, and I don\u0026rsquo;t know why. :(\nThen I tried to make changes on server side. In /etc/ssh/sshd_config, add or uncomment the following lines:\nClientAliveInterval 50 ClientAliveCountMax 10 Then restart or reload SSH server to help it recognize the configuration change\nsudo service ssh restart # for ubuntu linux sudo service sshd restart # for other linux dist Finally, log out and try to login again\nlogout This time it works! :)\nAdding SSH Public Key to Server Adding ssh public key to server sometimes can make the connections eaiser. The command is simple:\ncat ~/.ssh/id_ras.pub | ssh -i \u0026#34;my-keypair.pem\u0026#34; ubuntu@myserver \u0026#39;cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026#39; ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/ssh/","summary":"\u003ch2 id=\"keep-ssh-connected\"\u003eKeep SSH Connected\u003c/h2\u003e\n\u003cp\u003eThere is always one issue that bothers me when using SSH to access server (e.g. EC2) which is that the ssh connection can disconnect very soon. I tried to make changes in the local ssh config: \u003ccode\u003e~/.ssh/config\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eHost remotehost\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tHostName remotehost.com\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tServerAliveInterval 50\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen do a permission change\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003echmod 600 ~/.ssh/config\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHowever, this doesn\u0026rsquo;t work for me on Mac, and I don\u0026rsquo;t know why. :(\u003c/p\u003e\n\u003cp\u003eThen I tried to make changes on server side.\nIn \u003ccode\u003e/etc/ssh/sshd_config\u003c/code\u003e, add or uncomment the following lines:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eClientAliveInterval 50\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eClientAliveCountMax 10\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen restart or reload SSH server to help it recognize the configuration change\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo service ssh restart  # for ubuntu linux\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo service sshd restart  # for other linux dist\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFinally, log out and try to login again\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003elogout\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis time it works! :)\u003c/p\u003e\n\u003ch2 id=\"adding-ssh-public-key-to-server\"\u003eAdding SSH Public Key to Server\u003c/h2\u003e\n\u003cp\u003eAdding ssh public key to server sometimes can make the connections eaiser. The command is simple:\u003c/p\u003e","title":"SSH Connection"},{"content":"1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找\nscan总共有这几种命令：scan、sscan、hscan、zscan，分别用于迭代数据库中的：数据库中所有键、集合键、哈希键、有序集合键，命令具体结构如下：\nscan cursor [MATCH pattern] [COUNT count] [TYPE type] sscan key cursor [MATCH pattern] [COUNT count] hscan key cursor [MATCH pattern] [COUNT count] zscan key cursor [MATCH pattern] [COUNT count] 2. scan scan cursor [MATCH pattern] [COUNT count] [TYPE type]，cursor表示游标，指查询开始的位置，count默认为10，查询完后会返回下一个开始的游标，当返回0的时候表示所有键查询完了\n127.0.0.1:6379[2]\u0026gt; scan 0 1) \u0026#34;3\u0026#34; 2) 1) \u0026#34;mystring\u0026#34; 2) \u0026#34;myzadd\u0026#34; 3) \u0026#34;myhset\u0026#34; 4) \u0026#34;mylist\u0026#34; 5) \u0026#34;myset2\u0026#34; 6) \u0026#34;myset1\u0026#34; 7) \u0026#34;mystring1\u0026#34; 8) \u0026#34;mystring3\u0026#34; 9) \u0026#34;mystring4\u0026#34; 10) \u0026#34;myset\u0026#34; 127.0.0.1:6379[2]\u0026gt; scan 3 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;myzadd1\u0026#34; 2) \u0026#34;mystring2\u0026#34; 3) \u0026#34;mylist2\u0026#34; 4) \u0026#34;myhset1\u0026#34; 5) \u0026#34;mylist1\u0026#34; MATCH可以采用模糊匹配找出自己想要查找的键，这里的逻辑是先查出20个，再匹配，而不是先匹配再查询，这里加上count 20是因为默认查出的10个数中可能不能包含所有的相关项，所以把范围扩大到查20个，我这里测试的键总共有15个\n127.0.0.1:6379[2]\u0026gt; scan 0 match mylist* count 20 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;mylist\u0026#34; 2) \u0026#34;mylist2\u0026#34; 3) \u0026#34;mylist1\u0026#34; TYPE可以根据具体的结构类型来匹配该类型的键\n127.0.0.1:6379[2]\u0026gt; scan 0 count 20 type list 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;mylist\u0026#34; 2) \u0026#34;mylist2\u0026#34; 3) \u0026#34;mylist1\u0026#34; 3. sscan sscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是集合类型的key\n127.0.0.1:6379[2]\u0026gt; sadd myset1 a b c d (integer) 4 127.0.0.1:6379[2]\u0026gt; smembers myset1 1) \u0026#34;d\u0026#34; 2) \u0026#34;a\u0026#34; 3) \u0026#34;c\u0026#34; 4) \u0026#34;b\u0026#34; 127.0.0.1:6379[2]\u0026gt; sscan myset1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;d\u0026#34; 2) \u0026#34;c\u0026#34; 3) \u0026#34;b\u0026#34; 4) \u0026#34;a\u0026#34; 127.0.0.1:6379[2]\u0026gt; sscan myset1 0 match a 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;a\u0026#34; 4. hscan hscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是哈希类型的key\n127.0.0.1:6379[2]\u0026gt; hset myhset1 kk1 vv1 kk2 vv2 kk3 vv3 (integer) 3 127.0.0.1:6379[2]\u0026gt; hgetall myhset1 1) \u0026#34;kk1\u0026#34; 2) \u0026#34;vv1\u0026#34; 3) \u0026#34;kk2\u0026#34; 4) \u0026#34;vv2\u0026#34; 5) \u0026#34;kk3\u0026#34; 6) \u0026#34;vv3\u0026#34; 127.0.0.1:6379[2]\u0026gt; hscan myhset1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;kk1\u0026#34; 2) \u0026#34;vv1\u0026#34; 3) \u0026#34;kk2\u0026#34; 4) \u0026#34;vv2\u0026#34; 5) \u0026#34;kk3\u0026#34; 6) \u0026#34;vv3\u0026#34; 5. zscan zscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是有序集合类型的key\n127.0.0.1:6379[2]\u0026gt; zadd myzadd1 1 zz1 2 zz2 3 zz3 (integer) 3 127.0.0.1:6379[2]\u0026gt; zrange myzadd1 0 -1 withscores 1) \u0026#34;zz1\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;zz2\u0026#34; 4) \u0026#34;2\u0026#34; 5) \u0026#34;zz3\u0026#34; 6) \u0026#34;3\u0026#34; 127.0.0.1:6379[2]\u0026gt; zscan myzadd1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;zz1\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;zz2\u0026#34; 4) \u0026#34;2\u0026#34; 5) \u0026#34;zz3\u0026#34; 6) \u0026#34;3\u0026#34; ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/redis/","summary":"\u003ch2 id=\"1-介绍\"\u003e1. 介绍\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003escan\u003c/code\u003e命令的作用和\u003ccode\u003ekeys *\u003c/code\u003e的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用\u003ccode\u003ekeys *\u003c/code\u003e这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003escan\u003c/code\u003e总共有这几种命令：\u003ccode\u003escan\u003c/code\u003e、\u003ccode\u003esscan\u003c/code\u003e、\u003ccode\u003ehscan\u003c/code\u003e、\u003ccode\u003ezscan\u003c/code\u003e，分别用于迭代数据库中的：数据库中所有键、集合键、哈希键、有序集合键，命令具体结构如下：\u003c/p\u003e","title":"Redis scan命令学习"},{"content":"About Me\nHi there, this is Jun Wang. Welcome to my website. I'm a programmer. I love programming. I am a Senior Applied Scientist at Amazon, specializing in foundational model pre-training, post-training, and reinforcement learning for complex reasoning tasks. My expertise spans the entire lifecycle of large language model (LLM) development, including data curation, pretraining, supervised fine-tuning (SFT), and reinforcement learning with human feedback (RLHF). I have experience training both text-based and code-focused models. Most recently, I\u0026rsquo;ve been focusing on deep RL for math, code generation tasks.\nOpinions are my own and I do not speak for my employer or my team. On this webpage, I'd like to share my study and my paper reading. Feel free to reach out to me if you have any comments/suggestions\nIn my spare time, I like hiking a lot. I'm also very much into reading books of history, biography etc. Drop me a line if you want to chat more! Publications\nAll my publications can be found on google scholar Professional Services\nReviewer for ACL ARR 2024, EMNLP 2025, ICLR 2025 Reviewer for NeurIPS, ICLR 2023 Reviewer for AMLC 2020, 2021 Reviewer for ICLR 2022 Reviewer for AAAI 2021 I served as the Amazon Research Award reviewer and was an AC for AMLC conference at Amazon.\n","permalink":"https://rich-junwang.github.io/en-us/about/","summary":"\u003cp style=\"font-size: 25px;\"\u003eAbout Me\u003c/p\u003e\n\u003cstyle\u003e\n .wrap {\n   float: right; \n   margin: 5px;\n  }\n  #mypic {\n  width: 200px;\n  border-radius: 200%;\n  align: left;\n  padding: 20px;\n  border: 2px solid #D3D3D3;\n}\n\u003c/style\u003e\n\u003cdiv class=\"wrap\"\u003e\n\u003cimg class=\"img-fluid z-depth-1 rounded\" id=\"mypic\" src=\"/img/junwang.jpeg\"\u003e\n\u003c/div\u003e\n\u003cp\u003eHi there, this is Jun Wang. Welcome to my website. I'm a programmer. I love programming. \n\u003cp\u003eI am a Senior Applied Scientist at Amazon, specializing in foundational model pre-training, post-training, and reinforcement learning for complex reasoning tasks. My expertise spans the entire lifecycle of large language model (LLM) development, including data curation, pretraining, supervised fine-tuning (SFT), and reinforcement learning with human feedback (RLHF). I have experience training both text-based and code-focused models. Most recently, I\u0026rsquo;ve been focusing on deep RL for math, code generation tasks.\u003c/p\u003e\n\u003c/p\u003e\n\u003cp\u003eOpinions are my own and I do not speak for my employer or my team. On this webpage, I'd like to share my study and my\npaper reading. Feel free to reach out to me if you have any comments/suggestions\u003c/p\u003e","title":"🙋🏻‍♂️About"},{"content":"Add the following into extend_head.html\n{{ if or .Params.math .Site.Params.math }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body, // The following is to parse inline math equation { delimiters: [ {left: \u0026#39;$$\u0026#39;, right: \u0026#39;$$\u0026#39;, display: true}, {left: \u0026#39;\\\\[\u0026#39;, right: \u0026#39;\\\\]\u0026#39;, display: true}, {left: \u0026#39;$\u0026#39;, right: \u0026#39;$\u0026#39;, display: false}, {left: \u0026#39;\\\\(\u0026#39;, right: \u0026#39;\\\\)\u0026#39;, display: false} ] } );\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} Then in the markdown file, in the header section we add math: true.\nMethods to Show Math The the above setup, you can use the following ways in the markdown writeup.\n\\\\(E=mc^2\\\\) $$E=mc^2$$ $E=mc^2$ ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/add_math/","summary":"\u003cp\u003eAdd the following into \u003ccode\u003eextend_head.html\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e{{ if or .Params.math .Site.Params.math }}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;\u003cspan style=\"color:#f92672\"\u003elink\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erel\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;stylesheet\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ehref\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eintegrity\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecrossorigin\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;anonymous\u0026#34;\u003c/span\u003e\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;\u003cspan style=\"color:#f92672\"\u003escript\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edefer\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esrc\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eintegrity\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecrossorigin\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;anonymous\u0026#34;\u003c/span\u003e\u0026gt;\u0026lt;/\u003cspan style=\"color:#f92672\"\u003escript\u003c/span\u003e\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;\u003cspan style=\"color:#f92672\"\u003escript\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edefer\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esrc\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eintegrity\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecrossorigin\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;anonymous\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eonload\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;renderMathInElement(document.body, \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e//   The following is to parse inline math equation\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e  {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e      delimiters: [\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e                  {left: \u0026#39;$$\u0026#39;, right: \u0026#39;$$\u0026#39;, display: true},\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e                  {left: \u0026#39;\\\\[\u0026#39;, right: \u0026#39;\\\\]\u0026#39;, display: true},\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e                  {left: \u0026#39;$\u0026#39;, right: \u0026#39;$\u0026#39;, display: false},\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e                  {left: \u0026#39;\\\\(\u0026#39;, right: \u0026#39;\\\\)\u0026#39;, display: false}\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e              ]\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e          }\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e);\u0026#34;\u003c/span\u003e\u0026gt;\u0026lt;/\u003cspan style=\"color:#f92672\"\u003escript\u003c/span\u003e\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e{{ end }}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen in the markdown file, in the header section we add \u003ccode\u003emath: true\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"methods-to-show-math\"\u003eMethods to Show Math\u003c/h3\u003e\n\u003cp\u003eThe the above setup, you can use the following ways in the markdown writeup.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\\\\(E=mc^2\\\\) \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$$E=mc^2$$\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$E=mc^2$\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Add Math Equation in Blog"},{"content":"In the last blog, we talked about commonly used AWS commands. In this blog, I\u0026rsquo;ll document some commonly used docker commands to save some time when I need them. Images defines what the container is. Container is the actually running virtual machine.\nDocker setup # check docker status systemctl show --property ActiveState docker # if it\u0026#39;s inactive, then start the docker daemon sudo systemctl start docker Image # list all images docker image ls # list all containers docker container ls # stop/remove a container docker container stop container_id docker container rm container_id # pull an image docker pull iamge_name Run docker container # rm is to clean constainer after exit # it is interactive tty # for normal docker image docker run --entrypoint /bin/bash -it \u0026lt;image_name\u0026gt; # for nvidia docker image nvidia-docker run --entrypoint /bin/bash --rm -it --name my_container_name image_name # mount a volume to docker # --rm delete docker on exit nvidia-docker run --entrypoint /bin/bash -v $PWD/transforms_cache:/transforms_cache --rm -it image_name # add env to docker system nvidia-docker run --entrypoint /bin/bash -v $PWD/transforms_cache:/transforms_cache --rm --env SM_CHANNEL_TRAIN=/opt/ml/input/data/train -it image_name # docker run to use GPU, we can use another command docker run --entrypoint /bin/bash --gpus all -it xxxx_image_name Check all containers docker ps -a Clean space docker rmi -f $(docker images -a -q) sudo docker system prune Install package Install packages inside a running docker. Usually we\u0026rsquo;re able to install package based on distributeion of linux system running in the docker. For example, if it\u0026rsquo;s ubuntu, then the command is\napt-get -y update apt-get -y install tmux # package name Docker build We can use the following command to build docker image. Notice that the path is . (current directory). The path (a set of files) is called context and files inside can be used in COPY command in dockerfile. In building process, context will be packed into a tar file. So it\u0026rsquo;s good to put unnecessary files into .dockerignore file and select a reasonable path as context.\ndocker build -f Dockerfile_my_docker -t ${TAG} . --build-arg REGION=${region} Login AWS ECR REGION=us-east-1 ; aws ecr get-login-password --region ${REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com/ # sometimes we can get errors like `no basic auth credentials`, simple method is to remove docker config file rm .docker/config.json # Due to some reason, the docker is not able to update the config file. ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/docker-commands/","summary":"\u003cp\u003eIn the last blog, we talked about commonly used AWS commands. In this blog, I\u0026rsquo;ll document some commonly used docker commands to save some time when I need them. Images defines what the container is. Container is the actually running virtual machine.\u003c/p\u003e\n\u003ch3 id=\"docker-setup\"\u003eDocker setup\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# check docker status\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esystemctl show --property ActiveState docker\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# if it\u0026#39;s inactive, then start the docker daemon\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo systemctl start docker\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"image\"\u003eImage\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# list all images\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker image ls\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# list all containers\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker container ls\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# stop/remove a container\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker container stop container_id\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker container rm container_id\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# pull an image\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker pull iamge_name\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"run-docker-container\"\u003eRun docker container\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# rm is to clean constainer after exit\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# it is interactive tty\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# for normal docker image\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003eentrypoint \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebin\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebash \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eit \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003eimage_name\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# for nvidia docker image\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003envidia\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003edocker run \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003eentrypoint \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebin\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebash \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003erm \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eit \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003ename my_container_name  image_name\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# mount a volume to docker\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# --rm delete docker on exit\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003envidia\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003edocker run \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003eentrypoint \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebin\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebash \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ev \u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ePWD\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etransforms_cache:\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etransforms_cache \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003erm \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eit image_name\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# add env to docker system\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003envidia\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003edocker run \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003eentrypoint \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebin\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebash \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ev \u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ePWD\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etransforms_cache:\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etransforms_cache \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003erm \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003eenv SM_CHANNEL_TRAIN\u003cspan style=\"color:#f92672\"\u003e=/\u003c/span\u003eopt\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003eml\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003einput\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etrain \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eit image_name\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# docker run to use GPU, we can use another command\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003eentrypoint \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebin\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003ebash \u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003egpus all \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eit xxxx_image_name\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"check-all-containers\"\u003eCheck all containers\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker ps -a\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"clean-space\"\u003eClean space\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker rmi -f $(docker images -a -q)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo docker system prune\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"install-package\"\u003eInstall package\u003c/h3\u003e\n\u003cp\u003eInstall packages inside a running docker. Usually we\u0026rsquo;re able to install package based on distributeion of linux system running in the docker. For example, if it\u0026rsquo;s ubuntu, then the command is\u003c/p\u003e","title":"Docker Commands"},{"content":"Bayesian Method Maximium likelihood estimation assumes that there is one distribution with a fix set of parameters which describes data samples, i.e. all data are sampled from this specific distribution. On the other hand, Bayesian method thinks that there could be multiple distrbutions that can describe the data. We choose one set of parameters which parameterize one distribution based on observations to describe the data. The distribution we choose is shaped by the prior we use.\nEM Algorithm ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/probability/","summary":"\u003ch3 id=\"bayesian-method\"\u003eBayesian Method\u003c/h3\u003e\n\u003cp\u003eMaximium likelihood estimation assumes that there is one distribution with a fix set of parameters which describes data samples, i.e. all data are sampled from this specific distribution. On the other hand, Bayesian method thinks that there could be multiple distrbutions that can describe the data. We choose one set of parameters which parameterize one distribution based on observations to describe the data. The distribution we choose is shaped by the prior we use.\u003c/p\u003e\n\u003ch3 id=\"em-algorithm\"\u003eEM Algorithm\u003c/h3\u003e","title":"EM Algorithm"},{"content":"LayerNorm vs BatchNorm BatchNorm is commonly used in computer vision. LayerNorm is widely used in NLP. In CV, the channel dimension is RGB channel. In NLP, the channel dimension is feature dimin (embedding dim). Layer norm normalizes across feature dimension (such as embedding dim) for each of the inputs which removes the dependence on batches. This makes layer normalization well suited for sequence models such as transformers.\nFigure 1. batch norm vs layer norm After understanding of the basics, we can write down the pseudo code as below Pseudo code for batch norm\nfor i in range(seq_len): for j in range(hidden_size): Norm([bert_tensor[k][i][j] for k in range(batch_size)]) Pseudo code for layer norm\nfor i in range(batch_size): for j in range(seq_len): Norm([bert_tensor[i][j][k] for k in range(hidden_size)]) PyTorch implementation is shown blow. When using for BatchNorm, dim=0, assuming x is of the shape (batch_size, H, W, channel). When using for LayerNorm, dim=-1, assuming x is of shape (batch_size, seq_len, embed_dim).\nclass Norm(nn.Module): def __init__(self, num_features, variance_epsilon=1e-12): super(Norm, self).__init__() self.gamma = nn.Parameter(torch.ones(num_features)) self.beta = nn.Parameter(torch.zeros(num_features)) self.variance_epsilon = variance_epsilon def forward(self, x, dim): # layer norm, x is [bz, seq_len, dim], u is [bz, seq_len, 1], x_norm is the same shape with u u = x.mean(dim, keepdim=True) s = (x - u).pow(2).mean(dim, keepdim=True) x_norm = (x - u) / torch.sqrt(s + self.variance_epsilon) return self.gamma * x_norm + self.beta EMA in BN Note that at inference time, there could be no batch dimension for batch norm. In practice, during training people will keep record of moving average of mean and variance. During inference time, these values will be used. The exponential moving average is calculated as follows\nmoving_mean = moving_mean * momentum + batch_mean * (1 - momentum) moving_var = moving_var * momentum + batch_var * (1 - momentum) The momentum is a hyperparameter which is generally chosen to be close to 1. A lower value of momentum means that older values are forgotten sooner. A more efficient way to calculate it is as follows:\nmoving_mean -= (moving_mean - batch_mean) * (1 - momentum) moving_var -= (moving_var - batch_var) * (1 - momentum) ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/transformer/","summary":"\u003ch3 id=\"layernorm-vs-batchnorm\"\u003eLayerNorm vs BatchNorm\u003c/h3\u003e\n\u003cp\u003eBatchNorm is commonly used in computer vision. LayerNorm is widely used in NLP. In CV, the channel dimension is RGB channel. In NLP, the channel dimension is feature dimin (embedding dim).\nLayer norm normalizes across feature dimension (such as embedding dim) for each of the inputs which removes the dependence on batches. This makes layer normalization well suited for sequence models such as transformers.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003cimg alt=\"batch norm vs layer norm\" src=\"images/norm.png\" width=\"80%\" height=auto/\u003e \n    \u003cem\u003eFigure 1. batch norm vs layer norm\u003c/em\u003e\n    \u003cbr\u003e\n\u003c/p\u003e\n\u003cp\u003eAfter understanding of the basics, we can write down the pseudo code as below\nPseudo code for batch norm\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(seq_len):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e j \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(hidden_size):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Norm([bert_tensor[k][i][j] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(batch_size)])\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePseudo code for layer norm\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(batch_size):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e j \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(seq_len):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Norm([bert_tensor[i][j][k] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(hidden_size)])\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyTorch implementation is shown blow. When using for BatchNorm, dim=0, assuming x is of the shape (batch_size, H, W, channel). When using for LayerNorm, dim=-1, assuming x is of shape (batch_size, seq_len, embed_dim).\u003c/p\u003e","title":"Transformer"},{"content":"RL Basics There are two fundamental problems in the sequential decision making process: reinforcement learning and planning. In reinforcement learning, the environment is unknown and agent interacts with environment to improve its policy. Within reinforcement learning there are two kinds of operation: prediction and control. Prediction is given policy, evaluate the future. Control is to optimize the future to find the optimal policy. In RL, we alternatively do predition and control to get the best policy.\nIn terms of methods, RL algorithm can be categorized into two types: model-free algorithm and model-based algorithm. In model-free algorithms, we don\u0026rsquo;t want to or we can\u0026rsquo;t learn the system dynamics. We sample actions and get corresponding rewards to optimize policy or fit a value function. It can further be divied into two methods: policy optimization or value learning.\nPlanning Value Iteration: Value iteration uses dynamic programming to compute the value function iteratively using Bellman equation. Policy iteration — Compute the value function and optimize the policy in alternative steps RL Value-learning/Q-learning: Without an explicit policy, we fit the value-function or Q-value function iteratively with observed rewards under actions taken by an off-policy, like an ε-greedy policy which selects action based on the Q-value function and sometimes random actions for exploration. Policy gradients: using neural network to approximate policy and optimize policy using gradient ascent. Concepts A return is a measured value (or a random variable), representing the actual discounted sum of rewards seen following a specific state or state/action pair. Value function is the expected return function. The discounted return for a trajectory is defined as: $$ U_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + \u0026hellip; $$\nConsequently, the action-value function is defined as $$ Q_{\\pi}(s_t, a_t) = \\mathbb{E_t}[U_t|S_t=s_t, A_t=a_t] $$\nState-value function (or value function) can be calculated as: $$ V_{\\pi}(s_t) = \\mathbb{E_A}[Q_{\\pi}(s_t, A)] = \\sum_a \\pi(a|s_t) \\cdot Q_{\\pi}(s_t, a) $$\nBellman Equations From the definition of the return, we can get the following equation $$ U_t = R_t + \\gamma U_{t+1} $$\nSimilarly it\u0026rsquo;s easy to get the value function $$ \\begin{aligned} V_{\\pi}(s_t) \u0026amp;= \\mathbb{E}[R_t + \\gamma V(s_{t+1}) \\bigm\\vert S_{t} = s] \\\\ \u0026amp;=\\mathbb{E}[R_t \\bigm\\vert S_{t} = s] + \\mathbb{E}[\\gamma V(s_{t+1}) \\bigm\\vert S_{t} = s] \\end{aligned} $$\nFor action-value function, we have $$ \\begin{aligned} Q_{\\pi}(s_t, a_t) =\\mathbb{E}[R_t \\bigm\\vert S_{t} = s, A_{t} = t] + \\mathbb{E}[\\gamma Q_{\\pi}(s_{t+1}, a_{t+1}) \\bigm\\vert S_{t} = s, A_{t} = t] \\end{aligned} $$\nReferences RL — Reinforcement Learning Algorithms Overview Spinning Up in Deep RL ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/rl/rl_basics/","summary":"\u003ch3 id=\"rl-basics\"\u003eRL Basics\u003c/h3\u003e\n\u003cp\u003eThere are two fundamental problems in the sequential decision making process: reinforcement learning and planning.\nIn reinforcement learning, the environment is unknown and agent interacts with environment to improve its policy. Within reinforcement learning there are two kinds of operation: prediction and control. Prediction is given policy, evaluate the future. Control is to optimize the future to find the optimal policy. In RL, we alternatively do predition and control to get the best policy.\u003c/p\u003e\n\u003cp\u003eIn terms of methods, RL algorithm can be categorized into two types: model-free algorithm and model-based algorithm.\nIn model-free algorithms, we don\u0026rsquo;t want to or we can\u0026rsquo;t learn the system dynamics. We sample actions and get corresponding rewards to optimize policy or fit a value function. It can further be divied into two methods: policy optimization or value learning.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePlanning\n\u003cul\u003e\n\u003cli\u003eValue Iteration: Value iteration uses dynamic programming to compute the value function iteratively using Bellman equation.\u003c/li\u003e\n\u003cli\u003ePolicy iteration — Compute the value function and optimize the policy in alternative steps\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRL\n\u003cul\u003e\n\u003cli\u003eValue-learning/Q-learning: Without an explicit policy, we fit the value-function or Q-value function iteratively with observed rewards under actions taken by an off-policy, like an ε-greedy policy which selects action based on the Q-value function and sometimes random actions for exploration.\u003c/li\u003e\n\u003cli\u003ePolicy gradients: using neural network to approximate policy and optimize policy using gradient ascent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"concepts\"\u003eConcepts\u003c/h3\u003e\n\u003cp\u003eA return is a measured value (or a random variable), representing the actual discounted sum of rewards seen following a specific state or state/action pair. Value function is the expected return function.\nThe discounted return for a trajectory is defined as:\n$$\nU_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + \u0026hellip;\n$$\u003c/p\u003e","title":"RL Basics"},{"content":"Remote Development I used to do development on my local machine, and use fswatch and rsync to sync changes to server in real time. It works perfectly when development dependencies are simple and easy to set up. Generally I refer this development mode as local development. However, as more and more development environments are containerized, it becomes non-trivial to set up environment everytime. Recently, I started using VSCode as it has better support to leverage remote server development environment.\nOne great feature of VScode is that it works well with docker and kubernetes, i.e. we can attach VSCode to docker or kubernetes pods easily. In vscode terminal, we can execute commands like we\u0026rsquo;re doing on the server. I call this kind of development as remote development.\nOne problem with remote development is that we can\u0026rsquo;t save our changes locally. Once server dies, all our changes are gone. The solution is to use git. Since docker doesn\u0026rsquo;t come with an editor, when we use git, we have to set vscode as the editor:\ngit config --global core.editor \u0026#34;code --wait\u0026#34; Another issue is we have to install extensions on remote server. For instance, we have to install python extension in order to use python interpreter in remote mode.\nVScode also has a nice extension tool to sync code to remote server.\nVScode Shortcuts If it\u0026rsquo;s on Mac, replace CTRL key with CMD key\n(1) CTRL + X : cut a line (2) duplicate a line: duplicate can be achieved by CTRL+C and CTRL+V with cursor in the line (without selection)\n(3) edit multiple line\nedit multiple line simultaneously: CTRL + SHIFT + up/down arrow. (This will be continuous) ALT + CLICK: can select multiple place and edit CTRL + SHIFT + L: edit all variable in the file (4) block comment: select the block, then CTRL + SHIFT + A\n(5) line comment: CTRL + /\n(6) search \u0026amp; replace\nsingle file search: CTRL + F single file replace: CTRL + H global search: CTRL + SHIFT + F global replace: CTRL + SHIFT + H (7) move a line upward or downward: ALT + up/down arrow\n(8) select a line: CTRL + L\n(9) palette\nopen palette: CTRL + P \u0026gt;: type command @: find symbol #: find all relevant ones : go to line (10) split screen vertical split: CTRL + \\\n(11) open a new window for a new project CTRL + SHIFT + N\n(12) Open terminal CTRL + ` to toggle terminal panel. Note one mac here it\u0026rsquo;s CTRL as well.\n(13) Close search window after global search In keybindings.json add the following lines\n{ \u0026#34;key\u0026#34;: \u0026#34;Escape\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;workbench.view.explorer\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;searchViewletVisible\u0026#34; } (14) How to open unlimited number of tabs In settings.json add the following key-value pair:\n\u0026#34;workbench.editor.enablePreview\u0026#34;: false Config Here is the config settings I used.\n{ \u0026#34;editor.fontSize\u0026#34;: 16, \u0026#34;editor.fontFamily\u0026#34;: \u0026#34;Monaco, \u0026#39;Courier New\u0026#39;, monospace\u0026#34;, \u0026#34;editor.wordWrap\u0026#34;: \u0026#34;on\u0026#34;, \u0026#34;editor.tabCompletion\u0026#34;: \u0026#34;on\u0026#34;, \u0026#34;editor.tabSize\u0026#34;: 4, \u0026#34;editor.suggest.snippetsPreventQuickSuggestions\u0026#34;: false, \u0026#34;gitlens.codeLens.authors.enabled\u0026#34;: false, \u0026#34;git.timeline.showAuthor\u0026#34;: false, \u0026#34;gitlens.codeLens.recentChange.enabled\u0026#34;: false, \u0026#34;gitlens.codeLens.enabled\u0026#34;: false, \u0026#34;gitlens.currentLine.enabled\u0026#34;: false, \u0026#34;gitlens.currentLine.pullRequests.enabled\u0026#34;: false, \u0026#34;redhat.telemetry.enabled\u0026#34;: false, \u0026#34;terminal.integrated.fontSize\u0026#34;: 16, \u0026#34;editor.minimap.enabled\u0026#34;: false, \u0026#34;python.terminal.activateEnvironment\u0026#34;: false, \u0026#34;workbench.editor.enablePreview\u0026#34;: false, \u0026#34;[python]\u0026#34;: { \u0026#34;editor.formatOnType\u0026#34;: true }, \u0026#34;settingsSync.ignoredSettings\u0026#34;: [], \u0026#34;settingsSync.ignoredExtensions\u0026#34;: [], //失去焦点后自动保存 \u0026#34;files.autoSave\u0026#34;: \u0026#34;onFocusChange\u0026#34;, \u0026#34;terminal.integrated.inheritEnv\u0026#34;: false, \u0026#34;gitlens.codeLens.recentChange.enabled\u0026#34;: false, \u0026#34;gitlens.blame.avatars\u0026#34;: false, \u0026#34;gitlens.blame.format\u0026#34;: \u0026#34;${message|20?} ${agoOrDate|14-}\u0026#34;, \u0026#34;gitlens.hovers.avatars\u0026#34;: false, \u0026#34;json.format.enable\u0026#34;: true, \u0026#34;files.associations\u0026#34;: { \u0026#34;*.jsonl\u0026#34;: \u0026#34;json\u0026#34; }, \u0026#34;update.showReleaseNotes\u0026#34;: false, \u0026#34;workbench.editor.revealIfOpen\u0026#34;: true } Remote SSH config Install the plugin from here Connecting to a single serve the sftp.json is like this:\n{ \u0026#34;name\u0026#34;: \u0026#34;Profile1\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;42-32-123-45.mycompute.com\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;sftp\u0026#34;, \u0026#34;port\u0026#34;: 22, \u0026#34;secure\u0026#34;: true, \u0026#34;username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;remotePath\u0026#34;: \u0026#34;/home/ubuntu/myproject\u0026#34;, // \u0026lt;--- This is the path which will be downloaded if you \u0026#34;Download Project\u0026#34; \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;openSsh\u0026#34;: false, \u0026#34;context\u0026#34;: \u0026#34;my_local_directory_on_mac\u0026#34;, // source dir \u0026#34;privateKeyPath\u0026#34;: \u0026#34;/Users/xxx/.ssh/id_rsa\u0026#34;, // needed when use passwordless ssh \u0026#34;uploadOnSave\u0026#34;: true, \u0026#34;useTempFile\u0026#34;: false, \u0026#34;ignore\u0026#34;: [ \u0026#34;**/.vscode/**\u0026#34;, \u0026#34;**/.git/**\u0026#34;, \u0026#34;**/.DS_Store\u0026#34; ] } Connecting to multiple servers, the \u0026ldquo;sftp.json\u0026rdquo; is like this:\n{ \u0026#34;protocol\u0026#34;: \u0026#34;sftp\u0026#34;, \u0026#34;port\u0026#34;: 22, \u0026#34;secure\u0026#34;: true, \u0026#34;remotePath\u0026#34;: \u0026#34;/home/ubuntu/myproject\u0026#34;, // \u0026lt;--- This is the path which will be downloaded if you \u0026#34;Download Project\u0026#34; \u0026#34;openSsh\u0026#34;: false, \u0026#34;context\u0026#34;: \u0026#34;my_local_directory_on_mac\u0026#34;, // source dir \u0026#34;privateKeyPath\u0026#34;: \u0026#34;/Users/xxx/.ssh/id_rsa\u0026#34;, // needed when use passwordless ssh \u0026#34;uploadOnSave\u0026#34;: false, \u0026#34;useTempFile\u0026#34;: false, \u0026#34;ignore\u0026#34;: [ \u0026#34;**/.vscode/**\u0026#34;, \u0026#34;**/.git/**\u0026#34;, \u0026#34;**/.DS_Store\u0026#34; ], \u0026#34;profiles\u0026#34;: { \u0026#34;my_server1\u0026#34;:{ \u0026#34;name\u0026#34;: \u0026#34;Profile1\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;42-32-123-45.mycompute.com\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34; }, \u0026#34;my_server2\u0026#34;:{ \u0026#34;name\u0026#34;: \u0026#34;Profile2\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;42-32-123-46.mycompute.com\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34; }, \u0026#34;my_server3\u0026#34;:{ \u0026#34;name\u0026#34;: \u0026#34;Profile3\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;42-32-123-47.mycompute.com\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34; } } } Paste and Indentation Install Paste and Indent from g3rry Adding the following to keybindings json\n{ \u0026#34;key\u0026#34;: \u0026#34;ctrl+v\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;pasteAndIndent.action\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;editorTextFocus \u0026amp;\u0026amp; !editorReadonly\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;ctrl+v\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;editor.action.clipboardPasteAction\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;!editorTextFocus\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;ctrl+shift+v\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;editor.action.clipboardPasteAction\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;editorTextFocus \u0026amp;\u0026amp; !editorReadonly\u0026#34; } References [1] https://code.visualstudio.com/shortcuts/keyboard-shortcuts-linux.pdf\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/programming_language/remote-development-with-vscode/","summary":"\u003ch3 id=\"remote-development\"\u003eRemote Development\u003c/h3\u003e\n\u003cp\u003eI used to do development on my local machine, and use \u003cstrong\u003efswatch\u003c/strong\u003e and \u003cstrong\u003ersync\u003c/strong\u003e to sync changes to server in real time. It works perfectly when development dependencies are simple and easy to set up. Generally I refer this development mode as local development. However, as more and more development environments are containerized, it becomes non-trivial to set up environment everytime. Recently, I started using VSCode as it has better support to leverage remote server development environment.\u003c/p\u003e\n\u003cp\u003eOne great feature of VScode is that it works well with docker and kubernetes, i.e. we can attach VSCode to docker or kubernetes pods easily. In vscode terminal, we can execute commands like we\u0026rsquo;re doing on the server. I call this kind of development as remote development.\u003c/p\u003e\n\u003cp\u003eOne problem with remote development is that we can\u0026rsquo;t save our changes locally. Once server dies, all our changes are gone. The solution is to use git. Since docker doesn\u0026rsquo;t come with an editor, when we use git, we have to set vscode as the editor:\u003c/p\u003e","title":"Remote Development with VSCode"},{"content":"Makefile Syntax A Makefile consists of a set of rules. A rule generally looks like this:\ntargets: prerequisites command command command` The targets are file names, separated by spaces. Typically, there is only one per rule. The commands are a series of steps typically used to make the target(s). These need to start with a tab character, not spaces. The prerequisites are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are also called dependencies Commands and execution Command Echoing/Silencing Add an @ before a command to stop it from being printed\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/makefile/","summary":"\u003ch3 id=\"makefile-syntax\"\u003eMakefile Syntax\u003c/h3\u003e\n\u003cp\u003eA Makefile consists of a set of \u003cem\u003erules\u003c/em\u003e. A rule generally looks like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etargets: prerequisites\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    command\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    command\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    command`\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eThe \u003cem\u003etargets\u003c/em\u003e are file names, separated by spaces. Typically, there is only one per rule.\u003c/li\u003e\n\u003cli\u003eThe \u003cem\u003ecommands\u003c/em\u003e are a series of steps typically used to make the target(s). These \u003cem\u003eneed to start with a tab character\u003c/em\u003e, not spaces.\u003c/li\u003e\n\u003cli\u003eThe \u003cem\u003eprerequisites\u003c/em\u003e are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are also called \u003cem\u003edependencies\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"commands-and-execution\"\u003eCommands and execution\u003c/h3\u003e\n\u003ch4 id=\"command-echoingsilencing\"\u003eCommand Echoing/Silencing\u003c/h4\u003e\n\u003cp\u003eAdd an \u003ccode\u003e@\u003c/code\u003e before a command to stop it from being printed\u003c/p\u003e","title":"Makefile"},{"content":"Sigmoid Sigmoid is one of the most used activation functions. $$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$ It has nice mathematical proprities: $$ \\sigma^\\prime(x) = \\sigma(x) \\left[ 1 - \\sigma(x) \\right] $$ and $$ \\left[log\\sigma(x)\\right]^\\prime = 1 - \\sigma(x) \\\\ \\left[log\\left(1 - \\sigma(x)\\right)\\right]^\\prime = - \\sigma(x) $$\nLogistic Regression For a binary classification problem, for an example $x = (x_1, x_2, \\dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as: $$ \\begin{aligned} h_{\\theta}(1|x) \u0026amp;= \\sigma(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dotsb + \\theta_nx_n) \\\\ \u0026amp;= \\sigma(\\theta^{\\mathrm{T}}x) \\\\ \u0026amp;= \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\end{aligned} $$ Consequently, for the negative class, $$ \\begin{aligned} h_{\\theta}(0|x) \u0026amp;= 1 - \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\\\ \u0026amp;= \\frac{1}{1+e^{\\theta^{\\mathrm{T}}x}} \\\\ \u0026amp;= \\sigma(-\\theta^{\\mathrm{T}}x) \\end{aligned} $$\nSingle sample cost function of logistic regression is expressed as: $$ L(\\theta) = -y_i \\cdot \\log(h_\\theta(x_i)) - (1-y_i) \\cdot \\log(1 - h_\\theta(x_i)) $$ Notice that in the second term $1 - h_\\theta(x_i)$ is the negative class probability\nCross Entropy Cross entropy defines the distance between model output distribution and the groudtruth distribution. $$ H(y,p) = -\\sum_{i}y_i \\log(p_i) $$ Since the $y_i$ is the class label (1 for positive class, 0 for negative), essentially here we\u0026rsquo;re summing up the negative log probably of the positive label. What is the reason why we say that negative log likehood and cross entropy is equivalent.\nWhen normalization function (we can say activation function of last layer) is softmax function, namely, for each class $s_i$ the probability is given by $$ f(s)_i = \\frac{e^{s_i}}{ \\sum_j ^{C} e^{s_j}} $$ Given the above cross entropy equation, and there is only one positive class, the softmax cross entropy loss is: $$ L = -log(\\frac{e^{s_p}}{ \\sum_j ^{C} e^{s_j}}) $$ here $p$ stands for positive class. If we want to get the gradient of loss with respect to the logits ($s_i$ here), for positive class we can have $$ \\frac{\\partial{L}}{\\partial{s_p}} = \\left(\\frac{e^{s_p}}{ \\sum_j ^{C} e^{s_j}} - 1 \\right) \\\\ \\frac{\\partial{L}}{\\partial{s_n}} = \\left(\\frac{e^{s_n}}{ \\sum_j ^{C} e^{s_j}} - 1 \\right) $$ We can put this in one equation, which is what we commonly see as the graident of cross entropy loss for softmax $$ \\frac{\\partial{L}}{\\partial{s_i}} = p_i - y_i $$ $p_i$ is the probability and $y_i$ is the label, 1 for positive class and 0 for negative class.\nBinary Cross Entropy In the above section, we talked about softmax cross entropy loss, here we talk about binary cross entropy loss which is also called Sigmoid cross entropy loss.\nWe apply sigmoid function to the output logits before BCE. Notice that here we apply the function to each element in the tensor, all the elements are not related to each other, this is why BCE is widely used for multi-label classification task.\nFor each label, we can calculate the loss in the same way as the logistic regression loss.\nimport torch import numpy as np pred = np.array([[-0.4089, -1.2471, 0.5907], [-0.4897, -0.8267, -0.7349], [0.5241, -0.1246, -0.4751]]) # after sigmod, pred becomes # [[0.3992, 0.2232, 0.6435], # [0.3800, 0,3044, 0.3241], # [0.6281, 0.4689, 0.3834]] label = np.array([[0, 1, 1], [0, 0, 1], [1, 0, 1]]) # after cross entropy, pred becomes # [[-0.5095, -1.4997, -0.4408], take neg and avg 0.8167 # [-0.4780, -0.3630, -1.1267], take neg and avg 0.6559 # [-0.4651, -0.6328, -0.9587]] take neg and avg 0.6855 # 0 * ln0.3992 + (1-0) * ln(1-0.3992) = -0.5095 # (0.8167 + 0.6559 + 0.6855) / 3. = 0.7194 pred = torch.from_numpy(pred).float() label = torch.from_numpy(label).float() crition1 = torch.nn.BCEWithLogitsLoss() loss1 = crition1(pred, label) print(loss1) # 0.7193 crition2 = torch.nn.MultiLabelSoftMarginLoss() loss2 = crition2(pred, label) print(loss2) # 0.7193 Noise Contrastive Estimation Noise contrastive estimation or negative sampling is a commonly used computation trick in ML to deal with expansive softmax computation or intractable partition function in computation.\nThe derivation of NCE loss sometimes can be bewildering, but the idea is actually very simple. For example, in word2vec implementation, the negative sampling is to choose 1 positive target and 5 negative target, and calculate the binary cross entropy loss (binary logistic loss) and then do backward propagation.\nContrastive Loss CLIP Loss CLIP loss is the same with the paper from [3]. The negatives here are used for contrastive learning. However, they\u0026rsquo;re not using NCE method like word2vec. It\u0026rsquo;s more like softmax cross entropy.\nimport torch from torch import nn import torch.nn.functional as F import config as CFG from modules import ImageEncoder, TextEncoder, ProjectionHead class CLIPModel(nn.Module): def __init__( self, temperature=CFG.temperature, image_embedding=CFG.image_embedding, text_embedding=CFG.text_embedding, ): super().__init__() self.image_encoder = ImageEncoder() self.text_encoder = TextEncoder() self.image_projection = ProjectionHead(embedding_dim=image_embedding) self.text_projection = ProjectionHead(embedding_dim=text_embedding) self.temperature = temperature def forward(self, batch): # Getting Image and Text Features image_features = self.image_encoder(batch[\u0026#34;image\u0026#34;]) text_features = self.text_encoder( input_ids=batch[\u0026#34;input_ids\u0026#34;], attention_mask=batch[\u0026#34;attention_mask\u0026#34;] ) # Getting Image and Text Embeddings (with same dimension) image_embeddings = self.image_projection(image_features) text_embeddings = self.text_projection(text_features) # Calculating the Loss logits = (text_embeddings @ image_embeddings.T) / self.temperature images_similarity = image_embeddings @ image_embeddings.T texts_similarity = text_embeddings @ text_embeddings.T targets = F.softmax( (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1 ) texts_loss = cross_entropy(logits, targets, reduction=\u0026#39;none\u0026#39;) images_loss = cross_entropy(logits.T, targets.T, reduction=\u0026#39;none\u0026#39;) loss = (images_loss + texts_loss) / 2.0 # shape: (batch_size) return loss.mean() def cross_entropy(preds, targets, reduction=\u0026#39;none\u0026#39;): log_softmax = nn.LogSoftmax(dim=-1) loss = (-targets * log_softmax(preds)).sum(1) if reduction == \u0026#34;none\u0026#34;: return loss elif reduction == \u0026#34;mean\u0026#34;: return loss.mean() if __name__ == \u0026#39;__main__\u0026#39;: images = torch.randn(8, 3, 224, 224) input_ids = torch.randint(5, 300, size=(8, 25)) attention_mask = torch.ones(8, 25) batch = { \u0026#39;image\u0026#39;: images, \u0026#39;input_ids\u0026#39;: input_ids, \u0026#39;attention_mask\u0026#39;: attention_mask } CLIP = CLIPModel() loss = CLIP(batch) print(\u0026#34;\u0026#34;) Reference Learning Transferable Visual Models From Natural Language Supervision http://www.cnblogs.com/peghoty/p/3857839.html contrastive learning of medical visual representations from paired images and text https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/CLIP.py ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/loss/","summary":"\u003ch3 id=\"sigmoid\"\u003eSigmoid\u003c/h3\u003e\n\u003cp\u003eSigmoid is one of the most used activation functions.\n$$\n\\sigma(x) = \\frac{1}{1+e^{-x}}\n$$\nIt has nice mathematical proprities:\n$$\n\\sigma^\\prime(x) = \\sigma(x) \\left[ 1 - \\sigma(x) \\right]\n$$\nand\n$$\n\\left[log\\sigma(x)\\right]^\\prime = 1 - \\sigma(x) \\\\\n\\left[log\\left(1 - \\sigma(x)\\right)\\right]^\\prime = - \\sigma(x)\n$$\u003c/p\u003e\n\u003ch3 id=\"logistic-regression\"\u003eLogistic Regression\u003c/h3\u003e\n\u003cp\u003eFor a binary classification problem, for an example $x = (x_1, x_2, \\dotsb , x_n)^T$, the hypothesis function (for the positive class) can be written as:\n$$\n\\begin{aligned}\nh_{\\theta}(1|x) \u0026amp;= \\sigma(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dotsb + \\theta_nx_n) \\\\\n\u0026amp;= \\sigma(\\theta^{\\mathrm{T}}x) \\\\\n\u0026amp;= \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}}\n\\end{aligned}\n$$\nConsequently, for the negative class,\n$$\n\\begin{aligned}\nh_{\\theta}(0|x) \u0026amp;= 1 - \\frac{1}{1+e^{-\\theta^{\\mathrm{T}}x}} \\\\\n\u0026amp;= \\frac{1}{1+e^{\\theta^{\\mathrm{T}}x}} \\\\\n\u0026amp;= \\sigma(-\\theta^{\\mathrm{T}}x)\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eSingle sample cost function of logistic regression is expressed as:\n$$\nL(\\theta) = -y_i \\cdot \\log(h_\\theta(x_i)) -  (1-y_i) \\cdot \\log(1 - h_\\theta(x_i))\n$$\nNotice that in the second term $1 - h_\\theta(x_i)$ is the negative class probability\u003c/p\u003e","title":"Loss in ML"},{"content":"Git Git merge Suppose we\u0026rsquo;re on master branch, if we want to override the changes in the master branch with feature branch, we can use the following command\ngit merge -X theirs feature to keep the master branch changes:\ngit merge -X ours feature If we want to rebase of current branch onto the master, and want to keep feature branch\ngit rebase master -X theirs if we want to keep master branch changes over our feature branch, the\ngit rebase master -X ours To summarize, we can have the following table:\n\u0026nbsp; Currently on Command \u0026nbsp; \u0026nbsp; Strategy \u0026nbsp; \u0026nbsp; Outcome \u0026nbsp;master git merge feature \u0026nbsp; -Xtheirs \u0026nbsp; \u0026nbsp; Keep changes from feature branch master git merge feature -Xours keep changes from master branch \u0026nbsp;feature git rebase master \u0026nbsp; -Xtheirs \u0026nbsp; \u0026nbsp; Keep changes from feature branch feature git rebase master -Xours keep changes from master branch {:.mbtablestyle} #### Git Diff To check two branch difference, suppose we\u0026rsquo;re on branch1, then we can do,\ngit diff HEAD..master Delete a remote branch Delete a remote branch\ngit push origin -d remote_branch_name Git rebase To fixup, squash, edit, drop, reword and many other operations on the previous N commit:\ngit rebase -i HAED~N Git commit git commit --amend --no-edit Undo git add The simplest way to undo a git add is to use git reset. It removes staged file, but will keeop the local changes there.\ngit reset file_path Git check difference Use the following command to checkout COMMIT (commit hash) ancestor and COMMIT difference\ngit diff COMMIT~ COMMIT git diff HEAD~ HEAD Git rebase resolve conflicts Sometimes when we do git rebase and we have many commits, we have to resolve a lot of conflicts, which can be really frustrating. One quick way might be to squash commits first to have a single commit, then do rebase. Another way is what we describe below.\nFirst, checkout temp branch from feature branch and start a standard merge\ngit checkout -b temp git merge origin/master git commit -m \u0026#34;Merge branch \u0026#39;origin/master\u0026#39; into \u0026#39;temp\u0026#39;\u0026#34; You will have to resolve conflicts, but only once and only real ones. Then stage all files and finish merge. Then return to your feature branch and start rebase, but with automatically resolving any conflicts.\ngit checkout feature git rebase origin/master -X theirs Branch has been rebased, but project is probably in invalid state. We just need to restore project state, so it will be exact as on branch \u0026rsquo;temp\u0026rsquo;. Technically we just need to copy its tree (folder state) via low-level command git commit-tree. Plus merging into current branch just created commit.\ngit merge --ff $(git commit-tree temp^{tree} -m \u0026#34;Fix after rebase\u0026#34; -p HEAD) git branch -D temp More details is here: https://github.com/capslocky/git-rebase-via-merge. Thanks to the original author!\nHow to Split Large PR Sometimes, we have a giant PR which we want to merge. Often times, it gives reviewer a lot of headache. Now we learn how to split large PR into smaller ones. Suppose our feature branch is my_feature_branch, then we can get the diff file using:\nStep 1 git diff master my_feature_branch \u0026gt; ../huge_pr_file Step 2 We switch back to master and create a new feature branch for the first small pr.\ngit checkout master git checkout -b first_small_feature_pr Step 3 Whilst on the first small feature branch, we apply all the code changes.\ngit apply ../huge_pr_file After running this command, we\u0026rsquo;ll see all the unstaged changes on the first_small_feature_pr branch. Now we can stage any files we want, commit and push them.\nStep 4 After pushing/committing the first feature pr, we can stash all the remaining changes (so that we won\u0026rsquo;t commit in this branch).\ngit stash --include-untracked --keep-index Step 5 Repeat this process from above to create a second PR. Based on the dependency or references, we might have to create a new branch based on the other small PR branch.\ngit checkout master git checkout -b second_small_feature_pr #OR git checkout first_small_feature_pr git checkout -b second_small_feature_pr How to show difference of commits without checkout to the branch # If we know the git commit git diff xxxx~ xxxx # if we know the branch name git diff my_branch~ my_branch Git submodule # Add submodule git submodule add -b branch_name URL_to_Git_repo optional_dir_rename # update you submodules with origin/main or origin/your_branch git submodule update --remote # clone repo with submodules git clone --recurse-submodules repo_url # get submodules after clone git submodule update --init git submodule update --init --recursive # if there are nested submodules Git config # set default branch name git config --global init.defaultBranch main How to Push When we clone a repo using HTTPS, how to push the repo using SSH. We have to set a new remote url.\ngit remote set-url origin git@github.com:xxx/yyy.git References [1] https://itsnotbugitsfeature.com/2019/10/22/splitting-a-big-pull-request-into-smaller-review-able-ones/\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/git/git/","summary":"\u003ch3 id=\"git\"\u003eGit\u003c/h3\u003e\n\u003ch4 id=\"git-merge\"\u003eGit merge\u003c/h4\u003e\n\u003cp\u003eSuppose we\u0026rsquo;re on \u003cstrong\u003emaster\u003c/strong\u003e branch, if we want to override the changes in the master branch with feature branch, we can use the following command\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit merge -X theirs feature\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eto keep the master branch changes:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit merge -X ours feature\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIf we want to rebase of current branch onto the master, and want to keep feature branch\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit rebase master -X theirs\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eif we want to keep master branch changes over our feature branch, the\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit rebase master -X ours\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo summarize, we can have the following table:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u0026nbsp; Currently on\u003c/th\u003e\n\u003cth\u003eCommand \u0026nbsp; \u0026nbsp;\u003c/th\u003e\n\u003cth\u003eStrategy \u0026nbsp; \u0026nbsp;\u003c/th\u003e\n\u003cth\u003eOutcome\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u0026nbsp;master\u003c/td\u003e\n\u003ctd\u003egit merge feature \u0026nbsp;\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003e-Xtheirs\u003c/strong\u003e \u0026nbsp; \u0026nbsp;\u003c/td\u003e\n\u003ctd\u003eKeep changes from feature branch\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emaster\u003c/td\u003e\n\u003ctd\u003egit merge feature\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e-Xours\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003ekeep changes from master branch\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u0026nbsp;feature\u003c/td\u003e\n\u003ctd\u003egit rebase master \u0026nbsp;\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003e-Xtheirs\u003c/strong\u003e \u0026nbsp; \u0026nbsp;\u003c/td\u003e\n\u003ctd\u003eKeep changes from feature branch\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003efeature\u003c/td\u003e\n\u003ctd\u003egit rebase master\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e-Xours\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003ekeep changes from master branch\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n{:.mbtablestyle}\n\u003cbr\u003e\n#### Git Diff\n\u003cp\u003eTo check two branch difference, suppose we\u0026rsquo;re on branch1, then we can do,\u003c/p\u003e","title":"Git"},{"content":"Commonly Used Commands SCRIPT_DIR=\u0026#34;$( cd \u0026#34;$( dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34; )\u0026#34; \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; pwd )\u0026#34; Git Git merge Suppose we\u0026rsquo;re on master branch, if we want to override the changes in the master branch with feature branch, we can use the following command\ngit merge -X theirs feature to keep the master branch changes:\ngit merge -X ours feature If we want to rebase of current branch onto the master, and want to keep feature branch\ngit rebase master -X theirs if we want to keep master branch changes over our feature branch, the\ngit rebase master -X ours To summarize, we can have the following table:\n\u0026nbsp; Currently on Command \u0026nbsp; \u0026nbsp; Strategy \u0026nbsp; \u0026nbsp; Outcome \u0026nbsp;master git merge feature \u0026nbsp; -Xtheirs \u0026nbsp; \u0026nbsp; Keep changes from feature branch master git merge feature -Xours keep changes from master branch \u0026nbsp;feature git rebase master \u0026nbsp; -Xtheirs \u0026nbsp; \u0026nbsp; Keep changes from feature branch feature git rebase master -Xours keep changes from master branch {:.mbtablestyle} #### Git Diff To check two branch difference, suppose we\u0026rsquo;re on branch1, then we can do,\ngit diff HEAD..master Delete a remote branch Delete a remote branch\ngit push origin -d remote_branch_name Git rebase To fixup, squash, edit, drop, reword and many other operations on the previous N commit:\ngit rebase -i HAED~N Git commit git commit --amend --no-edit Undo git add The simplest way to undo a git add is to use git reset. It removes staged file, but will keeop the local changes there.\ngit reset file_path Git check difference Use the following command to checkout COMMIT (commit hash) ancestor and COMMIT difference\ngit diff COMMIT~ COMMIT git diff HEAD~ HEAD Git rebase resolve conflicts Sometimes when we do git rebase and we have many commits, we have to resolve a lot of conflicts, which can be really frustrating. One quick way might be to squash commits first to have a single commit, then do rebase. Another way is what we describe below.\nFirst, checkout temp branch from feature branch and start a standard merge\ngit checkout -b temp git merge origin/master git commit -m \u0026#34;Merge branch \u0026#39;origin/master\u0026#39; into \u0026#39;temp\u0026#39;\u0026#34; You will have to resolve conflicts, but only once and only real ones. Then stage all files and finish merge. Then return to your feature branch and start rebase, but with automatically resolving any conflicts.\ngit checkout feature git rebase origin/master -X theirs Branch has been rebased, but project is probably in invalid state. We just need to restore project state, so it will be exact as on branch \u0026rsquo;temp\u0026rsquo;. Technically we just need to copy its tree (folder state) via low-level command git commit-tree. Plus merging into current branch just created commit.\ngit merge --ff $(git commit-tree temp^{tree} -m \u0026#34;Fix after rebase\u0026#34; -p HEAD) git branch -D temp More details is here: https://github.com/capslocky/git-rebase-via-merge. Thanks to the original author!\nFind Command The original post is here: https://www.baeldung.com/linux/find-exec-command\n1. Basics The find command is comprised of two main parts, the expression and the action. When we initially use find, we usually start with the expression part. This is the part that allows us to specify a filter that defines which files to select.\nA classic example would be:\n$ find Music/ -name *.mp3 -type f Music/Gustav Mahler/01 - Das Trinklied vom Jammer der Erde.mp3 Music/Gustav Mahler/02 - Der Einsame im Herbst.mp3 The action part in this example is the default action, -print. This action prints the resulting paths with newline characters in between. It’ll run if no other action is specified.\nIn contrast, the -exec action allows us to execute commands on the resulting paths. Let’s say we want to run the file command on the list of mp3 files we just found to determine their filetype. We can achieve this by running the following command:\n$ find Music/ -name *.mp3 -exec file {} \\; Music/Gustav Mahler/01 - Das Trinklied vom Jammer der Erde.mp3: Audio file with ID3 version 2.4.0, contains:MPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo Let’s dissect the arguments passed to the -exec flag, which include: A command: file A placeholder: {} A command delimiter: ; Now we’ll walk through each of these three parts in-depth.\n2. The Command Any command that can be executed by our shell is acceptable here. We should note that this isn’t our shell executing the command, rather we’re using Linux’s exec directly to execute the command. This means that any shell expansion won’t work here, as we don’t have a shell. Another effect is the unavailability of shell functions or aliases.\nAs a workaround for our missing shell functions, we can export them and call bash -c with our requested function on our file. To see this in action, we’ll continue with our directory of Mahler’s mp3 files. Let’s create a shell function that shows the track name and some details about the quality:\nfunction mp3info() { TRACK_NAME=$(basename \u0026#34;$1\u0026#34;) FILE_DATA=$(file \u0026#34;$1\u0026#34; | awk -F, \u0026#39;{$1=$2=$3=$4=\u0026#34;\u0026#34;; print $0 }\u0026#39;) echo \u0026#34;${TRACK_NAME%.mp3} : $FILE_DATA\u0026#34; } If we try to run the mp3info command on all of our files, -exec will complain that it doesn’t know about mp3info:\nfind . -name \u0026#34;*.mp3\u0026#34; -exec mp3info {} \\; find: ‘mp3info’: No such file or directory As mentioned earlier, to fix this, we’ll need to export our shell function and run it as part of a spawned shell:\n$ export -f mp3info $ find . -name \u0026#34;*.mp3\u0026#34; -exec bash -c \u0026#34;mp3info \\\u0026#34;{}\\\u0026#34;\u0026#34; \\; 01 - Das Trinklied vom Jammer der Erde : 128 kbps 44.1 kHz Stereo 02 - Der Einsame im Herbst : 128 kbps 44.1 kHz Stereo 03 - Von der Jugend : 128 kbps 44.1 kHz Stereo Note that because some of our file names hold spaces, we need to quote the results placeholder.\n3. The Results Placeholder The results placeholder is denoted by two curly braces {}.\nWe can use the placeholder multiple times if necessary:\nfind . -name \u0026#34;*.mp3\u0026#34; -exec bash -c \u0026#34;basename \\\u0026#34;{}\\\u0026#34; \u0026amp;\u0026amp; file \\\u0026#34;{}\\\u0026#34; | awk -F: \u0026#39;{\\$1=\\\u0026#34;\\\u0026#34;; print \\$0 }\u0026#39;\u0026#34; \\; 01 - Das Trinklied vom Jammer der Erde.mp3 Audio file with ID3 version 2.4.0, contains MPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo 02 - Der Einsame im Herbst.mp3 Audio file with ID3 version 2.4.0, contains MPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo 03 - Von der Jugend.mp3 Audio file with ID3 version 2.4.0, contains MPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo In the above example, we ran both the basename, as well as the file commands. To allow us to concatenate the commands, we spawned a separate shell, as explained above.\n4. The Delimiter We need to provide the find command with a delimiter so it’ll know where our -exec arguments stop. Two types of delimiters can be provided to the -exec argument: the semi-colon(;) or the plus sign (+). As we don’t want our shell to interpret the semi-colon, we need to escape it (;).\nThe delimiter determines the way find handles the expression results. If we use the semi-colon (;), the -exec command will be repeated for each result separately. On the other hand, if we use the plus sign (+), all of the expressions’ results will be concatenated and passed as a whole to the -exec command, which will run only once.\nLet’s see the use of the plus sign with another example:\n$ find . -name \u0026#34;*.mp3\u0026#34; -exec echo {} + ./Gustav Mahler/01 - Das Trinklied vom Jammer der Erde.mp3 ./Gustav Mahler/02 - Der Einsame im Herbst.mp3 ./Gustav Mahler/03 - Von der Jugend.mp3 ./Gustav Mahler/04 - Von der Schönheit.mp3 ./Gustav Mahler/05 - Der Trunkene im Frühling.mp3 ./Gustav Mahler/06 - Der Abschied.mp3 When running echo, a newline is generated for every echo call, but since we used the plus-delimiter, only a single echo call was made. Let’s compare this result to the semi-colon version:\n$ find . -name \u0026#34;*.mp3\u0026#34; -exec echo {} \\; ./Gustav Mahler/01 - Das Trinklied vom Jammer der Erde.mp3 ./Gustav Mahler/02 - Der Einsame im Herbst.mp3 From a performance point of view, we usually prefer to use the plus-sign delimiter, as running separate processes for each file can incur a serious penalty in both RAM and processing time.\nHowever, we may prefer using the semi-colon delimiter in one of the following cases:\nThe tool run by -exec doesn’t accept multiple files as an argument. Running the tool on so many files at once might use up too much memory. We want to start getting some results as soon as possible, even though it’ll take more time to get all the results. One of the commands I use often with is\nfind my_directory/ -type f -exec lfs hsm_restore {} \\; Xargs Command There are commands that only take input as arguments like cp, rm, echo etc. We can use xargs to convert input coming from standard input to arguements.\n$find . -type f -name \u0026#34;*.log\u0026#34; | xargs -n 1 echo rm rm ./log/file5.log rm ./log/file6.log -n 1 argument, xargs turns each line into a command of its own.\n-I option takes a string that gets replaced with the supplied input before the command executes. Commond choices are {} and %.\nfind ./log -type f -name \u0026#34;*.log\u0026#34; | xargs -I % mv % backup/ aws s3 ls --recursive s3://my-bucket/ | grep \u0026#34;my_test\u0026#34; | cut -d\u0026#39; \u0026#39; -f4 | xargs -I{} aws s3 rm s3://my-bucket/{} -P option specify the number of parallel processes used in executing the commands over the input argument list.\nThe command below parallelly encodes a series of wav files to mp3 format: $find . -type f -name \u0026lsquo;*.wav\u0026rsquo; -print0 |xargs -0 -P 3 -n 1 mp3 -V8\nWhen combining find with xargs, it\u0026rsquo;s usually faster than using exec mentioned above.\nExport and Xargs To export each line as an environment variable we can use\nexport $(cat filename | xargs -L 1) rsync command When use the following command, be careful about the relative path. In this command, we\u0026rsquo;re using 4 processes.\nls /my_model/checkpoints/source_dir | xargs -n1 -P4 -I% rsync -aP % target_dir # the above command may suffer when there is only one folder inside the source dir that is too big. To solve this issue, use the following command. Note the -R here is relative path. cd src_dir \u0026amp;\u0026amp; find . -type f -print0 | xargs -0 -P4 -I% rsync -avR % target_dir Tips and Tricks Sometimes we need to copy multiple files from a directory. In order to copy multiple ones without explicitly listing all the absolute paths, we can use the following way. However, to use the autocomplete, we need to type left { first without the right one. cp /root/local/libs/{a.py, b.py} target_dir Sort and Cut Sometimes we want to sort based on a specific field in string. We can use the following command\n# cut 2nd field, from 2nd field split based on equal sign and sort based on the 3rd field # cut field delimiter is -d, and sort field delimiter is -t echo xxx | grep yyy | cut -d \u0026#34; \u0026#34; -f2 | | sort -t = -k 3 -n | uniq | less How to Split Large PR Sometimes, we have a giant PR which we want to merge. Often times, it gives reviewer a lot of headache. Now we learn how to split large PR into smaller ones. Suppose our feature branch is my_feature_branch, then we can get the diff file using:\nStep 1 git diff master my_feature_branch \u0026gt; ../huge_pr_file Step 2 We switch back to master and create a new feature branch for the first small pr.\ngit checkout master git checkout -b first_small_feature_pr Step 3 Whilst on the first small feature branch, we apply all the code changes.\ngit apply ../huge_pr_file After running this command, we\u0026rsquo;ll see all the unstaged changes on the first_small_feature_pr branch. Now we can stage any files we want, commit and push them.\nStep 4 After pushing/committing the first feature pr, we can stash all the remaining changes (so that we won\u0026rsquo;t commit in this branch).\ngit stash --include-untracked --keep-index Step 5 Repeat this process from above to create a second PR. Based on the dependency or references, we might have to create a new branch based on the other small PR branch.\ngit checkout master git checkout -b second_small_feature_pr #OR git checkout first_small_feature_pr git checkout -b second_small_feature_pr Heredoc In Bash and other shells like Zsh, a Here document (Heredoc) is a type of redirection that allows you to pass multiple lines of input to a command.\nThe syntax of writing HereDoc takes the following form:\n[COMMAND] \u0026lt;\u0026lt;[-] \u0026#39;DELIMITER\u0026#39; HERE-DOCUMENT DELIMITER The first line starts with an optional command followed by the special redirection operator \u0026laquo; and the delimiting identifier. You can use any string as a delimiting identifier, the most commonly used are EOF or END. If the delimiting identifier is unquoted, the shell will substitute all variables, commands and special characters before passing the here-document lines to the command. Appending a minus sign to the redirection operator \u0026laquo;-, will cause all leading tab characters to be ignored. This allows you to use indentation when writing here-documents in shell scripts. Leading whitespace characters are not allowed, only tab. The here-document block can contain strings, variables, commands and any other type of input. The last line ends with the delimiting identifier. White space in front of the delimiter is not allowed. Basic Heredoc Examples In this section, we will look at some basic examples of how to use heredoc.\nHeredoc is most often used in combination with the cat command .\nIn the following example, we are passing two lines of text containing an environment variable and a command to cat using a here document.\ncat \u0026lt;\u0026lt; EOF The current working directory is: $PWD You are logged in as: $(whoami) EOF As you can see from the output below, both the variable and the command output are substituted:\nThe current working directory is: /home/linuxize You are logged in as: linuxize Let’s see what will happen if we enclose the delimiter in single or double quotes.\ncat \u0026lt;\u0026lt;- \u0026#34;EOF\u0026#34; The current working directory is: $PWD You are logged in as: $(whoami) EOF You can notice that when the delimiter is quoted no parameter expansion and command substitution is done by the shell.\nThe current working directory is: $PWD You are logged in as: $(whoami) If you are using a heredoc inside a statement or loop, use the \u0026laquo;- redirection operation that allows you to indent your code.\nif true; then cat \u0026lt;\u0026lt;- EOF Line with a leading tab. EOF fi #output Line with a leading tab. Instead of displaying the output on the screen you can redirect it to a file using the \u0026gt;, \u0026raquo; operators.\ncat \u0026lt;\u0026lt; EOF \u0026gt; file.txt The current working directory is: $PWD You are logged in as: $(whoami) EOF If the file.txt doesn’t exist it will be created. When using \u0026gt; the file will be overwritten, while the \u0026raquo; will append the output to the file.\nThe heredoc input can also be piped. In the following example the sed command will replace all instances of the l character with e:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; | sed \u0026#39;s/l/e/g\u0026#39; Hello World EOF #output Heeeo Wored To write the piped data to a file:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; | sed \u0026#39;s/l/e/g\u0026#39; \u0026gt; file.txt Hello World EOF Using Heredoc is one of the most convenient and easiest ways to execute multiple commands on a remote system over SSH .\nWhen using unquoted delimiter make sure you escape all variables, commands and special characters otherwise they will be interpolated locally:\nssh -T user@host.com \u0026lt;\u0026lt; EOF echo \u0026#34;The current local working directory is: $PWD\u0026#34; echo \u0026#34;The current remote working directory is: \\$PWD\u0026#34; EOF #above command output The current local working directory is: /home/linuxize The current remote working directory is: /home/user Another usage is to read strings into a variable. The following is an example where we can read a json format config to my_config_json variable.\nread -r -d \u0026#39;\u0026#39; my_config_json \u0026lt;\u0026lt;EOF { \u0026#34;working_dir\u0026#34;: \u0026#34;${WORK_DIR}\u0026#34;, \u0026#34;env_vars\u0026#34;: { \u0026#34;PYTHONPATH\u0026#34;: \u0026#34;${SRC_DIR}:$PYTHONPATH\u0026#34;, \u0026#34;LD_LIBRARY_PATH\u0026#34;: \u0026#34;my_ld_path:$LD_LIBRARY_PATH\u0026#34; } } EOF # we can use the above config in Ray ray job submit --address=\u0026#34;http://127.0.0.1:8265\u0026#34; \\ --runtime-env-json=\u0026#34;${my_config_json}\u0026#34; \\ -- python xx Another example on heredoc:\n# https://github.com/bigscience-workshop/bigscience/blob/master/train/tr8b-104B/tr8b-104B-emb-norm-64n.slurm config_json=\u0026#34;./ds_config.$SLURM_JOBID.json\u0026#34; # Deepspeed figures out GAS dynamically from dynamic GBS via set_train_batch_size() cat \u0026lt;\u0026lt;EOT \u0026gt; $config_json { \u0026#34;train_micro_batch_size_per_gpu\u0026#34;: $MICRO_BATCH_SIZE, \u0026#34;train_batch_size\u0026#34;: $GLOBAL_BATCH_SIZE, \u0026#34;gradient_clipping\u0026#34;: 1.0, \u0026#34;zero_optimization\u0026#34;: { \u0026#34;stage\u0026#34;: $ZERO_STAGE }, \u0026#34;fp16\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;loss_scale\u0026#34;: 0, \u0026#34;loss_scale_window\u0026#34;: 500, \u0026#34;hysteresis\u0026#34;: 2, \u0026#34;min_loss_scale\u0026#34;: 1, \u0026#34;initial_scale_power\u0026#34;: 12 }, \u0026#34;zero_allow_untested_optimizer\u0026#34;: true, \u0026#34;steps_per_print\u0026#34;: 2000, \u0026#34;wall_clock_breakdown\u0026#34;: false } EOT Tmux How to run a python script on each GPU\nfor rank in {0..7} ; do CUDA_VISIBILE_DEVICES=$rank tmux new-session -d -s my_session_${rank} python3 my_python_script my_arguments 2\u0026gt;\u0026amp;1 \u0026amp; References [1] https://itsnotbugitsfeature.com/2019/10/22/splitting-a-big-pull-request-into-smaller-review-able-ones/\n","permalink":"https://rich-junwang.github.io/en-us/posts/tech/shell-commands/","summary":"\u003ch3 id=\"commonly-used-commands\"\u003eCommonly Used Commands\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSCRIPT_DIR\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e$(\u003c/span\u003e cd \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003e$(\u003c/span\u003e dirname \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003eBASH_SOURCE[0]\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u0026amp;\u0026gt; /dev/null \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e pwd \u003cspan style=\"color:#66d9ef\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"git\"\u003eGit\u003c/h3\u003e\n\u003ch4 id=\"git-merge\"\u003eGit merge\u003c/h4\u003e\n\u003cp\u003eSuppose we\u0026rsquo;re on \u003cstrong\u003emaster\u003c/strong\u003e branch, if we want to override the changes in the master branch with feature branch, we can use the following command\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit merge -X theirs feature\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eto keep the master branch changes:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit merge -X ours feature\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIf we want to rebase of current branch onto the master, and want to keep feature branch\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit rebase master -X theirs\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eif we want to keep master branch changes over our feature branch, the\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit rebase master -X ours\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo summarize, we can have the following table:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u0026nbsp; Currently on\u003c/th\u003e\n\u003cth\u003eCommand \u0026nbsp; \u0026nbsp;\u003c/th\u003e\n\u003cth\u003eStrategy \u0026nbsp; \u0026nbsp;\u003c/th\u003e\n\u003cth\u003eOutcome\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u0026nbsp;master\u003c/td\u003e\n\u003ctd\u003egit merge feature \u0026nbsp;\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003e-Xtheirs\u003c/strong\u003e \u0026nbsp; \u0026nbsp;\u003c/td\u003e\n\u003ctd\u003eKeep changes from feature branch\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emaster\u003c/td\u003e\n\u003ctd\u003egit merge feature\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e-Xours\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003ekeep changes from master branch\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u0026nbsp;feature\u003c/td\u003e\n\u003ctd\u003egit rebase master \u0026nbsp;\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003e-Xtheirs\u003c/strong\u003e \u0026nbsp; \u0026nbsp;\u003c/td\u003e\n\u003ctd\u003eKeep changes from feature branch\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003efeature\u003c/td\u003e\n\u003ctd\u003egit rebase master\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e-Xours\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003ekeep changes from master branch\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n{:.mbtablestyle}\n\u003cbr\u003e\n#### Git Diff\n\u003cp\u003eTo check two branch difference, suppose we\u0026rsquo;re on branch1, then we can do,\u003c/p\u003e","title":"Shell Commands"},{"content":"Three-way Code Management # Assuming the current directory is \u0026lt;your-repo-name\u0026gt; git remote add upstream https://github.com/alshedivat/al-folio.git git fetch upstream # check all branches git branch -avv git checkout main # local main tracking personal repo git rebase upstream/main # rebase from upstream git push -f # push to personal repo Manually Add Upstream Branch We can also manually do the changes. This is the original .git/config\n[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \u0026#34;origin\u0026#34;] url = https://github.com/yyy/zzz fetch = +refs/heads/*:refs/remotes/origin/* [branch \u0026#34;master\u0026#34;] remote = origin merge = refs/heads/master Making changes like this:\n[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \u0026#34;origin\u0026#34;] url = https://github.com/xxx/zzz fetch = +refs/heads/*:refs/remotes/origin/* [remote \u0026#34;upstream\u0026#34;] url = https://github.com/yyy/zzz fetch = +refs/heads/*:refs/remotes/upstream/* [branch \u0026#34;master\u0026#34;] remote = origin merge = refs/heads/master Sync New Commits from Public Repo If we haven’t make changes at master, we can do:\ngit checkout master git pull upstream master # pull public (upstream) to local branch git push origin master If there is conflict with your changes:\ngit checkout master git fetch upstream git merge --no-ff upstream/master git push origin master ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/git/git_fork/","summary":"\u003ch3 id=\"three-way-code-management\"\u003eThree-way Code Management\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"./image.png\" alt=\"alt text\"  /\u003e\n\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Assuming the current directory is \u0026lt;your-repo-name\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit remote add upstream https://github.com/alshedivat/al-folio.git\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit fetch upstream\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# check all branches\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit branch -avv\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit checkout main \u003cspan style=\"color:#75715e\"\u003e# local main tracking personal repo\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit rebase upstream/main \u003cspan style=\"color:#75715e\"\u003e# rebase from upstream\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit push -f \u003cspan style=\"color:#75715e\"\u003e# push to personal repo\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"manually-add-upstream-branch\"\u003eManually Add Upstream Branch\u003c/h3\u003e\n\u003cp\u003eWe can also manually do the changes. This is the original .git/config\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[core]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    repositoryformatversion = 0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    filemode = true\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    bare = false\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    logallrefupdates = true\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[remote \u0026#34;origin\u0026#34;]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    url = https://github.com/yyy/zzz\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    fetch = +refs/heads/*:refs/remotes/origin/*\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[branch \u0026#34;master\u0026#34;]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    remote = origin\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    merge = refs/heads/master\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eMaking changes like this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[core]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    repositoryformatversion = 0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    filemode = true\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    bare = false\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    logallrefupdates = true\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[remote \u0026#34;origin\u0026#34;]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    url = https://github.com/xxx/zzz\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    fetch = +refs/heads/*:refs/remotes/origin/*\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[remote \u0026#34;upstream\u0026#34;]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    url = https://github.com/yyy/zzz\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    fetch = +refs/heads/*:refs/remotes/upstream/*\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[branch \u0026#34;master\u0026#34;]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    remote = origin\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    merge = refs/heads/master\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"sync-new-commits-from-public-repo\"\u003eSync New Commits from Public Repo\u003c/h2\u003e\n\u003cp\u003eIf we haven’t make changes at master, we can do:\u003c/p\u003e","title":"Using Git to Create a Private Fork"},{"content":"Forward and Reverse Proxy Server When clients (web browsers) make requests to sites and services on the Internet, the forward proxy server intercepts those requests and then communicates with servers on behalf of these clients like a middleman. Why we use forward proxy server?\nCircumvent restrictions. Sometimes restrictions through firewall are put on the access of the internet. A forward proxy can get around these restriction. Block contents. Proxies can be set up to block a certain type of user to access a specific type of online contents Protect online ID. A reverse proxy is an application that sits in front of back-end applications and forwards client (e.g. browser) requests to those applications. Why we use reverse proxy server?\nLoad balancing Protection from attacks Caching (such as CDN caching) SSL encryption A simplified way to sum it up would be to say that a forward proxy sits in front of a client and ensures that no origin server ever communicates directly with that specific client. On the other hand, a reverse proxy sits in front of an origin server and ensures that no client ever communicates directly with that origin server.\nPort Forwarding Port forwarding or port mapping is a technique used in networking to redirect communication requests from one address and port number combination to another while the packets traverse a network gateway, such as a router or firewall. It is commonly used to allow external devices to access services on a private network, such as a home network, from the outside world.\nNetwork Address Translation (NAT): Most routers use NAT to allow multiple devices on a local network to share a single public IP address. NAT changes the private IP addresses of devices on the local network to the public IP address of the router when sending data out to the internet.\nPort Numbers: When data is sent over the internet, it is associated with a port number, which helps identify the specific service or application that data should be directed to. For example, HTTP traffic typically uses port 80, and HTTPS traffic uses port 443.\nPort Forwarding Rule: When you set up port forwarding on a router, you create a rule that tells the router to forward traffic received on a specific port (or range of ports) to a specific internal IP address and port on the local network. This allows external devices to access a service on a device within the private network.\nHome Network Debugging I have a MBP, but somehow the internet speed is very slow. When I use iphone the speed could be 300M+, however, while I use MBP, it\u0026rsquo;s like 30M+. I did some research, the following command helped me.\nifconfig awdl0 down Turning off bluetooth might help. But from my experience, it\u0026rsquo;s not that much.\nReferences https://www.speedtest.net/ https://www.reddit.com/r/macbookpro/comments/rtyjbt/finally_solved_my_slow_wifi_speeds_on_my_2021/ ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/web/forward_reverse_proxy/","summary":"\u003ch3 id=\"forward-and-reverse-proxy-server\"\u003eForward and Reverse Proxy Server\u003c/h3\u003e\n\u003cp\u003eWhen clients (web browsers) make requests to sites and services on the Internet, the forward proxy server intercepts those requests and then communicates with servers on behalf of these clients like a middleman.\n\u003cimg loading=\"lazy\" src=\"images/forward_proxy.png\" alt=\"\"  /\u003e\n\nWhy we use forward proxy server?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCircumvent restrictions. Sometimes restrictions through firewall are put on the access of the internet. A forward proxy can get around these restriction.\u003c/li\u003e\n\u003cli\u003eBlock contents. Proxies can be set up to block a certain type of user to access a specific type of online contents\u003c/li\u003e\n\u003cli\u003eProtect online ID.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA reverse proxy is an application that sits in front of back-end applications and forwards client (e.g. browser) requests to those applications.\n\u003cimg loading=\"lazy\" src=\"images/reverse_proxy.png\" alt=\"\"  /\u003e\n\nWhy we use reverse proxy server?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLoad balancing\u003c/li\u003e\n\u003cli\u003eProtection from attacks\u003c/li\u003e\n\u003cli\u003eCaching (such as CDN caching)\u003c/li\u003e\n\u003cli\u003eSSL encryption\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA simplified way to sum it up would be to say that a forward proxy sits in front of a client and ensures that no origin server ever communicates directly with that specific client. On the other hand, a reverse proxy sits in front of an origin server and ensures that no client ever communicates directly with that origin server.\u003c/p\u003e","title":"Basics of Web"},{"content":"Concurrency in Python Parallelism consists of performing multiple operations at the same time. Multiprocessing is a means to effect parallelism, and it entails spreading tasks over a computer’s central processing units (CPUs, or cores). Multiprocessing is well-suited for CPU-bound tasks: tightly bound for loops and mathematical computations usually fall into this category.\nConcurrency is a slightly broader term than parallelism. It suggests that multiple tasks have the ability to run in an overlapping manner. (There’s a saying that concurrency does not imply parallelism.)\nThreading is a concurrent execution model whereby multiple threads take turns executing tasks. One process can contain multiple threads. Python has a complicated relationship with threading thanks to its GIL, but that’s beyond the scope of this article.\nWhat’s important to know about threading is that it’s better for IO-bound tasks. While a CPU-bound task is characterized by the computer’s cores continually working hard from start to finish, an IO-bound job is dominated by a lot of waiting on input/output to complete.\nTo recap the above, concurrency encompasses both multiprocessing (ideal for CPU-bound tasks) and threading (suited for IO-bound tasks). Multiprocessing is a form of parallelism, with parallelism being a specific type (subset) of concurrency. The Python standard library has offered longstanding support for both of these through its multiprocessing, threading, and concurrent.futures packages.\nupdate python on ubuntu When there are multiple version of python in the system, how to set the default python to use. Below we suppose to install newer version of python3.9\nsudo apt install python3.9 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.[old-version] 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2 #after the following command, select the new version when being prompted and press enter sudo update-alternatives --config python3 # There is a simpler way. From system admin perspective, this is not scalable. sudo ln -sf /usr/bin/python3.9 /usr/bin/python3 find a python package related files pip show -f package-name Find out python binry file path \u0026gt;\u0026gt;\u0026gt; import streamlit \u0026gt;\u0026gt;\u0026gt; print(streamlit.__file__) Process Got Killed Once in a while, I found my python process got killed without any errors. Most of time, it\u0026rsquo;s related to out of memory (OOM) issue. We can quickly check that using the following command\ndmesg | grep \u0026#34;oom-kill\u0026#34; | less Virtual Env python3 -m pip install --upgrade pip python3 -m pip install --user virtualenv python3 -m venv .venv source .venv/bin/activate # exit deactivate Debug # using pdb import pdb; pdb.set_trace() # using ipdb if torch.distributed.get_rank() == 0: import ipdb; ipdb.set_trace() # using ipdb. Program will enter ipython at the exception from ipdb import launch_ipdb_on_exception def filter_even(nums): for i in range(len(nums)): if nums[i] % 2 == 0: del nums[i] with launch_ipdb_on_exception(): print(filter_even(list(range(6)))) # we can check internal stack, `i`, `nums` etc and can also execute the next step. # We can also use the following way. The program will error out and enter ipdb. import sys from IPython.core import ultratb sys.excepthook = ultratb.FormattedTB(call_pdb=1) def filter_even(nums): for i in range(len(nums)): if nums[i] % 2 == 0: del nums[i] filter_even(list(range(6))) Exception How to catch generic exception type\ntry: someFunction() except Exception as ex: template = \u0026#34;An exception of type {0} occurred. Arguments:\\n{1!r}\u0026#34; message = template.format(type(ex).__name__, ex.args) print(message) The difference between the above and using just except without any argument is twofold: (1) A bare except doesn\u0026rsquo;t give you the exception object to inspect (2) The exceptions SystemExit, KeyboardInterrupt and GeneratorExit aren\u0026rsquo;t caught by the above code, which is generally what you want.\nIf you also want the same stacktrace you get if you do not catch the exception, you can get that like this (still inside the except clause):\nimport traceback print(traceback.format_exc()) # or traceback.print_exc() If you use the logging module, you can print the exception to the log (along with a message) like this:\nimport logging log = logging.getLogger() log.exception(\u0026#34;Message for you, sir!\u0026#34;) To dig deeper and examine the stack, look at variables etc., use the post_mortem function of the pdb module inside the except block:\nimport pdb pdb.post_mortem() new method The new() is a static method of the object class. When you create a new object by calling the class, Python calls the new() method to create the object first and then calls the init() method to initialize the object’s attributes. Override the new() method if you want to tweak the object at creation time. Hacky Way to Add File into PYTHONPATH curr_file_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__))) sys.path.append(curr_file_path) sys.path.append(os.path.dirname(curr_file_path)) Subprocess import subprocess # Download dl = subprocess.Popen([\u0026#34;git\u0026#34;, \u0026#34;clone\u0026#34;, str(repo_path), str(repo_dir)]) # It also accepts str as the input command output_path = \u0026#34;my_out\u0026#34; cmd = \u0026#34;\u0026#34;\u0026#34;python3 train.py --local-rank 0\u0026#34;\u0026#34;\u0026#34; proc = subprocess.Popen( cmd, stdout=subprocess.PIPE, stderr=open(f\u0026#34;{output_path}.stderr\u0026#34;, \u0026#34;wt\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;), shell=True, encoding=\u0026#39;utf-8\u0026#39;, bufsize=0) Dataclass Data classes use something called a default_factory to handle mutable default values. To use default_factory, we need to use the field() specifier.\nfrom dataclasses import dataclass, field from typing import List RANKS = \u0026#39;2 3 4 5 6 7 8 9 10 J Q K A\u0026#39;.split() SUITS = \u0026#39;♣ ♢ ♡ ♠\u0026#39;.split() def make_french_deck(): return [PlayingCard(r, s) for s in SUITS for r in RANKS] @dataclass class Deck: # we can\u0026#39;t do the following way to assign a mutable default value to a field. # cards: List[PlayingCard] = make_french_deck() cards: List[PlayingCard] = field(default_factory=make_french_deck) A few commonly used parameters that field supports\ndefault: Default value of the field default_factory: Function that returns the initial value of the field init: Use field in .init() method? (Default is True.) repr: Use field in repr of the object? (Default is True.) References https://realpython.com/python-data-classes/ ","permalink":"https://rich-junwang.github.io/en-us/posts/tech/programming_language/python/python_basics/","summary":"\u003ch3 id=\"concurrency-in-python\"\u003eConcurrency in Python\u003c/h3\u003e\n\u003cp\u003eParallelism consists of performing multiple operations at the same time. Multiprocessing is a means to effect parallelism, and it entails spreading tasks over a computer’s central processing units (CPUs, or cores). Multiprocessing is well-suited for CPU-bound tasks: tightly bound for loops and mathematical computations usually fall into this category.\u003c/p\u003e\n\u003cp\u003eConcurrency is a slightly broader term than parallelism. It suggests that multiple tasks have the ability to run in an overlapping manner. (There’s a saying that concurrency does not imply parallelism.)\u003c/p\u003e\n\u003cp\u003eThreading is a concurrent execution model whereby multiple threads take turns executing tasks. One process can contain multiple threads. Python has a complicated relationship with threading thanks to its GIL, but that’s beyond the scope of this article.\u003c/p\u003e","title":"Python"}]